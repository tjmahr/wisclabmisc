% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/utils-stats.R
\name{info_surprisal}
\alias{info_surprisal}
\alias{info_cross_entropy}
\alias{info_entropy}
\alias{info_kl_divergence}
\alias{info_kl_divergence_matrix}
\title{Compute entropy and related measures}
\usage{
info_surprisal(p)

info_cross_entropy(p, q)

info_entropy(p)

info_kl_divergence(p, q)

info_kl_divergence_matrix(mat)
}
\arguments{
\item{p}{a probability distribution (vector of probabilities that sum to 1)}

\item{mat}{a matrix where each row is a probability distribution}

\item{x}{a vector of probabilities}
}
\value{
the entropy
}
\description{
Compute entropy and related measures
}
\section{Information theory basics}{


Given a probability \code{x}, the \strong{surprisal value} (or information content) of
\code{x} is the log of the inverse probability, \code{log(1 / x)}. Rare events (smaller
probabilities) have larger surprisal values than more common events. The word
"surprise" here conveys how unexpected or informative an event is. (The idea
that surprises are "informative" or contain information makes sense with the
intuition that there isn't much to be learned from predictable events.)
Because of how division works with logarithms, \code{log(1 / x)} simplifies so
that \code{info_surprisal(x)} is the negative log probability,\code{-log(x)}. The units
of the information content depends on the base of the logarithm. Our
functions use the natural \code{log()} which provides information in \emph{nats}, but
it is also common to see \code{log2()}-based surprisals which provide information
in \emph{bits}.

Given a probability distribution \code{p}---a vector of probabilities that
sum to 1---the \strong{entropy} of the distribution is the
probability-weighted average of the surprisal values. A weighted average
is \code{sum(weights * values) / sum(weights)}, but because the weights here
are probabilities that sum to 1, the \code{info_entropy(p)} is \code{sum(p * info_surprisal(p))}. Entropy can be interpreted as a measure of
uncertainty in the distribution; it's the expected surprisal value in
the distribution.

In \code{info_entropy(p)} the surprisal values and their weights came from
the same probability distribution. But this doesn't need to be the case.
Suppose that there are ground-truth probabilities \code{p} and there are also
estimated probabilities \code{q}. \code{info_entropy(q)} computes a weighted
average where events with surprisal \code{info_surprisal(q)} have frequencies
of \code{q}. But if we knew the true frequencies, the surprisals in \code{q} would
occur with frequency \code{p}, so their weighted average should be \code{sum(p * info_surprisal(q))}. This value is the \strong{cross entropy} of \code{q} with
respect to \code{p}, and \code{info_cross_entropy(p, q)} implements this function.
Cross entropy is commonly used as a loss function in machine learning.

\href{https://en.wikipedia.org/wiki/Gibbs\%27_inequality}{Gibb's inequality}
says that \code{info_entropy(p) <= info_cross_entropy(p, q)}. Unless \code{p} and
\code{q} are the same distribution, there will always be some excess
uncertainty or surprise in the cross entropy compared to the entropy:
\verb{info_cross_entropy(p, q) = info_entropy(p) + *excess*}. This excess is
the \strong{Kullback-Liebler divergence} (KL divergence or relative entropy).
Due to the properties of logarithms, \code{info_kl_divergence(p, q)}
simplifies to \code{sum (p * log(p / q))}. KL divergence is an important
quantity for comparing probability distributions. For example, each row
in a confusion matrix is a probability distribution, and KL divergence
can identify which rows are most similar. \code{info_kl_divergence_matrix(mat)}
computes a distance matrix using on each pair of rows in \code{mat} using KL
divergence.
}

\examples{
wikipedia_example <- rbind(
  p = c(9, 12, 4) / 25,
  q = c(1,  1, 1) / 3
)

info_kl_divergence_matrix(wikipedia_example)
}
