<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Compute entropy and related measures — info_surprisal • wisclabmisc</title><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Compute entropy and related measures — info_surprisal"><meta name="description" content="Compute entropy and related measures"><meta property="og:description" content="Compute entropy and related measures"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css" integrity="sha384-nB0miv6/jRmo5UMMR1wu3Gz6NLsoTkbqJghGIsx//Rlm+ZU03BU6SQNC66uf4l5+" crossorigin="anonymous"><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js" integrity="sha384-7zkQWkzuo3B5mTepMUcHkMB5jZaolc2xDwL6VFqjFALcbeS9Ggm/Yr2r3Dy4lfFg" crossorigin="anonymous"></script><script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-primary" data-bs-theme="dark" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">wisclabmisc</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="../articles/gamlss-tools.html">Tools for GAMLSS models</a></li>
    <li><a class="dropdown-item" href="../articles/roc.html">Tools for ROC curves</a></li>
    <li><a class="dropdown-item" href="../articles/utterance-length-imputation.html">Utterance length imputation and weighting</a></li>
    <li><a class="dropdown-item" href="../articles/brms.html">A Workflow for brms</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><a class="dropdown-item" href="../articles/index.html">More articles...</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/tjmahr/wisclabmisc/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Compute entropy and related measures</h1>
      <small class="dont-index">Source: <a href="https://github.com/tjmahr/wisclabmisc/blob/main/R/utils-stats.R" class="external-link"><code>R/utils-stats.R</code></a></small>
      <div class="d-none name"><code>information.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>Compute entropy and related measures</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">info_surprisal</span><span class="op">(</span><span class="va">x</span>, base <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">info_entropy</span><span class="op">(</span><span class="va">p</span>, base <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">info_cross_entropy</span><span class="op">(</span><span class="va">p</span>, <span class="va">q</span>, base <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">info_kl_divergence</span><span class="op">(</span><span class="va">p</span>, <span class="va">q</span>, base <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span>
<span></span>
<span><span class="fu">info_kl_divergence_matrix</span><span class="op">(</span><span class="va">mat</span>, base <span class="op">=</span> <span class="cn">NULL</span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-x">x<a class="anchor" aria-label="anchor" href="#arg-x"></a></dt>
<dd><p>a vector of probabilities</p></dd>


<dt id="arg-base">base<a class="anchor" aria-label="anchor" href="#arg-base"></a></dt>
<dd><p>the base of the logarithm. Defaults to <em>e</em> (<code>exp(1)</code>).</p></dd>


<dt id="arg-p-q">p, q<a class="anchor" aria-label="anchor" href="#arg-p-q"></a></dt>
<dd><p>a probability distribution (vector of probabilities that sum to 1)</p></dd>


<dt id="arg-mat">mat<a class="anchor" aria-label="anchor" href="#arg-mat"></a></dt>
<dd><p>a matrix where each row is a probability distribution</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>the entropy</p>
    </div>
    <div class="section level2">
    <h2 id="information-theory-basics">Information theory basics<a class="anchor" aria-label="anchor" href="#information-theory-basics"></a></h2>



<p>Given a probability <code>x</code>, the <strong>surprisal value</strong> (or information content) of
<code>x</code> is the log of the inverse probability, <code>log(1 / x)</code>. Rare events (smaller
probabilities) have larger surprisal values than more common events. The word
"surprise" here conveys how unexpected or informative an event is. (The idea
that surprises are "informative" or contain information makes sense with the
intuition that there isn't much to be learned from predictable events.)
Because of how division works with logarithms, <code>log(1 / x)</code> simplifies so
that <code>info_surprisal(x)</code> is the negative log probability,<code>-log(x)</code>. The units
of the information content depends on the base of the logarithm. Our
functions by default use the natural <code><a href="https://rdrr.io/r/base/Log.html" class="external-link">log()</a></code> which provides information in
<em>nats</em>, but it is also common to see <code><a href="https://rdrr.io/r/base/Log.html" class="external-link">log2()</a></code>-based surprisals which provide
information in <em>bits</em>.</p>
<p>Given a probability distribution <code>p</code>—a vector of probabilities that
sum to 1—the <strong>entropy</strong> of the distribution is the
probability-weighted average of the surprisal values. A weighted average
is <code>sum(weights * values) / sum(weights)</code>, but because the weights here
are probabilities that sum to 1, the <code>info_entropy(p)</code> is <code>sum(p * info_surprisal(p))</code>. Entropy can be interpreted as a measure of
uncertainty in the distribution; it's the expected surprisal value in
the distribution.</p>
<p>In <code>info_entropy(p)</code> the surprisal values and their weights came from
the same probability distribution. But this doesn't need to be the case.
Suppose that there are ground-truth probabilities <code>p</code> and there are also
estimated probabilities <code>q</code>. <code>info_entropy(q)</code> computes a weighted
average where events with surprisal <code>info_surprisal(q)</code> have frequencies
of <code>q</code>. But if we knew the true frequencies, the surprisals in <code>q</code> would
occur with frequency <code>p</code>, so their weighted average should be <code>sum(p * info_surprisal(q))</code>. This value is the <strong>cross entropy</strong> of <code>q</code> with
respect to <code>p</code>, and <code>info_cross_entropy(p, q)</code> implements this function.
Cross entropy is commonly used as a loss function in machine learning.</p>
<p><a href="https://en.wikipedia.org/wiki/Gibbs%27_inequality" class="external-link">Gibb's inequality</a>
says that <code>info_entropy(p) &lt;= info_cross_entropy(p, q)</code>. Unless <code>p</code> and
<code>q</code> are the same distribution, there will always be some excess
uncertainty or surprise in the cross entropy compared to the entropy:
<code>info_cross_entropy(p, q) = info_entropy(p) + *excess*</code>. This excess is
the <strong>Kullback-Liebler divergence</strong> (KL divergence or relative entropy).
Due to the properties of logarithms, <code>info_kl_divergence(p, q)</code>
simplifies to <code>sum (p * log(p / q))</code>. KL divergence is an important
quantity for comparing probability distributions. For example, each row
in a confusion matrix is a probability distribution, and KL divergence
can identify which rows are most similar. <code>info_kl_divergence_matrix(mat)</code>
computes a distance matrix using on each pair of rows in <code>mat</code> using KL
divergence.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="va">wikipedia_example</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span></span></span>
<span class="r-in"><span>  p <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">9</span>, <span class="fl">12</span>, <span class="fl">4</span><span class="op">)</span> <span class="op">/</span> <span class="fl">25</span>,</span></span>
<span class="r-in"><span>  q <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>,  <span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span> <span class="op">/</span> <span class="fl">3</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="fu">info_kl_divergence_matrix</span><span class="op">(</span><span class="va">wikipedia_example</span><span class="op">)</span></span></span>
<span class="r-out co"><span class="r-pr">#&gt;</span>            p         q</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> p 0.00000000 0.0852996</span>
<span class="r-out co"><span class="r-pr">#&gt;</span> q 0.09745501 0.0000000</span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Tristan Mahr.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer></div>





  </body></html>

