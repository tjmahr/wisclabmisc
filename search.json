[{"path":"https://www.tjmahr.com/wisclabmisc/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2020 Tristan Mahr Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/articles/gamlss-tools.html","id":"a-gamlss-that-remembers-the-data","dir":"Articles","previous_headings":"","what":"A gamlss() that remembers the data","title":"Tools for GAMLSS models","text":"mem_gamlss() (memory gamlss) provides drop-replacement gamlss() function. difference mem_gamlss() gamlss() modified version includes bundle data .user records original dataset, session information call used fit model. gamlss store data part model object, need dataset prediction centile prediction often fails without dataset: including original dataset works: (“Centile prediction” means predicting percentiles data along single variable. ’s function just needs single xname: single predictor variable used. use centile prediction compute growth curves can look smooth changes percentiles age.)","code":"library(wisclabmisc) library(gamlss) library(tidyverse)  data <- as.data.frame(nlme::Orthodont) model <- mem_gamlss(distance ~ age, data = data) #> GAMLSS-RS iteration 1: Global Deviance = 505.577  #> GAMLSS-RS iteration 2: Global Deviance = 505.577 str(model$.user, max.level = 1) #> List of 3 #>  $ data        :'data.frame':    108 obs. of  4 variables: #>  $ session_info:List of 2 #>   ..- attr(*, \"class\")= chr [1:2] \"session_info\" \"list\" #>  $ call        : language mem_gamlss(distance ~ age, data = data) newdata <- distinct(data, age) centiles.pred(   model,    cent = c(25, 50, 75),   xname = \"age\",    xvalues = newdata$age,    plot = FALSE ) #> Error in data.frame(data, source = namelist): arguments imply differing number of rows: 4, 5 centiles.pred(   model,    cent = c(25, 50, 75),   xname = \"age\",    xvalues = newdata$age,    plot = FALSE,   data = model$.user$data ) #>    x       25       50       75 #> 1  8 20.34723 22.04259 23.73796 #> 2 10 21.66760 23.36296 25.05833 #> 3 12 22.98797 24.68333 26.37870 #> 4 14 24.30834 26.00370 27.69907"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/gamlss-tools.html","id":"centile-prediction-and-tidying","dir":"Articles","previous_headings":"","what":"Centile prediction and tidying","title":"Tools for GAMLSS models","text":"package provides predict_centiles() streamlined version code, : assumes model fitted mem_gamlss() returns tibble keeps predictor name (, age instead x) prefixes centiles q (quantile) predicted centiles wide format. can tidy long format pivot_centiles_longer(). also includes .pair column helps mark commonly paired quantiles 25:75, 10:90, 5:95.","code":"centiles <- predict_centiles(   newdata,   model,    cent = c(25, 50, 75) ) centiles #> # A tibble: 4 × 4 #>     age   c25   c50   c75 #>   <dbl> <dbl> <dbl> <dbl> #> 1     8  20.3  22.0  23.7 #> 2    10  21.7  23.4  25.1 #> 3    12  23.0  24.7  26.4 #> 4    14  24.3  26.0  27.7 pivot_centiles_longer(centiles) #> # A tibble: 12 × 4 #>      age .centile .value .centile_pair   #>    <dbl>    <dbl>  <dbl> <chr>           #>  1     8       25   20.3 centiles 25, 75 #>  2     8       50   22.0 median          #>  3     8       75   23.7 centiles 25, 75 #>  4    10       25   21.7 centiles 25, 75 #>  5    10       50   23.4 median          #>  6    10       75   25.1 centiles 25, 75 #>  7    12       25   23.0 centiles 25, 75 #>  8    12       50   24.7 median          #>  9    12       75   26.4 centiles 25, 75 #> 10    14       25   24.3 centiles 25, 75 #> 11    14       50   26.0 median          #> 12    14       75   27.7 centiles 25, 75"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/gamlss-tools.html","id":"sample-centiles-checks","dir":"Articles","previous_headings":"Centile prediction and tidying","what":"Sample centiles checks","title":"Tools for GAMLSS models","text":"Half data 50% centile line half 50% centile line. holds centile lines. check_sample_centiles() performs check computing percentages observations less equal centile line. matches gamlss package’s output: function also supports grouped data check centile performance different subsets data. output also matches output provide gamlss’s centile.split() function:","code":"check_sample_centiles(data, model, age, distance) #> # A tibble: 7 × 4 #>   .centile     n n_under_centile percent_under_centile #>      <dbl> <int>           <int>                 <dbl> #> 1        5   108               6                  5.56 #> 2       10   108               9                  8.33 #> 3       25   108              25                 23.1  #> 4       50   108              61                 56.5  #> 5       75   108              85                 78.7  #> 6       90   108              95                 88.0  #> 7       95   108             100                 92.6 centiles(   model,    model$.user$data$age,    data = model$.user$data,    cent = c(5, 10,25, 50, 75, 90, 95),    plot = FALSE ) #> % of cases below  5 centile is  5.555556  #> % of cases below  10 centile is  8.333333  #> % of cases below  25 centile is  23.14815  #> % of cases below  50 centile is  56.48148  #> % of cases below  75 centile is  78.7037  #> % of cases below  90 centile is  87.96296  #> % of cases below  95 centile is  92.59259 data %>%    mutate(age_bin = ntile(age, 2)) %>%    group_by(age_bin) %>%    check_sample_centiles(model, age, distance) #> # A tibble: 14 × 5 #>    age_bin .centile     n n_under_centile percent_under_centile #>      <int>    <dbl> <int>           <int>                 <dbl> #>  1       1        5    54               3                  5.56 #>  2       1       10    54               4                  7.41 #>  3       1       25    54              13                 24.1  #>  4       1       50    54              29                 53.7  #>  5       1       75    54              44                 81.5  #>  6       1       90    54              49                 90.7  #>  7       1       95    54              51                 94.4  #>  8       2        5    54               3                  5.56 #>  9       2       10    54               5                  9.26 #> 10       2       25    54              12                 22.2  #> 11       2       50    54              32                 59.3  #> 12       2       75    54              41                 75.9  #> 13       2       90    54              46                 85.2  #> 14       2       95    54              49                 90.7 centiles.split(   model,    model$.user$data$age,    data = model$.user$data,    n.inter = 2,   cent = c(5, 10,25, 50, 75, 90, 95),    plot = FALSE ) #>      7 to 11  11 to 15 #> 5   5.555556  5.555556 #> 10  7.407407  9.259259 #> 25 24.074074 22.222222 #> 50 53.703704 59.259259 #> 75 81.481481 75.925926 #> 90 90.740741 85.185185 #> 95 94.444444 90.740741"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/roc.html","id":"a-primer-on-roc-curves","dir":"Articles","previous_headings":"","what":"A primer on ROC curves","title":"Tools for ROC curves","text":"wisclabmisc provides functions tidying results ROC curves. curves arise diagnostic classification settings want use test score determine whether individual belongs control group versus case group. binary classification normal versus clinical status, regular email versus spam status, . use terminology control case follow pROC package’s interface. classification literature, tons tons statistics describe classifier performance. ROC curve centers around two important quantities sensitivity specificity: Also called true positive rate recall. apply spam classifier 100 spam emails, many correctly flagged spam? P(case result | case status) Sensitivity makes sense think problem detecting something subtle. (Like Jedi “force sensitive” Spider-Man’s Spidey sense tingling ’s danger.) Also called true negative rate selectivity. apply spam classifier 100 safe (ham) emails, many correctly ignored? P(control result | control status) Specificity great term; selectivity makes slightly sense. don’t want sensor trip noise: needs specific selective. Suppose diagnostic instrument provides score, choose diagnostic threshold one scores. example, suppose decide scores 60 indicate email probably spam can moved spam folder. threshold specificity attached . can look proportion spam emails equal 60 (sensitivity), can look proportion ham emails 60 (specificity). number choose threshold sensitivity specificity score, ROC curve visualization sensitivity specificity change along range threshold scores. (impenetrable terminology: ROC stands “receiver operating characteristic”, something detections made radar receivers different operating levels.)","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/articles/roc.html","id":"a-worked-example","dir":"Articles","previous_headings":"A primer on ROC curves","what":"A worked example","title":"Tools for ROC curves","text":"can work example ROC curve using pROC package. pROC provides aSAH dataset provides “several clinical one laboratory variable 113 patients aneurysmal subarachnoid hemorrhage” (hence, aSAH). outcome (Good versus Poor) measure called s100b. can see many Good outcomes near 0 Poor outcomes.  point grid points along s100b, can compute proportions patients group threshold. can plot proportions visualize trading relations specificity sensitivity threshold changes.  took 5 tries get plot correct. able convince noting Good outcomes less .51 threshold catch single Good outcome hence specificity 1. Conversely, just Poor outcome 1, threshold 1 going detect 1 Poor outcome hence low sensitivity. ignore threshold visualization, can (finally) plot canonical ROC curve. shows specificity reversing order ideal point top left corner (sensitivity = 1, specificity = 1).  can compare plot one provided pROC package. find perfect match sensitivity specificity values.   Instead computing ROC curves hand, defer calculation ROC curves pROC package easy get confused calculating sensitivity specificity pROC provides tools working ROC curves. Thus, wisclabmisc’s goal ROC curves provide helper functions fit ROC curves pROC return results nice dataframe. contrast two types ROC curves: empirical ROC curve raw data used make jagged ROC curve (smooth) density ROC curve densities two distributions used make smooth ROC curve.","code":"data <- as_tibble(aSAH) data #> # A tibble: 113 × 7 #>    gos6  outcome gender   age wfns  s100b  ndka #>    <ord> <fct>   <fct>  <int> <ord> <dbl> <dbl> #>  1 5     Good    Female    42 1      0.13  3.01 #>  2 5     Good    Female    37 1      0.14  8.54 #>  3 5     Good    Female    42 1      0.1   8.09 #>  4 5     Good    Female    27 1      0.04 10.4  #>  5 1     Poor    Female    42 3      0.13 17.4  #>  6 1     Poor    Male      48 2      0.1  12.8  #>  7 4     Good    Male      57 5      0.47  6    #>  8 1     Poor    Male      41 4      0.16 13.2  #>  9 5     Good    Female    49 1      0.18 15.5  #> 10 4     Good    Female    75 2      0.1   6.01 #> # ℹ 103 more rows count(data, outcome) #> # A tibble: 2 × 2 #>   outcome     n #>   <fct>   <int> #> 1 Good       72 #> 2 Poor       41  ggplot(data) +    aes(x = s100b, y = outcome) +    geom_point(     position = position_jitter(width = 0, height = .2),     size = 3,     alpha = .2,   ) +   theme_grey(base_size = 12) +   labs(y = NULL) by_outcome <- split(data, data$outcome) smallest_diff <- min(diff(unique(sort(data$s100b)))) grid <- tibble(   threshold = seq(     min(data$s100b) - smallest_diff,      max(data$s100b) + smallest_diff,      length.out = 200   ) )  roc_coordinates <- grid %>%    rowwise() %>%    summarise(     threshold = threshold,     prop_poor_above = mean(by_outcome$Poor$s100b >= threshold),     prop_good_below = mean(by_outcome$Good$s100b < threshold),   )  ggplot(roc_coordinates) +    aes(x = threshold) +    geom_step(aes(y = prop_poor_above)) +    geom_step(aes(y = prop_good_below)) +   annotate(\"text\", x = 2, y = .9, hjust = 1, label = \"specificity\") +    annotate(\"text\", x = 2, y = .1, hjust = 1, label = \"sensitivity\") +   labs(     title = \"Sensitivity and specificity as cumulative proportions\",     x = \"threshold (diagnosis when score >= threshold)\",     y = NULL   ) roc_coordinates <- roc_coordinates %>%    rename(     sensitivities = prop_poor_above,      specificities = prop_good_below   ) %>%   # otherwise the stair-steps look wrong   arrange(sensitivities)  p <- ggplot(roc_coordinates) +    aes(x = specificities, y = sensitivities) +    geom_step() +   scale_x_reverse() +    coord_fixed() +    theme_grey(base_size = 14) p roc <- pROC::roc(data, response = outcome, predictor = s100b) #> Setting levels: control = Good, case = Poor #> Setting direction: controls < cases plot(roc) proc_coordinates <- roc[2:3] %>%    as.data.frame() %>%    arrange(sensitivities)  # Plot the pROC point as a wide semi-transparent blue # band on top of ours p +    geom_step(     data = proc_coordinates,      color = \"blue\",     alpha = .5,     size = 2   ) #> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #> ℹ Please use `linewidth` instead. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated."},{"path":"https://www.tjmahr.com/wisclabmisc/articles/roc.html","id":"emprical-roc-curves","dir":"Articles","previous_headings":"","what":"Emprical ROC curves","title":"Tools for ROC curves","text":"Let’s return example, predicting group label outcome (case: Poor, control: Good) predictor s100b. messages, can see pROC::roc() makes decisions us: Good control level Poor case level, controls lower s100b cases. pROC::roc() returns roc object bundles data model results together. Ultimately, want results dataframe one row provide sensitivity specificity threshold value. can get close dataframe manipulating list using coords(). pROC::coords() additional features allow identify “best” ROC points, strips useful data like direction used. wisclabmisc provides compute_empirical_roc() combines results pROC::roc() pROC::coords() tibble. includes metadata .controls .cases levels, .direction relationship, overall .auc curve. also identifies two “best” coordinates .is_best_youden is_best_closest_topleft. Finally, retains name predictor variable. can still see messages emitted pROC::roc() call use compute_empirical_roc(). can pass arguments direction levels pROC::roc() silence messages. According help page pROC::coords() Youden’s J statistic point farthest vertical distance diagonal line. “best” point point closest upper-left corner. following plot labels distances. Youden’s point topleft point point.","code":"r <- pROC::roc(data, outcome, s100b) #> Setting levels: control = Good, case = Poor #> Setting direction: controls < cases r #>  #> Call: #> roc.data.frame(data = data, response = outcome, predictor = s100b) #>  #> Data: s100b in 72 controls (outcome Good) < 41 cases (outcome Poor). #> Area under the curve: 0.7314 r #>  #> Call: #> roc.data.frame(data = data, response = outcome, predictor = s100b) #>  #> Data: s100b in 72 controls (outcome Good) < 41 cases (outcome Poor). #> Area under the curve: 0.7314 class(r) #> [1] \"roc\" str(r, max.level = 1, give.attr = FALSE) #> List of 15 #>  $ percent           : logi FALSE #>  $ sensitivities     : num [1:51] 1 0.976 0.976 0.976 0.976 ... #>  $ specificities     : num [1:51] 0 0 0.0694 0.1111 0.1389 ... #>  $ thresholds        : num [1:51] -Inf 0.035 0.045 0.055 0.065 ... #>  $ direction         : chr \"<\" #>  $ cases             : num [1:41] 0.13 0.1 0.16 0.12 0.44 0.71 0.49 0.07 0.33 0.09 ... #>  $ controls          : num [1:72] 0.13 0.14 0.1 0.04 0.47 0.18 0.1 0.1 0.04 0.08 ... #>  $ fun.sesp          :function (thresholds, controls, cases, direction)   #>  $ auc               : 'auc' num 0.731 #>  $ call              : language roc.data.frame(data = data, response = outcome, predictor = s100b) #>  $ original.predictor: num [1:113] 0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ... #>  $ original.response : Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ... #>  $ predictor         : num [1:113] 0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ... #>  $ response          : Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ... #>  $ levels            : chr [1:2] \"Good\" \"Poor\" r[1:5] %>%    as.data.frame() %>%    tibble::as_tibble() #> # A tibble: 51 × 5 #>    percent sensitivities specificities thresholds direction #>    <lgl>           <dbl>         <dbl>      <dbl> <chr>     #>  1 FALSE           1            0        -Inf     <         #>  2 FALSE           0.976        0           0.035 <         #>  3 FALSE           0.976        0.0694      0.045 <         #>  4 FALSE           0.976        0.111       0.055 <         #>  5 FALSE           0.976        0.139       0.065 <         #>  6 FALSE           0.902        0.222       0.075 <         #>  7 FALSE           0.878        0.306       0.085 <         #>  8 FALSE           0.829        0.389       0.095 <         #>  9 FALSE           0.780        0.486       0.105 <         #> 10 FALSE           0.756        0.542       0.115 <         #> # ℹ 41 more rows  pROC::coords(r) %>%    tibble::as_tibble() #> # A tibble: 51 × 3 #>    threshold specificity sensitivity #>        <dbl>       <dbl>       <dbl> #>  1  -Inf          0            1     #>  2     0.035      0            0.976 #>  3     0.045      0.0694       0.976 #>  4     0.055      0.111        0.976 #>  5     0.065      0.139        0.976 #>  6     0.075      0.222        0.902 #>  7     0.085      0.306        0.878 #>  8     0.095      0.389        0.829 #>  9     0.105      0.486        0.780 #> 10     0.115      0.542        0.756 #> # ℹ 41 more rows compute_empirical_roc(data, outcome, s100b) #> Setting levels: control = Good, case = Poor #> Setting direction: controls < cases data_roc <- compute_empirical_roc(   data,    outcome,    s100b,    direction = \"<\",   levels = c(\"Good\", \"Poor\") ) data_roc #> # A tibble: 51 × 11 #>       s100b .specificities .sensitivities  .auc .direction .controls .cases #>       <dbl>          <dbl>          <dbl> <dbl> <chr>      <chr>     <chr>  #>  1 -Inf             0               1     0.731 <          Good      Poor   #>  2    0.035         0               0.976 0.731 <          Good      Poor   #>  3    0.045         0.0694          0.976 0.731 <          Good      Poor   #>  4    0.055         0.111           0.976 0.731 <          Good      Poor   #>  5    0.065         0.139           0.976 0.731 <          Good      Poor   #>  6    0.075         0.222           0.902 0.731 <          Good      Poor   #>  7    0.085         0.306           0.878 0.731 <          Good      Poor   #>  8    0.095         0.389           0.829 0.731 <          Good      Poor   #>  9    0.105         0.486           0.780 0.731 <          Good      Poor   #> 10    0.115         0.542           0.756 0.731 <          Good      Poor   #> # ℹ 41 more rows #> # ℹ 4 more variables: .n_controls <int>, .n_cases <int>, .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl> data_roc <- data_roc %>%    arrange(.sensitivities)  p_best <- ggplot(data_roc) +    aes(x = .specificities, y = .sensitivities) +    geom_abline(     slope = 1,      intercept = 1,      linetype = \"dotted\",      color = \"grey20\"   ) +   geom_step() +    geom_segment(     aes(xend = .specificities, yend = 1 - .specificities),     data = . %>% filter(.is_best_youden),     color = \"blue\",     linetype = \"dashed\"   ) +    geom_segment(     aes(xend = 1, yend = 1),     data = . %>% filter(.is_best_closest_topleft),     color = \"maroon\",     linetype = \"dashed\"   ) +    # Basically, finding a point 9/10ths of the way   # along the line   geom_text(     aes(       x = weighted.mean(c(1, .specificities), c(9, 1)),        y = weighted.mean(c(1, .sensitivities), c(9, 1)),      ),     data = . %>% filter(.is_best_closest_topleft),     color = \"maroon\",     label = \"closest to topleft\",     hjust = 0,      nudge_x = .02,     size = 5   ) +    geom_text(     aes(       x = .specificities,        y = weighted.mean(c(1 - .specificities, .sensitivities), c(1, 2)),      ),     data = . %>% filter(.is_best_youden),     color = \"blue\",     label = \"Youden's J\\n(max height above diagonal)\",     hjust = 0,     vjust = .5,     nudge_x = .02,     size = 5   ) +    annotate(     \"text\",     x = .91,     y = .05,     hjust = 0,      size = 5,     label = \"diagonal: random classifier\",     color = \"grey20\"   ) +   scale_x_reverse() +   coord_fixed() +   theme_grey(base_size = 12) p_best"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/roc.html","id":"smooth-density-roc-curves","dir":"Articles","previous_headings":"","what":"(Smooth) density ROC curves","title":"Tools for ROC curves","text":"Instead looking observed data, let’s assume s100b values group drawn normal distribution means scales (standard deviations) different two groups. can compute group’s mean standard deviation plot normal density curves top . Pepe (2003) refers approach “binormal ROC curve”.  various points along x-axis range, stat_function() compute dnorm() (density normal curves). can hand . take full range data, within group, generate set points along range compute group’s density point. Next, pivot wide pivot format comparing two densities point. pROC::roc() can compute ROC curve densities. Note interface different. provide dataframe names columns data frame. Instead, provide two vectors densities, fact, densities lost computing ROC curve.  roc object returns coordinates sensitivity decreasing order, obvious map sensitivities back original densities. terms earlier density plot, don’t know whether sensitivities move x axis x axis. Let’s restate problem , clarity: want map thresholds densities ROC coordinates map ROC coordinates back densities thresholds. pROC::roc(density.controls, density.controls), hit brick wall map backwards ROC coordinates sensitivites may reversed respect densities. Fortunately, compute sensitivities hand, can figure coordinates ordered. try orderings find one best matches one provided pROC::roc(). < direction better matched ROC results, conclude sensitivities follow order densities. compute_smooth_density_roc() uses similar heuristic determine order ROC coordinates respect original densities. result, can map original threshold values sensitivity specificity values. function also lets us use column names directly. compute_smooth_density_roc() also provides coordinates “best” thresholds Youden topleft criteria. consistency two functions, can just replace data used make annotated ROC curve smoothed ROC coordinates. case, Youden topleft points different.  final demonstration, let’s compare smooth empirical ROC sensitivity specificity values along threshold values.","code":"data_stats <- data %>%    group_by(outcome) %>%    summarise(     mean = mean(s100b),     sd = sd(s100b)   )   l_control <- data_stats %>%    filter(outcome == \"Good\") %>%    as.list()  l_case <- data_stats %>%    filter(outcome != \"Good\") %>%    as.list()  ggplot(data) +    aes(x = s100b, color = outcome) +    # include a \"rug\" at the bottom   geom_jitter(aes(y = -.2), width = 0, height = .15, alpha = .4) +   stat_function(     data = . %>% filter(outcome == \"Good\"),     fun = dnorm,      args = list(mean = l_control$mean, sd = l_control$sd)   ) +   stat_function(     data = . %>% filter(outcome != \"Good\"),     fun = dnorm,      args = list(mean = l_case$mean, sd = l_case$sd)   ) +   geom_text(     aes(x = mean, y = dnorm(mean, mean, sd), label = outcome),     data = data_stats,     vjust = \"inward\",     hjust = 0,     nudge_x = .05,     nudge_y = .05,     size = 4   ) +   theme_grey(14) +   theme(legend.position = \"top\", legend.justification = \"left\") +   labs(y = NULL) +   guides(color = \"none\") data_grid <- data %>%    mutate(     xmin = min(s100b),     xmax = max(s100b)   ) %>%    group_by(outcome) %>%    summarise(     x = seq(xmin[1], xmax[1], length.out = 200),     group_mean = mean(s100b),     group_sd = sd(s100b),     density = dnorm(x, group_mean, group_sd),     .groups = \"drop\"   )  #> Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in #> dplyr 1.1.0. #> ℹ Please use `reframe()` instead. #> ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()` #>   always returns an ungrouped data frame and adjust accordingly. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated. data_grid #> # A tibble: 400 × 5 #>    outcome      x group_mean group_sd density #>    <fct>    <dbl>      <dbl>    <dbl>   <dbl> #>  1 Good    0.03        0.162    0.131    1.84 #>  2 Good    0.0403      0.162    0.131    1.98 #>  3 Good    0.0505      0.162    0.131    2.13 #>  4 Good    0.0608      0.162    0.131    2.27 #>  5 Good    0.0710      0.162    0.131    2.40 #>  6 Good    0.0813      0.162    0.131    2.53 #>  7 Good    0.0915      0.162    0.131    2.64 #>  8 Good    0.102       0.162    0.131    2.75 #>  9 Good    0.112       0.162    0.131    2.84 #> 10 Good    0.122       0.162    0.131    2.91 #> # ℹ 390 more rows data_dens <- data_grid %>%    rename(s100b = x) %>%    select(-group_mean, -group_sd) %>%    pivot_wider(names_from = outcome, values_from = density) data_dens #> # A tibble: 200 × 3 #>     s100b  Good  Poor #>     <dbl> <dbl> <dbl> #>  1 0.03    1.84 0.659 #>  2 0.0403  1.98 0.676 #>  3 0.0505  2.13 0.694 #>  4 0.0608  2.27 0.711 #>  5 0.0710  2.40 0.729 #>  6 0.0813  2.53 0.746 #>  7 0.0915  2.64 0.763 #>  8 0.102   2.75 0.780 #>  9 0.112   2.84 0.797 #> 10 0.122   2.91 0.813 #> # ℹ 190 more rows data_dens <- arrange(data_dens, s100b) r_dens <- roc(   density.controls = data_dens$Good,    density.cases = data_dens$Poor ) r_dens #>  #> Call: #> roc.default(density.controls = data_dens$Good, density.cases = data_dens$Poor) #>  #> Data: (unknown) in 0 controls ((unknown) )  0 cases ((unknown) ). #> Smoothing: density with controls: data_dens$Good; and cases: data_dens$Poor #> Area under the curve: 0.8299 plot(r_dens) # direction > : Good > threshold >= Poor sens_gt <- rev(cumsum(data_dens$Poor) / sum(data_dens$Poor)) # direction < : Good < threshold <= Poor sens_lt <- 1 - (cumsum(data_dens$Poor) / sum(data_dens$Poor)) # The model did ?? fitted_sensitivities <- r_dens$sensitivities[-c(1, 201)]  mean(fitted_sensitivities - sens_lt) #> [1] 0.004999997 mean(fitted_sensitivities - sens_gt) #> [1] -0.530585 data_smooth <- compute_smooth_density_roc(   data = data_dens,    controls = Good,    cases = Poor,    along = s100b ) data_smooth #> # A tibble: 202 × 10 #>     s100b  Good  Poor .sensitivities .specificities  .auc .roc_row .direction #>     <dbl> <dbl> <dbl>          <dbl>          <dbl> <dbl>    <int> <chr>      #>  1 0.03    1.84 0.659          1             0      0.830        2 <          #>  2 0.0403  1.98 0.676          0.992         0.0221 0.830        3 <          #>  3 0.0505  2.13 0.694          0.984         0.0460 0.830        4 <          #>  4 0.0608  2.27 0.711          0.975         0.0716 0.830        5 <          #>  5 0.0710  2.40 0.729          0.967         0.0989 0.830        6 <          #>  6 0.0813  2.53 0.746          0.958         0.128  0.830        7 <          #>  7 0.0915  2.64 0.763          0.949         0.158  0.830        8 <          #>  8 0.102   2.75 0.780          0.939         0.190  0.830        9 <          #>  9 0.112   2.84 0.797          0.930         0.223  0.830       10 <          #> 10 0.122   2.91 0.813          0.920         0.257  0.830       11 <          #> # ℹ 192 more rows #> # ℹ 2 more variables: .is_best_youden <lgl>, .is_best_closest_topleft <lgl> p_best + list(data_smooth) ggplot(data_smooth) +    aes(x = s100b) +    geom_line(     aes(color = \"smooth\", linetype = \"smooth\", y = .sensitivities),   ) +    geom_line(     aes(color = \"empirical\", linetype = \"smooth\", y = .sensitivities),     data = data_roc   ) +    geom_line(     aes(color = \"smooth\", linetype = \"empirical\", y = .specificities)   ) +    geom_line(     aes(color = \"empirical\", linetype = \"empirical\", y = .specificities),     data = data_roc   ) +   annotate(\"text\", x = 2, y = .9, hjust = 1, label = \"specificity\") +    annotate(\"text\", x = 2, y = .1, hjust = 1, label = \"sensitivity\") +   labs(     color = \"ROC type\",      linetype = \"ROC type\",     y = NULL   ) +    theme_grey(base_size = 12) +    theme(legend.position = \"top\") #> Warning: Removed 2 rows containing missing values (`geom_line()`). #> Removed 2 rows containing missing values (`geom_line()`)."},{"path":"https://www.tjmahr.com/wisclabmisc/articles/utterance-length-imputation.html","id":"background","dir":"Articles","previous_headings":"","what":"Background","title":"Utterance length imputation and weighting","text":"Hustad colleagues (2020), modeled intelligibility data young children’s speech. Children hear utterance repeat . utterances started 2 words length, increased 3 words length, batches 10 sentences, way 7 words length. problem, however: children produce utterances every length. Specifically, child reliably produced 5 utterances given length length, task halted. given nature task, child produced 5-word utterances, also produced 2–4-word utterances well. , modeled/re-simulated version dataset, observe number children per utterance length decreases: length utterance plausibly influenced outcome variable: Longer utterances words might help listener understand sentence, example. Therefore, seem appropriate ignore missing values. used following two-step procedure (see Supplemental Materials detail): impute missing values utterance length using values shorter lengths, imputation stages, imputed value length L can used predictor L + 1. weight utterance length probability produced child given age take weighted average outcome variable across length. goal data preparation produce single-number intelligibility score, final weighted average provides number. procedure, missing data ignored implausible data (like longest utterances youngest ages) downweighted.","code":"library(tidyverse)  data_demo |>    count(tocs_level) #> # A tibble: 7 × 2 #>   tocs_level     n #>        <int> <int> #> 1          1   164 #> 2          2   164 #> 3          3   162 #> 4          4   102 #> 5          5    47 #> 6          6    30 #> 7          7    24"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/utterance-length-imputation.html","id":"imputation","dir":"Articles","previous_headings":"","what":"Imputation","title":"Utterance length imputation and weighting","text":"flow code consists following steps: reshape data wide format, train series linear models trained observed data, predict responses missing values reshape back long format recent non-package version function: implementation unsatisfactory number reasons. hardcodes many unneeded variables, apply example dataset, add filler values variables. also hardcodes number models predictors imputation model. y_1 lm() indicates scores single-word trials used imputation. inclusion may may appropriate. speaking rate studies, doesn’t make sense use include data single-word trials. , caveats aside, works: can recreate Figure 3 supplemental materials: Results imputing multiword intelligibility using length--longest utterance average intelligibilities shorter utterance lengths. comparison, package version procedure. specify relevant variables ahead time, number models variables involved longer hard-coded. original models, length longest utterance used continuous predictor imputations. package version behavior optional disabled default. affect results noticeably.","code":"impute_values <- function(data, var, data_train = NULL) {   spec <- build_wider_spec_for_imputation(data, {{ var }})   data_wide <- pivot_wider_for_imputation(data, spec)    if (is.null(data_train)) {     data_train <- data   }    data_wide_train <- pivot_wider_for_imputation(data_train, spec)   models <- fit_imputation_models(data_wide_train)    data_imputed <- data_wide |>     mutate(       y_3 = ifelse(is.na(y_3), predict(models$m_3, pick(everything())), y_3)     ) |>     mutate(       y_4 = ifelse(is.na(y_4), predict(models$m_4, pick(everything())), y_4)     ) |>     mutate(       y_5 = ifelse(is.na(y_5), predict(models$m_5, pick(everything())), y_5)     ) |>     mutate(       y_6 = ifelse(is.na(y_6), predict(models$m_6, pick(everything())), y_6)     ) |>     mutate(       y_7 = ifelse(is.na(y_7), predict(models$m_7, pick(everything())), y_7)     )    # handle these separately because `length_longest` could have been imputed if   # there was a different strategy   data_child_ll <- data_wide |>     distinct(group, subject_num, visit_id, age_months, length_longest)    data_original_values <- data_wide |>     tidyr::pivot_longer_spec(spec) |>     distinct(group, subject_num, visit_id, tocs_level, {{ var }})    d <- data_imputed |>     tidyr::pivot_longer_spec(spec) |>     rename(\"imputed_{{ var }}\" := {{ var }}) |>     select(-length_longest) |>     left_join(       data_child_ll,       by = c(\"group\", \"subject_num\", \"visit_id\", \"age_months\")     ) |>     left_join(       data_original_values,       by = c(\"group\", \"subject_num\", \"visit_id\", \"tocs_level\")     ) |>     mutate(       imputed = ifelse(is.na({{ var }}), \"imputed\", \"observed\"),       facet_lab = paste0(tocs_level, \" words\")     )    d }  fit_imputation_models <- function(data) {   list(     m_7 = lm(y_7 ~ y_1 + y_2 + y_3 + y_4 + y_5 + y_6, data),     m_6 = lm(y_6 ~ y_1 + y_2 + y_3 + y_4 + y_5 + length_longest, data),     m_5 = lm(y_5 ~ y_1 + y_2 + y_3 + y_4 + length_longest, data),     m_4 = lm(y_4 ~ y_1 + y_2 + y_3 + length_longest, data),     m_3 = lm(y_3 ~ y_1 + y_2 + length_longest, data)   ) }  build_wider_spec_for_imputation <- function(data, var) {   tidyr::build_wider_spec(     data,     names_from = tocs_level,     names_prefix = \"y_\",     values_from = {{ var }}   ) }  pivot_wider_for_imputation <- function(data, spec) {   data |>     tidyr::pivot_wider_spec(       spec,       id_cols = c(         group, subject_num, visit_id, length_longest, age_months       )     ) } data_imputation_1 <- data_demo |>   mutate(group = \"fake\", visit_id = 1) |>   rename(subject_num = child) |>    impute_values(sim_intelligibility) #> Warning: There was 1 warning in `mutate()`. #> ℹ In argument: `y_7 = ifelse(is.na(y_7), predict(models$m_7, #>   pick(everything())), y_7)`. #> Caused by warning in `predict.lm()`: #> ! prediction from rank-deficient fit; attr(*, \"non-estim\") has doubtful cases plotting_constants <- list(   pal = c(     imputed = \"#C7A76C\",      observed = \"#7DB0DD\",      \"mean ± SE\" = \"grey30\"   ),   guides_pal = guides(     shape = \"none\",     color = guide_legend(       title = NULL,       override.aes = list(         alpha = 1,         shape = c(16, 17, 16),         linetype = c(\"blank\", \"blank\", \"solid\")       )     )   ) )   plotting_constants$scale_pal <- scale_color_manual(   values = plotting_constants$pal,    limits = names(plotting_constants$pal) )  set.seed(100) ggplot(data_imputation_1 |> filter(tocs_level != 1)) +    aes(     x = length_longest,      y = imputed_sim_intelligibility,      color = imputed,      shape = imputed   ) +    geom_jitter(width = .3, alpha = .5, height = 0) +   stat_summary(     aes(group = length_longest, color = \"mean ± SE\"),      fun.data = mean_se   ) +    plotting_constants$scale_pal +    plotting_constants$guides_pal +   facet_wrap(\"facet_lab\") +    scale_y_continuous(     \"Intelligibility\",      labels = scales::percent_format(1)   ) +   labs(     x = \"Length of longest utterance (observed)\"   ) +    theme(legend.position = \"bottom\", legend.justification = \"right\") data_imputation_2 <- data_demo |>    impute_values_by_length(     var_y = sim_intelligibility,     var_length = tocs_level,     id_cols = c(child, age_months, length_longest),      include_max_length = TRUE   ) #> Warning in predict.lm(models[[y_name]], data_imputed): prediction from #> rank-deficient fit; attr(*, \"non-estim\") has doubtful cases  all.equal(   data_imputation_1$imputed_sim_intelligibility,    data_imputation_2$sim_intelligibility_imputed ) #> [1] TRUE data_imputation_3 <- data_demo |>    impute_values_by_length(     var_y = sim_intelligibility,     var_length = tocs_level,     id_cols = c(child, age_months, length_longest),      include_max_length = FALSE   ) #> Warning in predict.lm(models[[y_name]], data_imputed): prediction from #> rank-deficient fit; attr(*, \"non-estim\") has doubtful cases  all.equal(   data_imputation_2$sim_intelligibility_imputed,    data_imputation_3$sim_intelligibility_imputed ) #> [1] \"Mean relative difference: 8.473438e-05\"  cor(   data_imputation_2$sim_intelligibility_imputed,    data_imputation_3$sim_intelligibility_imputed ) #> [1] 1  ggplot(data_imputation_2 |> filter(tocs_level != 1)) +    aes(     x = length_longest,      y = sim_intelligibility_imputed,      color = sim_intelligibility_imputation,      shape = sim_intelligibility_imputation   ) +    geom_jitter(width = .3, alpha = .5, height = 0) +   stat_summary(     aes(group = length_longest, color = \"mean ± SE\"),      fun.data = mean_se   ) +    plotting_constants$scale_pal +    plotting_constants$guides_pal +   facet_wrap(\"tocs_level\") +    scale_y_continuous(     \"Intelligibility\",      labels = scales::percent_format(1)   ) +   labs(     x = \"Length of longest utterance (observed)\"   ) +    theme(legend.position = \"bottom\", legend.justification = \"right\")"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/utterance-length-imputation.html","id":"utterance-weighting","dir":"Articles","previous_headings":"","what":"Utterance weighting","title":"Utterance length imputation and weighting","text":"recent non-package version utterance length weighting code. basic steps preparing dataset fitting ordinal regression model length longest utterance using age predictor computing probabilities utterance length function age Originally, used MASS::polr() models think ordinal::clm() easier use. can apply non-package implementation example data: can recreate Figure supplemental materials:  implementation somewhat unsatisfactory. hardcodes variable names, ’s harder reuse, complicated use, requiring three functions join. package implementation, hide everything behind single function. can see match weights two implementations.","code":"prepare_longest_length_model_data <- function(data) {   data |>     group_by(child) |>     mutate(       length_longest = max(tocs_level)     ) |>     ungroup() |>     distinct(child, tocs_level, age_months, length_longest) |>     filter(tocs_level == length_longest) |>     mutate(       lol = factor(length_longest),       lol_ord = ordered(lol)     ) }  fit_longest_length_model <- function(data, df = 2) {   ordinal::clm(     lol_ord ~ splines::ns(age_months, df = df),     data = data   ) }  compute_longest_length_weights <- function(data, model) {   d_ages <- data |>     distinct(age_months) |>     mutate(       # predicted longest length by age       lol_predicted = model |>         predict(newdata = pick(everything()), type = \"class\") |>         getElement(\"fit\")     )    d_ages |>     predict(model, newdata = _, type = \"prob\") |>     getElement(\"fit\") |>     bind_cols(d_ages) |>     tidyr::pivot_longer(       cols = -c(age_months, lol_predicted),       names_to = \"lol\",       values_to = \"lol_prob\"     ) |>     group_by(age_months) |>     mutate(       lol_num = as.numeric(as.character(lol)),       lol_weighted = lol_prob * lol_num     ) |>     arrange(-lol_num) |>     mutate(       prob_reach_length = cumsum(lol_prob),       normalized_prob_reach_length = prob_reach_length / sum(prob_reach_length)     ) |>     ungroup() } data_lol <- data_demo |>    filter(tocs_level != 1) |>    prepare_longest_length_model_data()  m <- fit_longest_length_model(data_lol, df = 2)  data_lol_weights <- data_lol |>    compute_longest_length_weights(model = m) |>   distinct(     age_months,      tocs_level = lol_num,      prob_reach_length,      normalized_prob_reach_length   ) p1 <- ggplot(data_lol_weights) +    aes(x = age_months, y = prob_reach_length  ) +    geom_line(aes(color = ordered(tocs_level))) +   geom_text(     aes(label = tocs_level, color = ordered(tocs_level)),     x = 29,     data = data_lol_weights |>        filter(age_months == 30, tocs_level %in% c(2, 3, 4, 7)),      fontface = \"bold\"   ) +   geom_text(     aes(label = tocs_level, color = ordered(tocs_level)),     x = 48,     data = data_lol_weights |>        filter(age_months == 47, tocs_level %in% c(2, 5, 6, 7)),      fontface = \"bold\"   ) +   scale_color_ordinal(end = .85) +   guides(color = \"none\") +    labs(x = \"Age [months]\", y = \"Prob. of reaching length\") +   scale_x_continuous(limits = c(28, 49)) p1  p2 <- p1 +    aes(y = normalized_prob_reach_length) +    labs(y = \"Weight of utterance length\") p2 data_plot <- weight_lengths_with_ordinal_model(   data_train = data_demo |>      filter(tocs_level != 1),    var_length = tocs_level,   var_x = age_months,    id_cols = c(child),    spline_df = 2 ) all.equal(   data_lol_weights$normalized_prob_reach_length,    data_plot$tocs_level_weight ) #> [1] TRUE  all.equal(   data_lol_weights$prob_reach_length,    data_plot$tocs_level_prob_reached ) #> [1] TRUE"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/utterance-length-imputation.html","id":"combining-the-imputation-and-the-weights","dir":"Articles","previous_headings":"","what":"Combining the imputation and the weights","title":"Utterance length imputation and weighting","text":"last step process combine imputed values length weights, take means imputed values. step straightforward table join. Although used single-word scores imputation (highly correlated intelligibility scores), use weighting. target variable single-number summary intelligibility 2–7-word utterances, exclude 1-word scores. can automate joining step. weight_lengths_with_ordinal_model() takes optional data_join argument specifying dataframe estimated weights joined onto. argument intended dataframe imputed values. pass imputed values data_train missing values imputed, everyone length longest utterance. can see results methods. three values overall similar:  importantly, imputation negatively biased younger children reach longer utterance lengths. Points fall diagonal two types scores compared, difference two scores negative younger ages.   weight utterance length probability, score tighter correlation observed, un-adjusted scores. pointwise differences weighted observed values small, ±2 percentage points intelligibility.","code":"data_weighted_1 <- data_imputation_1 |>    filter(tocs_level != 1) |>    left_join(data_lol_weights, by = join_by(age_months, tocs_level))   data_means_wide_1 <- data_weighted_1 |>    group_by(subject_num, age_months) |>    summarise(     observed = mean(sim_intelligibility, na.rm = TRUE),     imputed = mean(imputed_sim_intelligibility),     weighted = weighted.mean(       imputed_sim_intelligibility,        normalized_prob_reach_length     ),     .groups = \"drop\"   )  data_means_wide_1 #> # A tibble: 164 × 5 #>    subject_num age_months observed imputed weighted #>    <chr>            <int>    <dbl>   <dbl>    <dbl> #>  1 c001                45    0.533   0.530    0.531 #>  2 c002                42    0.389   0.337    0.360 #>  3 c003                41    0.544   0.532    0.540 #>  4 c004                42    0.660   0.660    0.655 #>  5 c005                43    0.707   0.725    0.710 #>  6 c006                42    0.524   0.513    0.523 #>  7 c007                32    0.355   0.307    0.350 #>  8 c008                43    0.828   0.828    0.832 #>  9 c009                38    0.868   0.873    0.869 #> 10 c010                47    0.767   0.767    0.765 #> # ℹ 154 more rows data_weighted_2 <- weight_lengths_with_ordinal_model(   data_train = data_demo |> filter(tocs_level != 1),    var_length = tocs_level,   var_x = age_months,    id_cols = c(child),    spline_df = 2,   data_join = data_imputation_2 |> filter(tocs_level != 1) )  data_means_wide_2 <- data_weighted_2 |>    group_by(child, age_months) |>    summarise(     observed = mean(sim_intelligibility, na.rm = TRUE),     imputed = mean(sim_intelligibility_imputed),     weighted = weighted.mean(sim_intelligibility_imputed, tocs_level_weight),     .groups = \"drop\"   ) all.equal(data_means_wide_1$observed, data_means_wide_2$observed) #> [1] TRUE all.equal(data_means_wide_1$imputed,  data_means_wide_2$imputed) #> [1] TRUE all.equal(data_means_wide_1$weighted, data_means_wide_2$weighted) #> [1] TRUE data_means <- data_means_wide_2 |>    tidyr::pivot_longer(     cols = c(-child, -age_months),     names_to = \"score\",     values_to = \"intelligibility\"   )  ggplot(data_means) +    aes(x = age_months, y = intelligibility, color = score) +    geom_point(alpha = .4) ggplot(data_means_wide_2) +    aes(x = observed, y = imputed) +   geom_abline() +   geom_point() ggplot(data_means_wide_2) +    aes(x = age_months, y = imputed - observed) +   geom_hline(yintercept = 0, linewidth = 2, color = \"white\") +   geom_point() ggplot(data_means_wide_2) +    aes(x = observed, y = weighted) +   geom_abline() +   geom_point() ggplot(data_means_wide_2) +    aes(x = age_months, y = weighted - observed) +   geom_hline(yintercept = 0, linewidth = 2, color = \"white\") +   geom_point()"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/utterance-length-imputation.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Utterance length imputation and weighting","text":"Hustad, K. C., Mahr, T., Natzke, P. E. M., & Rathouz, P. J. (2020). Development Speech Intelligibility 30 47 Months Typically Developing Children: Cross-Sectional Study Growth. Journal Speech, Language, Hearing Research, 63(6), 1675–1687. https://doi.org/10.1044/2020_JSLHR-20-00008 Hustad, K. C., Mahr, T., Natzke, P. E. M., & J. Rathouz, P. (2020). Supplemental Material S1 (Hustad et al., 2020). ASHA journals. https://doi.org/10.23641/asha.12330956.v1","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Tristan Mahr. Author, maintainer.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Mahr T (2023). wisclabmisc: Tools Support 'WiscLab'. R package version 0.1.0,  https://www.tjmahr.com/wisclabmisc/, https://github.com/tjmahr/wisclabmisc.","code":"@Manual{,   title = {wisclabmisc: Tools to Support the 'WiscLab'},   author = {Tristan Mahr},   year = {2023},   note = {R package version 0.1.0,  https://www.tjmahr.com/wisclabmisc/},   url = {https://github.com/tjmahr/wisclabmisc}, }"},{"path":"https://www.tjmahr.com/wisclabmisc/index.html","id":"wisclabmisc","dir":"","previous_headings":"","what":"Tools to Support the WiscLab","title":"Tools to Support the WiscLab","text":"goal wisclabmisc reuse analysis functions across WISC Lab project projects provide share analysis code used projects.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Tools to Support the WiscLab","text":"can install development version wisclabmisc GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"tjmahr/wisclabmisc\")"},{"path":"https://www.tjmahr.com/wisclabmisc/index.html","id":"acknowledgments","dir":"","previous_headings":"","what":"Acknowledgments","title":"Tools to Support the WiscLab","text":"wisclabmisc created process data WISC Lab project. Thus, development package supported NIH R01DC009411 NIH R01DC015653.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert age in months to years;months — format_year_month_age","title":"Convert age in months to years;months — format_year_month_age","text":"Convert age months years;months","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert age in months to years;months — format_year_month_age","text":"","code":"format_year_month_age(x)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert age in months to years;months — format_year_month_age","text":"x vector ages months","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert age in months to years;months — format_year_month_age","text":"ages years;months format","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert age in months to years;months — format_year_month_age","text":"Ages NA return \"NA;NA\". format default numerically ordered. means c(\"2;0\", \"10;10\", \"10;9\") sort c(\"10;10\", \"10;9\", \"2;0\"). function stringr::str_sort(..., numeric = TRUE) sort vector correctly.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert age in months to years;months — format_year_month_age","text":"","code":"ages <- c(26, 58, 25, 67, 21, 59, 36, 43, 27, 49) format_year_month_age(ages) #>  [1] \"2;2\"  \"4;10\" \"2;1\"  \"5;7\"  \"1;9\"  \"4;11\" \"3;0\"  \"3;7\"  \"2;3\"  \"4;1\""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"function fits type GAMLSS model used Hustad colleagues (2021) 🔓: beta regression model (via gamlss.dist::()) natural cubic splines mean (mu) scale (sigma). model fitted using package's mem_gamlss() wrapper function.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"","code":"fit_beta_gamlss(data, var_x, var_y, df_mu = 3, df_sigma = 2, control = NULL)  fit_beta_gamlss_se(   data,   name_x,   name_y,   df_mu = 3,   df_sigma = 2,   control = NULL )  predict_beta_gamlss(newdata, model, centiles = c(5, 10, 50, 90, 95))  optimize_beta_gamlss_slope(   model,   centiles = 50,   interval = c(30, 119),   maximum = TRUE )  uniroot_beta_gamlss(model, centiles = 50, targets = 0.5, interval = c(30, 119))"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"Associated article: https://doi.org/10.1044/2021_JSLHR-21-00142","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"data data frame var_x, var_y (unquoted) variable names giving predictor variable (e.g., age) outcome variable (.e.g, intelligibility). df_mu, df_sigma degrees freedom. 0 used, splines::ns() term dropped model formula parameter. control gamlss::gamlss.control() controller. Defaults NULL uses default settings, except setting trace FALSE silence output gamlss. name_x, name_y quoted variable names giving predictor variable (e.g., \"age\") outcome variable (.e.g, \"intelligibility\"). arguments apply fit_beta_gamlss_se(). newdata one-column dataframe predictions model model fitted fit_beta_gamlss() centiles centiles use prediction. Defaults c(5, 10, 50, 90, 95) predict_beta_gamlss(). Defaults 50 optimize_beta_gamlss_slope() uniroot_beta_gamlss(), although functions support multiple centile values. interval optimize_beta_gamlss_slope(), range x values optimize . uniroot_beta_gamlss(), range x values search roots (target y values) . maximum optimize_beta_gamlss_slope(), whether find maximum slope (TRUE) minimum slope (FALSE). targets uniroot_beta_gamlss(), target y values use roots. default, .5 used, uniroot_beta_gamlss() returns x value y value .5. Multiple targets supported.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"fit_beta_gamlss() fit_beta_gamlss_se(), mem_gamlss()-fitted model. .user data model includes degrees freedom parameter splines::ns() basis parameter. predict_beta_gamlss(), dataframe containing model predictions mu sigma, plus columns centile centiles. optimize_beta_gamlss_slope(), dataframe optimized x values (maximum minimum), gradient x value (objective), quantile (quantile). uniroot_beta_gamlss(), dataframe one row per quantile/target combination results calling stats::uniroot(). root column x value quantile curve crosses target value.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"two versions function. main version fit_beta_gamlss(), works unquoted column names (e.g., age). alternative version fit_beta_gamlss_se(); final \"se\" stands \"Standard Evaluation\". designation means variable names must given strings (, quoted \"age\" instead bare name age). alternative version necessary fit several models using parallel computing furrr::future_map() (using bootstrap resampling). predict_centiles() work function, likely throw warning message. Therefore, predict_beta_gamlss() provides alternative way compute centiles model. function manually computes centiles instead relying gamlss::centiles(). main difference new x values go splines::predict.ns() multiplied model coefficients. optimize_beta_gamlss_slope() computes point (.e., age) rate steepest growth different quantiles. function wraps following process: internal prediction function computes quantile x model coefficients spline bases. another internal function uses numDeriv::grad() get gradient prediction function x. optimize_beta_gamlss_slope() uses stats::optimize() gradient function find x maximum minimum slope. uniroot_beta_gamlss() also uses internal prediction function find quantile growth curve crosses given value. stats::uniroot() finds function crosses 0 (root). modify prediction function always subtract .5 end, root prediction function x value predicted value crosses .5. idea behind uniroot_beta_gamlss() works. work, use approach find, say, age (root) children 10th percentile (centiles) cross 50% intelligibility (targets).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"gamlss-does-beta-regression-differently","dir":"Reference","previous_headings":"","what":"GAMLSS does beta regression differently","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"part brief note GAMLSS uses different parameterization beta distribution beta family packages. canonical parameterization beta distribution uses shape parameters \\(\\alpha\\) \\(\\beta\\) probability density function: $$f(y;\\alpha,\\beta) = \\frac{1}{B(\\alpha,\\beta)} y^{\\alpha-1}(1-y)^{\\beta-1}$$ \\(B\\) beta function. beta regression, distribution reparameterized mean probability \\(\\mu\\) parameter represents spread around mean. GAMLSS (gamlss.dist::()), use scale parameter \\(\\sigma\\) (larger values mean spread around mean). Everywhere else—betareg::betareg() rstanarm::stan_betareg() vignette(\"betareg\", \"betareg\"), brms::Beta() vignette(\"brms_families\", \"brms\"), mgcv::betar()—precision parameter \\(\\phi\\) (larger values mean precision, less spread around mean). comparison: $$  \\text{betareg, brms, mgcv, etc.} \\\\  \\mu              = \\alpha / (\\alpha + \\beta) \\\\  \\phi             = \\alpha + b \\\\  \\textsf{E}(y)    = \\mu \\\\  \\textsf{VAR}(y)  = \\mu(1-\\mu)/(1 + \\phi) \\\\ $$ $$  \\text{GAMLSS} \\\\  \\mu             = \\alpha / (\\alpha + \\beta) \\\\  \\sigma          = (1 / (\\alpha + \\beta + 1))^.5 \\\\  \\textsf{E}(y)   = \\mu \\\\  \\textsf{VAR}(y) = \\mu(1-\\mu)\\sigma^2 $$","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"","code":"data_fake_intelligibility #> # A tibble: 200 × 2 #>    age_months intelligibility #>         <int>           <dbl> #>  1         28           0.539 #>  2         29           0.375 #>  3         31           0.221 #>  4         31           0.253 #>  5         32           0.276 #>  6         32           0.750 #>  7         32           0.820 #>  8         33           0.325 #>  9         33           0.446 #> 10         33           0.592 #> # ℹ 190 more rows  m <- fit_beta_gamlss(   data_fake_intelligibility,   age_months,   intelligibility )  # using \"qr\" in summary() just to suppress a warning message summary(m, type = \"qr\") #> ****************************************************************** #> Family:  c(\"BE\", \"Beta\")  #>  #> Call:  gamlss::gamlss(formula = intelligibility ~ ns(age_months, df = 3),   #>     sigma.formula = ~ns(age_months, df = 2), family = BE(), data = ~data_fake_intelligibility,   #>     control = list(c.crit = 0.001, n.cyc = 20, mu.step = 1, sigma.step = 1,   #>         nu.step = 1, tau.step = 1, gd.tol = Inf, iter = 0, trace = FALSE,   #>         autostep = TRUE, save = TRUE))  #>  #> Fitting method: RS()  #>  #> ------------------------------------------------------------------ #> Mu link function:  logit #> Mu Coefficients: #>                         Estimate Std. Error t value Pr(>|t|)     #> (Intercept)              -0.3414     0.2090  -1.634    0.104     #> ns(age_months, df = 3)1   2.4951     0.1896  13.162   <2e-16 *** #> ns(age_months, df = 3)2   5.0171     0.4960  10.116   <2e-16 *** #> ns(age_months, df = 3)3   3.1454     0.1746  18.017   <2e-16 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ------------------------------------------------------------------ #> Sigma link function:  logit #> Sigma Coefficients: #>                         Estimate Std. Error t value Pr(>|t|)     #> (Intercept)              -0.4380     0.1935  -2.264   0.0247 *   #> ns(age_months, df = 2)1  -1.8670     0.4175  -4.472 1.31e-05 *** #> ns(age_months, df = 2)2  -1.6193     0.2120  -7.639 9.29e-13 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ------------------------------------------------------------------ #> No. of observations in the fit:  200  #> Degrees of Freedom for the fit:  7 #>       Residual Deg. of Freedom:  193  #>                       at cycle:  8  #>   #> Global Deviance:     -510.3592  #>             AIC:     -496.3592  #>             SBC:     -473.271  #> ******************************************************************  # Alternative interface d <- data_fake_intelligibility m2 <- fit_beta_gamlss_se(   data = d,   name_x = \"age_months\",   name_y = \"intelligibility\" ) coef(m2) == coef(m) #>             (Intercept) ns(age_months, df = 3)1 ns(age_months, df = 3)2  #>                    TRUE                    TRUE                    TRUE  #> ns(age_months, df = 3)3  #>                    TRUE   # how to use control to change gamlss() behavior m_traced <- fit_beta_gamlss(   data_fake_intelligibility,   age_months,   intelligibility,   control = gamlss::gamlss.control(n.cyc = 15, trace = TRUE) ) #> GAMLSS-RS iteration 1: Global Deviance = -394.1232  #> GAMLSS-RS iteration 2: Global Deviance = -456.3473  #> GAMLSS-RS iteration 3: Global Deviance = -489.514  #> GAMLSS-RS iteration 4: Global Deviance = -506.4194  #> GAMLSS-RS iteration 5: Global Deviance = -510.1204  #> GAMLSS-RS iteration 6: Global Deviance = -510.3517  #> GAMLSS-RS iteration 7: Global Deviance = -510.359  #> GAMLSS-RS iteration 8: Global Deviance = -510.3592   # The `.user` space includes the spline bases, so that we can make accurate # predictions of new xs. names(m$.user) #> [1] \"data\"         \"session_info\" \"call\"         \"df_mu\"        \"df_sigma\"     #> [6] \"basis_mu\"     \"basis_sigma\"   # predict logit(mean) at 55 months: logit_mean_55 <- cbind(1, predict(m$.user$basis_mu, 55)) %*% coef(m) logit_mean_55 #>          [,1] #> [1,] 1.627416 stats::plogis(logit_mean_55) #>           [,1] #> [1,] 0.8358153  # But predict_gen_gamma_gamlss() does this work for us and also provides # centiles new_ages <- data.frame(age_months = 48:71) centiles <- predict_beta_gamlss(new_ages, m) centiles #> # A tibble: 24 × 8 #>    age_months    mu sigma    c5   c10   c50   c90   c95 #>         <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #>  1         48 0.761 0.296 0.527 0.586 0.778 0.912 0.936 #>  2         49 0.773 0.291 0.547 0.605 0.791 0.918 0.941 #>  3         50 0.785 0.286 0.566 0.623 0.803 0.924 0.946 #>  4         51 0.796 0.281 0.584 0.640 0.814 0.929 0.950 #>  5         52 0.807 0.276 0.602 0.656 0.824 0.934 0.953 #>  6         53 0.817 0.271 0.619 0.672 0.834 0.938 0.957 #>  7         54 0.827 0.266 0.636 0.687 0.844 0.943 0.960 #>  8         55 0.836 0.261 0.652 0.702 0.852 0.946 0.963 #>  9         56 0.844 0.256 0.668 0.716 0.861 0.950 0.965 #> 10         57 0.852 0.251 0.683 0.730 0.868 0.953 0.968 #> # ℹ 14 more rows  # Confirm that the manual prediction matches the automatic one centiles[centiles$age_months == 55, \"mu\"] #> # A tibble: 1 × 1 #>      mu #>   <dbl> #> 1 0.836 stats::plogis(logit_mean_55) #>           [,1] #> [1,] 0.8358153  if(requireNamespace(\"ggplot2\", quietly = TRUE)) {   library(ggplot2)   ggplot(pivot_centiles_longer(centiles)) +     aes(x = age_months, y = .value) +     geom_line(aes(group = .centile, color = .centile_pair)) +     geom_point(       aes(y = intelligibility),       data = subset(         data_fake_intelligibility,         48 <= age_months & age_months <= 71       )     ) }   # Age of steepest growth for each centile optimize_beta_gamlss_slope(   model = m,   centiles = c(5, 10, 50, 90),   interval = range(data_fake_intelligibility$age_months) ) #> # A tibble: 4 × 3 #>   maximum objective quantile #>     <dbl>     <dbl>    <dbl> #> 1    40.0    0.0222     0.05 #> 2    37.7    0.0225     0.1  #> 3    29.8    0.0217     0.5  #> 4    28.0    0.0153     0.9   # Manual approach: Make fine grid of predictions and find largest jump centiles_grid <- predict_beta_gamlss(   newdata = data.frame(age_months = seq(28, 95, length.out = 1000)),   model = m ) centiles_grid[which.max(diff(centiles_grid$c5)), \"age_months\"] #> # A tibble: 1 × 1 #>   age_months #>        <dbl> #> 1       39.9  # When do children in different centiles reach 50%, 70% intelligibility? uniroot_beta_gamlss(   model = m,   centiles = c(5, 10, 50),   targets = c(.5, .7) ) #> # A tibble: 6 × 7 #>   quantile target  root    f.root  iter init.it estim.prec #>      <dbl>  <dbl> <dbl>     <dbl> <int>   <int>      <dbl> #> 1     0.05    0.5  46.7 -3.31e-11     7      NA  0.0000610 #> 2     0.1     0.5  43.7  1.38e- 7     6      NA  0.0000610 #> 3     0.5     0.5  32.4 -1.91e- 9     5      NA  0.0000610 #> 4     0.05    0.7  58.2 -1.23e-11     7      NA  0.0000610 #> 5     0.1     0.7  54.8 -1.72e- 8     7      NA  0.0000610 #> 6     0.5     0.7  42.7 -2.51e- 7     7      NA  0.0000610"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/check_sample_centiles.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the percentage of points under each centile line — check_sample_centiles","title":"Compute the percentage of points under each centile line — check_sample_centiles","text":"Compute percentage points centile line","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/check_sample_centiles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the percentage of points under each centile line — check_sample_centiles","text":"","code":"check_sample_centiles(   data,   model,   var_x,   var_y,   centiles = c(5, 10, 25, 50, 75, 90, 95) )"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/check_sample_centiles.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the percentage of points under each centile line — check_sample_centiles","text":"data dataset used fit model. dataframe grouped dplyr::group_by(), sample centiles computed group. model gamlss model prepared mem_gamlss() var_x, var_y bare column names predictor outcome variables centiles centiles use prediction. Defaults c(5, 10, 25, 50, 75, 90, 95).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/check_sample_centiles.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the percentage of points under each centile line — check_sample_centiles","text":"tibble number points percentage points less equal quantile value.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/chrono_age.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute chronological age in months — chrono_age","title":"Compute chronological age in months — chrono_age","text":"Ages rounded nearest month. difference 20 months, 29 days interpreted 20 months.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/chrono_age.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute chronological age in months — chrono_age","text":"","code":"chrono_age(t1, t2)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/chrono_age.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute chronological age in months — chrono_age","text":"t1, t2 dates \"yyyy-mm-dd\" format","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/chrono_age.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute chronological age in months — chrono_age","text":"chronological ages months. NA returned age computed.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/chrono_age.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute chronological age in months — chrono_age","text":"","code":"# Two years exactly chrono_age(\"2014-01-20\", \"2012-01-20\") #> [1] 24 #> 24  # Shift a year chrono_age(\"2014-01-20\", \"2013-01-20\") #> [1] 12 #> 12 chrono_age(\"2014-01-20\", \"2011-01-20\") #> [1] 36 #> 36  # Shift a month chrono_age(\"2014-01-20\", \"2012-02-20\") #> [1] 23 #> 23 chrono_age(\"2014-01-20\", \"2011-12-20\") #> [1] 25 #> 25  # 3 months exactly chrono_age(\"2014-05-10\", \"2014-02-10\") #> [1] 3 #> 3  # Borrow a month when the earlier date has a later day chrono_age(\"2014-05-10\", \"2014-02-11\") #> [1] 2 #> 2, equal to 2 months, 29 days rounded down to nearest month  # Inverted argument order chrono_age(\"2012-01-20\", \"2014-01-20\") #> [1] 24 #> 24  # Multiple dates t1 <- c(\"2012-01-20\", \"2014-02-10\", \"2010-10-10\") t2 <- c(\"2014-01-20\", \"2014-05-10\", \"2014-11-10\") chrono_age(t1, t2) #> [1] 24  3 49 #> [1] 24  3 49"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_empirical_roc.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an ROC curve from observed data — compute_empirical_roc","title":"Create an ROC curve from observed data — compute_empirical_roc","text":"Create ROC curve observed data","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_empirical_roc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an ROC curve from observed data — compute_empirical_roc","text":"","code":"compute_empirical_roc(   data,   response,   predictor,   direction = \"auto\",   best_weights = c(1, 0.5),   ... )"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_empirical_roc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an ROC curve from observed data — compute_empirical_roc","text":"data dataframe containing responses (groupings) predictor variable response bare column name group status (control vs. cases) predictor bare column name predictor use classification direction direction set pROC::roc(). Defaults \"auto\". best_weights weights computing best ROC curve points. Defaults c(1, .5), defaults used pROC::coords(). ... additional arguments passed pROC::roc().","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_empirical_roc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an ROC curve from observed data — compute_empirical_roc","text":"new dataframe ROC coordinates returned columns predictor variable, .sensitivities, .specificities, .auc, .direction, .controls, .cases, .n_controls, .n_cases, .is_best_youden .is_best_closest_topleft.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_empirical_roc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an ROC curve from observed data — compute_empirical_roc","text":"","code":"set.seed(100) x1 <- rnorm(100, 4, 1) x2 <- rnorm(100, 2, .5) both <- c(x1, x2) steps <- seq(min(both), max(both), length.out = 200) d1 <- dnorm(steps, mean(x1), sd(x1)) d2 <- dnorm(steps, mean(x2), sd(x2)) data <- tibble::tibble(   y = steps,   d1 = d1,   d2 = d2,   outcome = rbinom(200, 1, prob = 1 - (d1 / (d1 + d2))),   group = ifelse(outcome, \"case\", \"control\") )  # get an ROC on the fake data compute_empirical_roc(data, outcome, y) #> Setting levels: control = 0, case = 1 #> Setting direction: controls > cases # this guess the cases and controls from the group name and gets it wrong compute_empirical_roc(data, group, y) #> Setting levels: control = case, case = control #> Setting direction: controls < cases # better compute_empirical_roc(data, group, y, levels = c(\"control\", \"case\")) #> Setting direction: controls > cases"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute positive and negative predictive value — compute_predictive_value_from_rates","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"Compute positive negative predictive value","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"","code":"compute_predictive_value_from_rates(sensitivity, specificity, prevalence)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"sensitivity, specificity, prevalence vectors confusion matrix rates","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"tibble columns sensitivity, specificity, prevalence, ppv, npv ppv npv positive predictive value negative predictive value.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"vectors passed function common length /length 1. example, 4 sensitivities, 4 specificities 1 incidence work sensitivities specificities common length can safely recycle (reuse) incidence value. 4 sensitivities, 2 specificities, 1 incidence fail common length.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"","code":"compute_predictive_value_from_rates(   sensitivity = .9,   specificity = .8,   prevalence = .05 ) #> # A tibble: 1 × 5 #>   sensitivity specificity prevalence   ppv   npv #>         <dbl>       <dbl>      <dbl> <dbl> <dbl> #> 1         0.9         0.8       0.05 0.191 0.993  compute_predictive_value_from_rates(   sensitivity = .67,   specificity = .53,   prevalence = c(.15, .3) ) #> # A tibble: 2 × 5 #>   sensitivity specificity prevalence   ppv   npv #>         <dbl>       <dbl>      <dbl> <dbl> <dbl> #> 1        0.67        0.53       0.15 0.201 0.901 #> 2        0.67        0.53       0.3  0.379 0.789"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_smooth_density_roc.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","title":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","text":"Create ROC curve smoothed densities","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_smooth_density_roc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","text":"","code":"compute_smooth_density_roc(   data,   controls,   cases,   along = NULL,   best_weights = c(1, 0.5),   direction = \"auto\",   ... )"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_smooth_density_roc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","text":"data dataframe containing densities controls, cases bare column name densities control group along optional bare column name response values best_weights weights computing best ROC curve points. Defaults c(1, .5), defaults used pROC::coords(). direction direction set pROC::roc(). Defaults \"auto\". ... additional arguments. used currently.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_smooth_density_roc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","text":"dataframe updated new columns .sensitivities, .specificities, .auc, .roc_row, .is_best_youden .is_best_closest_topleft.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_smooth_density_roc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","text":"","code":"set.seed(100) x1 <- rnorm(100, 4, 1) x2 <- rnorm(100, 2, .5) both <- c(x1, x2) steps <- seq(min(both), max(both), length.out = 200) d1 <- dnorm(steps, mean(x1), sd(x1)) d2 <- dnorm(steps, mean(x2), sd(x2)) data <- tibble::tibble(   y = steps,   d1 = d1,   d2 = d2,   outcome = rbinom(200, 1, prob = 1 - (d1 / (d1 + d2))),   group = ifelse(outcome, \"case\", \"control\") ) compute_smooth_density_roc(data, d1, d2) #> # A tibble: 202 × 12 #>        y      d1     d2 outcome group .sensitivities .specificities  .auc #>    <dbl>   <dbl>  <dbl>   <int> <chr>          <dbl>          <dbl> <dbl> #>  1 0.932 0.00423 0.0264       1 case        0.000751          1.00  0.967 #>  2 0.960 0.00460 0.0319       1 case        0.00166           1.00  0.967 #>  3 0.989 0.00499 0.0383       1 case        0.00275           1.00  0.967 #>  4 1.02  0.00542 0.0459       1 case        0.00406           0.999 0.967 #>  5 1.05  0.00587 0.0546       1 case        0.00561           0.999 0.967 #>  6 1.07  0.00636 0.0647       1 case        0.00746           0.999 0.967 #>  7 1.10  0.00689 0.0763       1 case        0.00963           0.999 0.967 #>  8 1.13  0.00745 0.0895       1 case        0.0122            0.999 0.967 #>  9 1.16  0.00806 0.104        1 case        0.0152            0.998 0.967 #> 10 1.19  0.00870 0.121        1 case        0.0186            0.998 0.967 #> # ℹ 192 more rows #> # ℹ 4 more variables: .roc_row <int>, .direction <chr>, .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl> compute_smooth_density_roc(data, d1, d2, along = y) #> # A tibble: 202 × 12 #>        y      d1     d2 outcome group .sensitivities .specificities  .auc #>    <dbl>   <dbl>  <dbl>   <int> <chr>          <dbl>          <dbl> <dbl> #>  1 0.932 0.00423 0.0264       1 case        0.000751          1.00  0.967 #>  2 0.960 0.00460 0.0319       1 case        0.00166           1.00  0.967 #>  3 0.989 0.00499 0.0383       1 case        0.00275           1.00  0.967 #>  4 1.02  0.00542 0.0459       1 case        0.00406           0.999 0.967 #>  5 1.05  0.00587 0.0546       1 case        0.00561           0.999 0.967 #>  6 1.07  0.00636 0.0647       1 case        0.00746           0.999 0.967 #>  7 1.10  0.00689 0.0763       1 case        0.00963           0.999 0.967 #>  8 1.13  0.00745 0.0895       1 case        0.0122            0.999 0.967 #>  9 1.16  0.00806 0.104        1 case        0.0152            0.998 0.967 #> 10 1.19  0.00870 0.121        1 case        0.0186            0.998 0.967 #> # ℹ 192 more rows #> # ℹ 4 more variables: .roc_row <int>, .direction <chr>, .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl>  # terrible ROC because the response is not present (just the densities) data_shuffled <- data[sample(seq_len(nrow(data))), ] compute_smooth_density_roc(data_shuffled, d1, d2) #> # A tibble: 202 × 12 #>        y     d1       d2 outcome group   .sensitivities .specificities  .auc #>    <dbl>  <dbl>    <dbl>   <int> <chr>            <dbl>          <dbl> <dbl> #>  1  2.29 0.0963 7.70e- 1       1 case            0.0219          0.997 0.551 #>  2  3.43 0.334  1.66e- 3       0 control         0.0220          0.988 0.551 #>  3  4.40 0.363  1.49e- 8       0 control         0.0220          0.977 0.551 #>  4  6.33 0.0293 2.63e-26       0 control         0.0220          0.976 0.551 #>  5  6.35 0.0275 1.21e-26       0 control         0.0220          0.976 0.551 #>  6  5.59 0.117  2.59e-18       0 control         0.0220          0.972 0.551 #>  7  3.00 0.242  4.30e- 2       0 control         0.0232          0.965 0.551 #>  8  3.26 0.300  7.00e- 3       1 case            0.0234          0.957 0.551 #>  9  3.20 0.288  1.09e- 2       0 control         0.0237          0.949 0.551 #> 10  4.20 0.384  2.64e- 7       0 control         0.0237          0.938 0.551 #> # ℹ 192 more rows #> # ℹ 4 more variables: .roc_row <int>, .direction <chr>, .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl>  # sorted along response first: correct AUC compute_smooth_density_roc(data_shuffled, d1, d2, along = y) #> # A tibble: 202 × 12 #>        y     d1       d2 outcome group   .sensitivities .specificities  .auc #>    <dbl>  <dbl>    <dbl>   <int> <chr>            <dbl>          <dbl> <dbl> #>  1  2.29 0.0963 7.70e- 1       1 case             0.776        0.952   0.967 #>  2  3.43 0.334  1.66e- 3       0 control          1.00         0.707   0.967 #>  3  4.40 0.363  1.49e- 8       0 control          1.00         0.342   0.967 #>  4  6.33 0.0293 2.63e-26       0 control          1            0.00551 0.967 #>  5  6.35 0.0275 1.21e-26       0 control          1            0.00472 0.967 #>  6  5.59 0.117  2.59e-18       0 control          1            0.0534  0.967 #>  7  3.00 0.242  4.30e- 2       0 control          0.995        0.833   0.967 #>  8  3.26 0.300  7.00e- 3       1 case             0.999        0.762   0.967 #>  9  3.20 0.288  1.09e- 2       0 control          0.999        0.779   0.967 #> 10  4.20 0.384  2.64e- 7       0 control          1.00         0.416   0.967 #> # ℹ 192 more rows #> # ℹ 4 more variables: .roc_row <int>, .direction <chr>, .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl>"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_example_intelligibility_by_length.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulated intelligibility scores by utterance length — data_example_intelligibility_by_length","title":"Simulated intelligibility scores by utterance length — data_example_intelligibility_by_length","text":"dataset simulated intelligibility scores testing demonstrating modeling functions. created fitting Bayesian model raw Hustad colleagues (2020) drawing 1 sample posterior distribution expected predictions (.e., \"epreds). words, values model predictions original dataset. correlated original dataset values r = .86. might think simulation adding random noise original dataset.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_example_intelligibility_by_length.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulated intelligibility scores by utterance length — data_example_intelligibility_by_length","text":"","code":"data_example_intelligibility_by_length"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_example_intelligibility_by_length.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Simulated intelligibility scores by utterance length — data_example_intelligibility_by_length","text":"data frame 694 rows 5 variables: child identifier child age_months child's age months length_longest length child's longest utterance tocs_level utterance length sim_intelligibility child's intelligibility given utterance length (proportion words said child correctly transcribed two listeners)","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_example_intelligibility_by_length.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Simulated intelligibility scores by utterance length — data_example_intelligibility_by_length","text":"Hustad, K. C., Mahr, T., Natzke, P. E. M., & Rathouz, P. J. (2020). Development Speech Intelligibility 30 47 Months Typically Developing Children: Cross-Sectional Study Growth. Journal Speech, Language, Hearing Research, 63(6), 1675–1687. https://doi.org/10.1044/2020_JSLHR-20-00008","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_intelligibility.html","id":null,"dir":"Reference","previous_headings":"","what":"Fake intelligibility data — data_fake_intelligibility","title":"Fake intelligibility data — data_fake_intelligibility","text":"dataset fake intelligibility scores testing demonstrating modeling functions. created randomly sampling 200 rows intelligibility dataset adding random noise age_months intelligibility variables. values measure real children represent plausible age intelligibility measurements kind work.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_intelligibility.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fake intelligibility data — data_fake_intelligibility","text":"","code":"data_fake_intelligibility"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_intelligibility.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fake intelligibility data — data_fake_intelligibility","text":"data frame 200 rows 2 variables: age_months child's age months intelligibility child's intelligibility (proportion words said child correctly transcribed two listeners)","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_rates.html","id":null,"dir":"Reference","previous_headings":"","what":"Fake speaking rate data — data_fake_rates","title":"Fake speaking rate data — data_fake_rates","text":"dataset fake speaking rate measures testing demonstrating modeling functions. created randomly sampling 200 rows speaking rate dataset adding random noise age_months speaking_sps variables. values measure real children represent plausible age rate measurements kind work.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_rates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fake speaking rate data — data_fake_rates","text":"","code":"data_fake_rates"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_rates.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fake speaking rate data — data_fake_rates","text":"data frame 200 rows 2 variables: age_months child's age months speaking_sps child's speaking rate syllables per second","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/fit_kmeans.html","id":null,"dir":"Reference","previous_headings":"","what":"Run (scaled) k-means on a dataset. — fit_kmeans","title":"Run (scaled) k-means on a dataset. — fit_kmeans","text":"Observations scale()-ed clustering.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/fit_kmeans.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run (scaled) k-means on a dataset. — fit_kmeans","text":"","code":"fit_kmeans(data, k, vars, args_kmeans = list())"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/fit_kmeans.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run (scaled) k-means on a dataset. — fit_kmeans","text":"data dataframe k number clusters create vars variable selection clustering. Select multiple variables c(), e.g., c(x, y). selection supports tidyselect semantics tidyselect::select_helpers, e.g., c(x, starts_with(\"mean_\"). args_kmeans additional arguments passed stats::kmeans().","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/fit_kmeans.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run (scaled) k-means on a dataset. — fit_kmeans","text":"original data augmented additional columns clustering details. including .kmeans_cluster (cluster number observation, factor) .kmeans_k (selected number clusters). Cluster-level information also included. example, suppose cluster using variable x. output column .kmeans_x giving cluster mean x .kmeans_rank_x giving cluster labels reordered using cluster means x. column .kmeans_sort contains cluster sorted using first principal component scaled variables. columns cluster indices factor() can plotted discrete variables.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/fit_kmeans.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run (scaled) k-means on a dataset. — fit_kmeans","text":"Note variable scaled() clustering cluster means unscaled match original data scale. function provides original kmeans labels .kmeans_cluster alternative labeling based different sortings data. provided order deal label-swapping Bayesian models. See bootstrapping example .","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/fit_kmeans.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run (scaled) k-means on a dataset. — fit_kmeans","text":"","code":"data_kmeans <- fit_kmeans(mtcars, 3, c(mpg, wt, hp))  library(ggplot2) ggplot(data_kmeans) +   aes(x = wt, y = mpg) +   geom_point(aes(color = .kmeans_cluster))   ggplot(data_kmeans) +   aes(x = wt, y = mpg) +   geom_point(aes(color = .kmeans_rank_wt))   # Example of label swapping set.seed(123) data_boots <- lapply(   1:10,   function(x) {     rows <- sample(seq_len(nrow(mtcars)), replace = TRUE)     data <- mtcars[rows, ]     data$.bootstrap <- x     data   } ) |>   lapply(fit_kmeans, k = 3, c(mpg, wt, hp)) |>   dplyr::bind_rows() |>   dplyr::select(.bootstrap, dplyr::starts_with(\".kmeans_\")) |>   dplyr::distinct()  # Clusters start off in random locations and move to center, so the labels # differ between model runs and across bootstraps. ggplot(data_boots) +   aes(x = .kmeans_wt, y = .kmeans_mpg) +   geom_point(aes(color = .kmeans_cluster)) +   labs(title = \"k-means centers on 10 bootstraps\")   # Labels sorted using first principal component # so the labels are more consistent. ggplot(data_boots) +   aes(x = .kmeans_wt, y = .kmeans_mpg) +   geom_point(aes(color = .kmeans_sort)) +   labs(title = \"k-means centers on 10 bootstraps\")"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"function fits type GAMLSS model used Mahr colleagues (2021) 🔓: generalized gamma regression model (via gamlss.dist::GG()) natural cubic splines mean (mu), scale (sigma), shape (nu) distribution. model fitted using package's mem_gamlss() wrapper function.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"","code":"fit_gen_gamma_gamlss(   data,   var_x,   var_y,   df_mu = 3,   df_sigma = 2,   df_nu = 1,   control = NULL )  fit_gen_gamma_gamlss_se(   data,   name_x,   name_y,   df_mu = 3,   df_sigma = 2,   df_nu = 1,   control = NULL )  predict_gen_gamma_gamlss(newdata, model, centiles = c(5, 10, 50, 90, 95))"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"Associated article: https://doi.org/10.1044/2021_JSLHR-21-00206","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"data data frame var_x, var_y (unquoted) variable names giving predictor variable (e.g., age) outcome variable (.e.g, rate). df_mu, df_sigma, df_nu degrees freedom. 0 used, splines::ns() term dropped model formula parameter. control gamlss::gamlss.control() controller. Defaults NULL uses default settings, except setting trace FALSE silence output gamlss. name_x, name_y quoted variable names giving predictor variable (e.g., \"age\") outcome variable (.e.g, \"rate\"). arguments apply fit_gen_gamma_gamlss_se(). newdata one-column dataframe predictions model model fitted fit_gen_gamma_gamlss() centiles centiles use prediction. Defaults c(5, 10, 50, 90, 95).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"fit_gen_gamma_gamlss() fit_gen_gamma_gamlss_se(), mem_gamlss()-fitted model. .user data model includes degrees freedom parameter splines::ns() basis parameter. predict_gen_gamma_gamlss(), dataframe containing model predictions mu, sigma, nu, plus columns centile centiles.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"two versions function. main version fit_gen_gamma_gamlss(), works unquoted column names (e.g., age). alternative version fit_gen_gamma_gamlss_se(); final \"se\" stands \"Standard Evaluation\". designation means variable names must given strings (, quoted \"age\" instead bare name age). alternative version necessary fit several models using parallel computing furrr::future_map() (using bootstrap resampling). predict_centiles() work function, likely throw warning message. Therefore, predict_gen_gamma_gamlss() provides alternative way compute centiles model. function manually computes centiles instead relying gamlss::centiles(). main difference new x values go splines::predict.ns() multiplied model coefficients.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"","code":"data_fake_rates #> # A tibble: 200 × 2 #>    age_months speaking_sps #>         <int>        <dbl> #>  1         66         3.76 #>  2         29         2.08 #>  3         90         3.07 #>  4         61         2.64 #>  5         46         3.54 #>  6         61         3.23 #>  7         63         3.55 #>  8         51         2.84 #>  9         48         3.24 #> 10         37         2.39 #> # ℹ 190 more rows  m <- fit_gen_gamma_gamlss(data_fake_rates, age_months, speaking_sps)  # using \"qr\" in summary() just to suppress a warning message summary(m, type = \"qr\") #> ****************************************************************** #> Family:  c(\"GG\", \"generalised Gamma Lopatatsidis-Green\")  #>  #> Call:  gamlss::gamlss(formula = speaking_sps ~ ns(age_months, df = 3),   #>     sigma.formula = ~ns(age_months, df = 2), nu.formula = ~ns(age_months,   #>         df = 1), family = GG(), data = ~data_fake_rates, control = list(  #>         c.crit = 0.001, n.cyc = 20, mu.step = 1, sigma.step = 1,   #>         nu.step = 1, tau.step = 1, gd.tol = Inf, iter = 0, trace = FALSE,   #>         autostep = TRUE, save = TRUE))  #>  #> Fitting method: RS()  #>  #> ------------------------------------------------------------------ #> Mu link function:  log #> Mu Coefficients: #>                         Estimate Std. Error t value Pr(>|t|)     #> (Intercept)              0.92763    0.04539  20.435  < 2e-16 *** #> ns(age_months, df = 3)1  0.14393    0.03919   3.672 0.000310 *** #> ns(age_months, df = 3)2  0.36779    0.10288   3.575 0.000441 *** #> ns(age_months, df = 3)3  0.20240    0.03780   5.355 2.38e-07 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ------------------------------------------------------------------ #> Sigma link function:  log #> Sigma Coefficients: #>                         Estimate Std. Error t value Pr(>|t|)     #> (Intercept)              -1.7623     0.1597 -11.038   <2e-16 *** #> ns(age_months, df = 2)1  -0.6923     0.3379  -2.049   0.0418 *   #> ns(age_months, df = 2)2  -0.3832     0.2073  -1.848   0.0661 .   #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ------------------------------------------------------------------ #> Nu link function:  identity  #> Nu Coefficients: #>                        Estimate Std. Error t value Pr(>|t|)   #> (Intercept)              -3.438      1.647  -2.088   0.0381 * #> ns(age_months, df = 1)    8.336      4.312   1.933   0.0547 . #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ------------------------------------------------------------------ #> No. of observations in the fit:  200  #> Degrees of Freedom for the fit:  9 #>       Residual Deg. of Freedom:  191  #>                       at cycle:  14  #>   #> Global Deviance:     184.5313  #>             AIC:     202.5313  #>             SBC:     232.2161  #> ******************************************************************  # Alternative interface d <- data_fake_rates m2 <- fit_gen_gamma_gamlss_se(   data = d,   name_x = \"age_months\",   name_y = \"speaking_sps\" ) coef(m2) == coef(m) #>             (Intercept) ns(age_months, df = 3)1 ns(age_months, df = 3)2  #>                    TRUE                    TRUE                    TRUE  #> ns(age_months, df = 3)3  #>                    TRUE   # how to use control to change gamlss() behavior m_traced <- fit_gen_gamma_gamlss(   data_fake_rates,   age_months,   speaking_sps,   control = gamlss::gamlss.control(n.cyc = 15, trace = TRUE) ) #> GAMLSS-RS iteration 1: Global Deviance = 185.9307  #> GAMLSS-RS iteration 2: Global Deviance = 185.2312  #> GAMLSS-RS iteration 3: Global Deviance = 184.9112  #> GAMLSS-RS iteration 4: Global Deviance = 184.7408  #> GAMLSS-RS iteration 5: Global Deviance = 184.6483  #> GAMLSS-RS iteration 6: Global Deviance = 184.5971  #> GAMLSS-RS iteration 7: Global Deviance = 184.5691  #> GAMLSS-RS iteration 8: Global Deviance = 184.553  #> GAMLSS-RS iteration 9: Global Deviance = 184.5436  #> GAMLSS-RS iteration 10: Global Deviance = 184.5381  #> GAMLSS-RS iteration 11: Global Deviance = 184.5348  #> GAMLSS-RS iteration 12: Global Deviance = 184.5329  #> GAMLSS-RS iteration 13: Global Deviance = 184.5319  #> GAMLSS-RS iteration 14: Global Deviance = 184.5313   # The `.user` space includes the spline bases, so that we can make accurate # predictions of new xs. names(m$.user) #> [1] \"data\"         \"session_info\" \"call\"         \"df_mu\"        \"df_sigma\"     #> [6] \"df_nu\"        \"basis_mu\"     \"basis_sigma\"  \"basis_nu\"      # predict log(mean) at 55 months: log_mean_55 <- cbind(1, predict(m$.user$basis_mu, 55)) %*% coef(m) log_mean_55 #>          [,1] #> [1,] 1.070221 exp(log_mean_55) #>          [,1] #> [1,] 2.916024  # But predict_gen_gamma_gamlss() does this work for us and also provides # centiles new_ages <- data.frame(age_months = 48:71) centiles <- predict_gen_gamma_gamlss(new_ages, m) centiles #> # A tibble: 24 × 9 #>    age_months    mu sigma     nu    c5   c10   c50   c90   c95 #>         <int> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #>  1         48  2.83 0.142 -1.60   2.29  2.40  2.86  3.47  3.68 #>  2         49  2.84 0.141 -1.50   2.30  2.41  2.87  3.48  3.68 #>  3         50  2.86 0.140 -1.40   2.32  2.43  2.88  3.48  3.68 #>  4         51  2.87 0.138 -1.31   2.33  2.44  2.89  3.48  3.68 #>  5         52  2.88 0.137 -1.21   2.34  2.45  2.90  3.49  3.68 #>  6         53  2.89 0.136 -1.11   2.35  2.46  2.91  3.49  3.68 #>  7         54  2.91 0.135 -1.02   2.36  2.47  2.92  3.49  3.68 #>  8         55  2.92 0.134 -0.920  2.37  2.48  2.93  3.50  3.68 #>  9         56  2.93 0.132 -0.823  2.38  2.49  2.94  3.50  3.68 #> 10         57  2.94 0.131 -0.726  2.39  2.50  2.95  3.50  3.68 #> # ℹ 14 more rows  # Confirm that the manual prediction matches the automatic one centiles[centiles$age_months == 55, \"mu\"] #> # A tibble: 1 × 1 #>      mu #>   <dbl> #> 1  2.92 exp(log_mean_55) #>          [,1] #> [1,] 2.916024  if(requireNamespace(\"ggplot2\", quietly = TRUE)) {   library(ggplot2)   ggplot(pivot_centiles_longer(centiles)) +     aes(x = age_months, y = .value) +     geom_line(aes(group = .centile, color = .centile_pair)) +     geom_point(       aes(y = speaking_sps),       data = subset(         data_fake_rates,         48 <= age_months & age_months <= 71       )     ) }   # Example of 0-df splines m <- fit_gen_gamma_gamlss(   data_fake_rates,   age_months,   speaking_sps,   df_mu = 0,   df_sigma = 2,   df_nu = 0 ) coef(m, what = \"mu\") #> (Intercept)  #>    1.113478  coef(m, what = \"sigma\") #>             (Intercept) ns(age_months, df = 2)1 ns(age_months, df = 2)2  #>              -1.5223591              -1.0914888              -0.2153066  coef(m, what = \"nu\") #> (Intercept)  #>    1.642225   # mu and nu fixed, c50 mostly locked in predict_gen_gamma_gamlss(new_ages, m)[c(1, 9, 17, 24), ] #> # A tibble: 4 × 9 #>   age_months    mu sigma    nu    c5   c10   c50   c90   c95 #>        <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #> 1         48  3.04 0.148  1.64  2.31  2.46  3.01  3.61  3.79 #> 2         56  3.04 0.132  1.64  2.39  2.52  3.02  3.55  3.70 #> 3         64  3.04 0.122  1.64  2.44  2.56  3.02  3.51  3.65 #> 4         71  3.04 0.118  1.64  2.46  2.58  3.02  3.50  3.64"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":null,"dir":"Reference","previous_headings":"","what":"Staged imputation — impute_values_by_length","title":"Staged imputation — impute_values_by_length","text":"Impute missing data different utterance lengths using successive linear models.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Staged imputation — impute_values_by_length","text":"","code":"impute_values_by_length(   data,   var_y,   var_length,   id_cols = NULL,   include_max_length = FALSE,   data_train = NULL )"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Staged imputation — impute_values_by_length","text":"data dataframe impute missing value var_y bare name response variable imputation var_length bare name length variable id_cols selection variable names uniquely identify group related observations. example, c(child_id, age_months). include_max_length whether use maximum length value predictor imputation models. Defaults FALSE. data_train (optional) dataframe used train imputation models. example, might data reference group children data_train clinical population data. omitted, dataframe data used train models.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Staged imputation — impute_values_by_length","text":"dataframe additional columns {var_y}_imputed (imputed value), .max_{var_length} highest value var_length observed data, {var_y}_imputation labeling whether observations \"imputed\" \"observed\".","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":"background","dir":"Reference","previous_headings":"","what":"Background","title":"Staged imputation — impute_values_by_length","text":"Hustad colleagues (2020), modeled intelligibility data young children's speech. Children hear utterance repeat . utterances started 2 words length, increased 3 words length, batches 10 sentences, way 7 words length. problem, however: children produce utterances every length. Specifically, child reliably produced 5 utterances given length length, task halted. given nature task, child produced 5-word utterances, also produced 2--4-word utterances well. length utterance probably influenced outcome variable: Longer utterances words might help listener understand sentence, example. Therefore, seem appropriate ignore missing values. used following two-step procedure (see Supplemental Materials detail):","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":"other-notes","dir":"Reference","previous_headings":"","what":"Other notes","title":"Staged imputation — impute_values_by_length","text":"Remark data data_train: One might ask, children help train data imputation models? consider norm-referenced standardized testing scenario: new participant (observations data), want know compare age peers (participants data_train). separating data_train fixing reference group, can apply adjustment/imputation procedure new participants.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Staged imputation — impute_values_by_length","text":"Hustad, K. C., Mahr, T., Natzke, P. E. M., & Rathouz, P. J. (2020). Development Speech Intelligibility 30 47 Months Typically Developing Children: Cross-Sectional Study Growth. Journal Speech, Language, Hearing Research, 63(6), 1675–1687. https://doi.org/10.1044/2020_JSLHR-20-00008 Hustad, K. C., Mahr, T., Natzke, P. E. M., & J. Rathouz, P. (2020). Supplemental Material S1 (Hustad et al., 2020). ASHA journals. https://doi.org/10.23641/asha.12330956.v1","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Staged imputation — impute_values_by_length","text":"","code":"set.seed(1) fake_data <- tibble::tibble(   child = c(     \"a\", \"a\", \"a\", \"a\", \"a\",     \"b\", \"b\", \"b\", \"b\", \"b\",     \"c\", \"c\", \"c\", \"c\", \"c\",     \"e\", \"e\", \"e\", \"e\", \"e\",     \"f\", \"f\", \"f\", \"f\", \"f\",     \"g\", \"g\", \"g\", \"g\", \"g\",     \"h\", \"h\", \"h\", \"h\", \"h\",     \"i\", \"i\", \"i\", \"i\", \"i\"   ),   level = c(1:5, 1:5, 1:5, 1:5, 1:5, 1:5, 1:5, 1:5),   x = c(     c(100, 110, 120, 130, 150) + c(-8, -5, 0, NA, NA),     c(100, 110, 120, 130, 150) + c(6, 6, 4, NA, NA),     c(100, 110, 120, 130, 150) + c(-5, -5, -2, 2, NA),     c(100, 110, 120, 130, 150) + rbinom(5, 12, .5) - 6,     c(100, 110, 120, 130, 150) + rbinom(5, 12, .5) - 6,     c(100, 110, 120, 130, 150) + rbinom(5, 12, .5) - 6,     c(100, 110, 120, 130, 150) + rbinom(5, 12, .5) - 6,     c(100, 110, 120, 130, 150) + rbinom(5, 12, .5) - 6   ) ) data_imputed <- impute_values_by_length(   fake_data,   x,   level,   id_cols = c(child),   include_max_length = FALSE )  if (requireNamespace(\"ggplot2\")) {   library(ggplot2)   ggplot(data_imputed) +     aes(x = level, y = x_imputed) +     geom_line(aes(group = child)) +     geom_point(aes(color = x_imputation)) }"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/join_to_split.html","id":null,"dir":"Reference","previous_headings":"","what":"Join data onto resampled IDs — join_to_split","title":"Join data onto resampled IDs — join_to_split","text":"Join data onto resampled IDs","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/join_to_split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Join data onto resampled IDs — join_to_split","text":"","code":"join_to_split(x, y, by, validate = FALSE)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/join_to_split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Join data onto resampled IDs — join_to_split","text":"x rset object created rsample::bootstraps() y y dataframe column id values resampled create x name column y data validate whether validate join counting number rows associated id. Defaults FALSE.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/join_to_split.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Join data onto resampled IDs — join_to_split","text":"original rset object x$data updated join y row numbers x$in_id updated work expanded dataset.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/join_to_split.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Join data onto resampled IDs — join_to_split","text":"","code":"library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union data_trees <- tibble::as_tibble(datasets::Orange)  data_tree_ids <- distinct(data_trees, Tree)  # Resample ids data_bootstraps <- data_tree_ids %>%   rsample::bootstraps(times = 20) %>%   rename(splits_id = splits) %>%   # Attach data to resampled ids   mutate(     data_splits = splits_id %>% purrr::map(       join_to_split,       data_trees,       by = \"Tree\",       validate = TRUE     )   )  data_bootstraps #> # A tibble: 20 × 3 #>    splits_id     id          data_splits     #>    <list>        <chr>       <list>          #>  1 <split [5/2]> Bootstrap01 <split [35/14]> #>  2 <split [5/2]> Bootstrap02 <split [35/14]> #>  3 <split [5/3]> Bootstrap03 <split [35/21]> #>  4 <split [5/2]> Bootstrap04 <split [35/14]> #>  5 <split [5/2]> Bootstrap05 <split [35/14]> #>  6 <split [5/1]> Bootstrap06 <split [35/7]>  #>  7 <split [5/1]> Bootstrap07 <split [35/7]>  #>  8 <split [5/2]> Bootstrap08 <split [35/14]> #>  9 <split [5/2]> Bootstrap09 <split [35/14]> #> 10 <split [5/2]> Bootstrap10 <split [35/14]> #> 11 <split [5/2]> Bootstrap11 <split [35/14]> #> 12 <split [5/1]> Bootstrap12 <split [35/7]>  #> 13 <split [5/2]> Bootstrap13 <split [35/14]> #> 14 <split [5/2]> Bootstrap14 <split [35/14]> #> 15 <split [5/2]> Bootstrap15 <split [35/14]> #> 16 <split [5/2]> Bootstrap16 <split [35/14]> #> 17 <split [5/2]> Bootstrap17 <split [35/14]> #> 18 <split [5/2]> Bootstrap18 <split [35/14]> #> 19 <split [5/1]> Bootstrap19 <split [35/7]>  #> 20 <split [5/1]> Bootstrap20 <split [35/7]>"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/logitnorm_mean.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the mean of logit-normal distribution(s) — logitnorm_mean","title":"Compute the mean of logit-normal distribution(s) — logitnorm_mean","text":"function wrapper around logitnorm::momentsLogitnorm().","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/logitnorm_mean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the mean of logit-normal distribution(s) — logitnorm_mean","text":"","code":"logitnorm_mean(mu, sigma)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/logitnorm_mean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the mean of logit-normal distribution(s) — logitnorm_mean","text":"mu mean(s) logit scale sigma standard deviation(s) logit scale","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/logitnorm_mean.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the mean of logit-normal distribution(s) — logitnorm_mean","text":"means distributions","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/logitnorm_mean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute the mean of logit-normal distribution(s) — logitnorm_mean","text":"","code":"# \\donttest{ x <- logitnorm_mean(2, 1) x #>      mean  #> 0.8445375  # } # compare to simulation set.seed(100) rnorm(1000, 2, 1) |> plogis() |> mean() #> [1] 0.8444758"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/mem_gamlss.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a gamlss model but store user data — mem_gamlss","title":"Fit a gamlss model but store user data — mem_gamlss","text":"Think gamlss model memories (mem. gamlss).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/mem_gamlss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a gamlss model but store user data — mem_gamlss","text":"","code":"mem_gamlss(...)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/mem_gamlss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a gamlss model but store user data — mem_gamlss","text":"... arguments passed gamlss::gamlss()","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/mem_gamlss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a gamlss model but store user data — mem_gamlss","text":"fitted model object updated include user information model$.user. Includes dataset used fit model model$.user$data, session info model$.user$session_info call used fit model model$.user$call. model$call updated match","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::[\\%>\\%][magrittr::pipe] details.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/predict_centiles.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict and tidy centiles from a GAMLSS model — predict_centiles","title":"Predict and tidy centiles from a GAMLSS model — predict_centiles","text":"gamlss trouble predictions without original training data.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/predict_centiles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict and tidy centiles from a GAMLSS model — predict_centiles","text":"","code":"predict_centiles(newdata, model, centiles = c(5, 10, 50, 90, 95), ...)  pivot_centiles_longer(data)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/predict_centiles.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict and tidy centiles from a GAMLSS model — predict_centiles","text":"newdata one-column dataframe predictions model gamlss model prepared mem_gamlss() centiles centiles use prediction. Defaults c(5, 10, 50, 90, 95). ... arguments passed gamlss::centiles.pred() data centile predictions reshape pivot_centiles_longer()","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/predict_centiles.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict and tidy centiles from a GAMLSS model — predict_centiles","text":"tibble fitted centiles predict_centiles() long-format tibble one centile value per row pivot_centiles_longer()","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/tidyeval.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy eval helpers — tidyeval","title":"Tidy eval helpers — tidyeval","text":"rlang::sym() creates symbol string syms() creates list symbols character vector. enquo() enquos() delay execution one several function arguments. enquo() returns single quoted expression, like blueprint delayed computation. enquos() returns list quoted expressions. expr() quotes new expression locally. mostly useful build new expressions around arguments captured enquo() enquos(): expr(mean(!!enquo(arg), na.rm = TRUE)). rlang::as_name() transforms quoted variable name string. Supplying something else quoted variable name error. unlike rlang::as_label() also returns single string supports kind R object input, including quoted function calls vectors. purpose summarise object single label. label often suitable default name. know quoted expression contains (instance expressions captured enquo() variable name, call function, unquoted constant), use as_label(). know quoted simple variable name, like enforce , use as_name(). learn tidy eval use tools, visit https://tidyeval.tidyverse.org Metaprogramming section Advanced R.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/tocs_item.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract the TOCS details from a string (usually a filename) — tocs_item","title":"Extract the TOCS details from a string (usually a filename) — tocs_item","text":"Extract TOCS details string (usually filename)","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/tocs_item.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract the TOCS details from a string (usually a filename) — tocs_item","text":"","code":"tocs_item(xs)  tocs_type(xs)  tocs_length(xs)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/tocs_item.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract the TOCS details from a string (usually a filename) — tocs_item","text":"xs character vector","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/tocs_item.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract the TOCS details from a string (usually a filename) — tocs_item","text":"tocs_item() returns substring TOCS item, tocs_type() returns whether item \"single-word\" \"multiword\", tocs_length() returns length TOCS item (.e., number words).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/tocs_item.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract the TOCS details from a string (usually a filename) — tocs_item","text":"","code":"x <- c(   \"XXv16s7T06.lab\", \"XXv15s5T06.TextGrid\", \"XXv13s3T10.WAV\",   \"XXv18wT11.wav\", \"non-matching\", \"s2T01\" ) data.frame(   x = x,   item = tocs_item(x),   type = tocs_type(x),   length = tocs_length(x) ) #>                     x  item        type length #> 1      XXv16s7T06.lab S7T06   multiword      7 #> 2 XXv15s5T06.TextGrid S5T06   multiword      5 #> 3      XXv13s3T10.WAV S3T10   multiword      3 #> 4       XXv18wT11.wav  WT11 single-word      1 #> 5        non-matching  <NA>        <NA>     NA #> 6               s2T01 S2T01   multiword      2"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/trapezoid_auc.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute AUCs using the trapezoid method — trapezoid_auc","title":"Compute AUCs using the trapezoid method — trapezoid_auc","text":"Compute AUCs using trapezoid method","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/trapezoid_auc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute AUCs using the trapezoid method — trapezoid_auc","text":"","code":"trapezoid_auc(xs, ys)  partial_trapezoid_auc(xs, ys, xlim)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/trapezoid_auc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute AUCs using the trapezoid method — trapezoid_auc","text":"xs, ys x y positions xlim two-element vector (range) xs sum ","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/trapezoid_auc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute AUCs using the trapezoid method — trapezoid_auc","text":"area curve computed using trapezoid method. partial_trapezoid_auc(), partial area curve computed.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/trapezoid_auc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute AUCs using the trapezoid method — trapezoid_auc","text":"","code":"if (requireNamespace(\"rstanarm\", quietly = TRUE)) {   wells <- rstanarm::wells   r <- pROC::roc(switch ~ arsenic, wells)   pROC::auc(r)   trapezoid_auc(r$specificities, r$sensitivities)    pROC::auc(r, partial.auc = c(.9, 1), partial.auc.focus = \"sp\")   partial_trapezoid_auc(r$specificities, r$sensitivities, c(.9, 1))    pROC::auc(r, partial.auc = c(.9, 1), partial.auc.focus = \"se\")   partial_trapezoid_auc(r$sensitivities, r$specificities, c(.9, 1))    pROC::auc(r, partial.auc = c(.1, .9), partial.auc.focus = \"sp\")   partial_trapezoid_auc(r$specificities, r$sensitivities, c(.1, .9))    pROC::auc(r, partial.auc = c(.1, .9), partial.auc.focus = \"se\")   partial_trapezoid_auc(r$sensitivities, r$specificities, c(.1, .9)) } #> Setting levels: control = 0, case = 1 #> Setting direction: controls < cases #> [1] 0.5086048"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/weight_lengths_with_ordinal_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Weight utterance lengths by using an ordinal regression model — weight_lengths_with_ordinal_model","title":"Weight utterance lengths by using an ordinal regression model — weight_lengths_with_ordinal_model","text":"participant, find length longest utterance. predict longest utterance length nonlinear function variable, compute probability reaching utterance length value predictor variable. probabilities normalized provide weights utterance length.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/weight_lengths_with_ordinal_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weight utterance lengths by using an ordinal regression model — weight_lengths_with_ordinal_model","text":"","code":"weight_lengths_with_ordinal_model(   data_train,   var_length,   var_x,   id_cols,   spline_df = 2,   data_join = NULL )"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/weight_lengths_with_ordinal_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Weight utterance lengths by using an ordinal regression model — weight_lengths_with_ordinal_model","text":"data_train dataframe used train ordinal model var_length bare name length variable. example, tocs_level. var_x bare name predictor variable. example, age_months. id_cols selection variable names uniquely identify group related observations. example, c(child_id, age_months). spline_df number degrees freedom use ordinal regression model. data_join (optional) dataset use join weights onto. feature necessary want train dataset observed data supply weights dataset missing values imputed.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/weight_lengths_with_ordinal_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Weight utterance lengths by using an ordinal regression model — weight_lengths_with_ordinal_model","text":"probability weights utterance length observed value var_x. added columns {var_length}_prob_reached {var_length}_weight, respectively.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/weight_lengths_with_ordinal_model.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Weight utterance lengths by using an ordinal regression model — weight_lengths_with_ordinal_model","text":"Hustad, K. C., Mahr, T., Natzke, P. E. M., & Rathouz, P. J. (2020). Development Speech Intelligibility 30 47 Months Typically Developing Children: Cross-Sectional Study Growth. Journal Speech, Language, Hearing Research, 63(6), 1675–1687. https://doi.org/10.1044/2020_JSLHR-20-00008 Hustad, K. C., Mahr, T., Natzke, P. E. M., & J. Rathouz, P. (2020). Supplemental Material S1 (Hustad et al., 2020). ASHA journals. https://doi.org/10.23641/asha.12330956.v1","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/weight_lengths_with_ordinal_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Weight utterance lengths by using an ordinal regression model — weight_lengths_with_ordinal_model","text":"","code":"data_weights <- weight_lengths_with_ordinal_model(   data_example_intelligibility_by_length,   tocs_level,   age_months,   child,   spline_df = 2 )  if (requireNamespace(\"ggplot2\")) {   library(ggplot2)   p1 <- ggplot(data_weights) +     aes(x = age_months, y = tocs_level_prob_reached) +     geom_line(aes(color = ordered(tocs_level)), linewidth = 1) +     scale_color_ordinal(end = .85) +     labs(y = \"Prob. of reaching length\", color = \"Utterance length\")   print(p1)    p2 <- p1 +     aes(y = tocs_level_weight) +     labs(y = \"Weight of utterance length\")   print(p2) }"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/wisclabmisc-package.html","id":null,"dir":"Reference","previous_headings":"","what":"wisclabmisc: Tools to Support the 'WiscLab' — wisclabmisc-package","title":"wisclabmisc: Tools to Support the 'WiscLab' — wisclabmisc-package","text":"collection 'R' functions use (re-use) across 'WiscLab' projects. analysis presentation oriented functions--, data reading data cleaning.","code":""},{"path":[]},{"path":"https://www.tjmahr.com/wisclabmisc/reference/wisclabmisc-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"wisclabmisc: Tools to Support the 'WiscLab' — wisclabmisc-package","text":"Maintainer: Tristan Mahr tristan.mahr@wisc.edu (ORCID)","code":""}]
