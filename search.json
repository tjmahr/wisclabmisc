[{"path":"https://www.tjmahr.com/wisclabmisc/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2020 Tristan Mahr Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/articles/gamlss-tools.html","id":"a-gamlss-that-remembers-the-data","dir":"Articles","previous_headings":"","what":"A gamlss() that remembers the data","title":"Tools for GAMLSS models","text":"mem_gamlss() (memory gamlss) provides drop-replacement gamlss() function. difference mem_gamlss() gamlss() modified version includes bundle data .user records original dataset, session information call used fit model. gamlss store data part model object, need dataset prediction centile prediction often fails without dataset: including original dataset works: (“Centile prediction” means predicting percentiles data along single variable. ’s function just needs single xname: single predictor variable used. use centile prediction compute growth curves can look smooth changes percentiles age.)","code":"library(wisclabmisc) library(gamlss) library(tidyverse)  data <- as.data.frame(nlme::Orthodont) model <- mem_gamlss(distance ~ age, data = data) #> GAMLSS-RS iteration 1: Global Deviance = 505.577  #> GAMLSS-RS iteration 2: Global Deviance = 505.577 str(model$.user, max.level = 1) #> List of 3 #>  $ data        :'data.frame':    108 obs. of  4 variables: #>  $ session_info:List of 2 #>   ..- attr(*, \"class\")= chr [1:2] \"session_info\" \"list\" #>  $ call        : language mem_gamlss(distance ~ age, data = data) newdata <- distinct(data, age) centiles.pred(   model,    cent = c(25, 50, 75),   xname = \"age\",    xvalues = newdata$age,    plot = FALSE ) #> Error in data.frame(data, source = namelist): arguments imply differing number of rows: 4, 5 centiles.pred(   model,    cent = c(25, 50, 75),   xname = \"age\",    xvalues = newdata$age,    plot = FALSE,   data = model$.user$data ) #>    x       25       50       75 #> 1  8 20.34723 22.04259 23.73796 #> 2 10 21.66760 23.36296 25.05833 #> 3 12 22.98797 24.68333 26.37870 #> 4 14 24.30834 26.00370 27.69907"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/gamlss-tools.html","id":"centile-prediction-and-tidying","dir":"Articles","previous_headings":"","what":"Centile prediction and tidying","title":"Tools for GAMLSS models","text":"package provides predict_centiles() streamlined version code, : assumes model fitted mem_gamlss() returns tibble keeps predictor name (, age instead x) prefixes centiles q (quantile) predicted centiles wide format. can tidy long format pivot_centiles_longer(). also includes .pair column helps mark commonly paired quantiles 25:75, 10:90, 5:95.","code":"centiles <- predict_centiles(   newdata,   model,    cent = c(25, 50, 75) ) centiles #> # A tibble: 4 × 4 #>     age   c25   c50   c75 #>   <dbl> <dbl> <dbl> <dbl> #> 1     8  20.3  22.0  23.7 #> 2    10  21.7  23.4  25.1 #> 3    12  23.0  24.7  26.4 #> 4    14  24.3  26.0  27.7 pivot_centiles_longer(centiles) #> # A tibble: 12 × 4 #>      age .centile .value .centile_pair   #>    <dbl>    <dbl>  <dbl> <chr>           #>  1     8       25   20.3 centiles 25, 75 #>  2     8       50   22.0 median          #>  3     8       75   23.7 centiles 25, 75 #>  4    10       25   21.7 centiles 25, 75 #>  5    10       50   23.4 median          #>  6    10       75   25.1 centiles 25, 75 #>  7    12       25   23.0 centiles 25, 75 #>  8    12       50   24.7 median          #>  9    12       75   26.4 centiles 25, 75 #> 10    14       25   24.3 centiles 25, 75 #> 11    14       50   26.0 median          #> 12    14       75   27.7 centiles 25, 75"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/gamlss-tools.html","id":"sample-centiles-checks","dir":"Articles","previous_headings":"Centile prediction and tidying","what":"Sample centiles checks","title":"Tools for GAMLSS models","text":"Half data 50% centile line half 50% centile line. holds centile lines. check_sample_centiles() performs check computing percentages observations less equal centile line. matches gamlss package’s output: function also supports grouped data check centile performance different subsets data. output also matches output provide gamlss’s centile.split() function:","code":"check_sample_centiles(data, model, age, distance) #> # A tibble: 7 × 4 #>   .centile     n n_under_centile percent_under_centile #>      <dbl> <int>           <int>                 <dbl> #> 1        5   108               6                  5.56 #> 2       10   108               9                  8.33 #> 3       25   108              25                 23.1  #> 4       50   108              61                 56.5  #> 5       75   108              85                 78.7  #> 6       90   108              95                 88.0  #> 7       95   108             100                 92.6 centiles(   model,    model$.user$data$age,    data = model$.user$data,    cent = c(5, 10,25, 50, 75, 90, 95),    plot = FALSE ) #> % of cases below  5 centile is  5.555556  #> % of cases below  10 centile is  8.333333  #> % of cases below  25 centile is  23.14815  #> % of cases below  50 centile is  56.48148  #> % of cases below  75 centile is  78.7037  #> % of cases below  90 centile is  87.96296  #> % of cases below  95 centile is  92.59259 data %>%    mutate(age_bin = ntile(age, 2)) %>%    group_by(age_bin) %>%    check_sample_centiles(model, age, distance) #> # A tibble: 14 × 5 #>    age_bin .centile     n n_under_centile percent_under_centile #>      <int>    <dbl> <int>           <int>                 <dbl> #>  1       1        5    54               3                  5.56 #>  2       1       10    54               4                  7.41 #>  3       1       25    54              13                 24.1  #>  4       1       50    54              29                 53.7  #>  5       1       75    54              44                 81.5  #>  6       1       90    54              49                 90.7  #>  7       1       95    54              51                 94.4  #>  8       2        5    54               3                  5.56 #>  9       2       10    54               5                  9.26 #> 10       2       25    54              12                 22.2  #> 11       2       50    54              32                 59.3  #> 12       2       75    54              41                 75.9  #> 13       2       90    54              46                 85.2  #> 14       2       95    54              49                 90.7 centiles.split(   model,    model$.user$data$age,    data = model$.user$data,    n.inter = 2,   cent = c(5, 10,25, 50, 75, 90, 95),    plot = FALSE ) #>      7 to 11  11 to 15 #> 5   5.555556  5.555556 #> 10  7.407407  9.259259 #> 25 24.074074 22.222222 #> 50 53.703704 59.259259 #> 75 81.481481 75.925926 #> 90 90.740741 85.185185 #> 95 94.444444 90.740741"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/roc.html","id":"a-primer-on-roc-curves","dir":"Articles","previous_headings":"","what":"A primer on ROC curves","title":"Tools for ROC curves","text":"wisclabmisc provides functions tidying results ROC curves. curves arise diagnostic classification settings want use test score determine whether individual belongs control group versus case group. binary classification normal versus clinical status, regular email versus spam status, . use terminology control case follow pROC package’s interface. classification literature, tons tons statistics describe classifier performance. ROC curve centers around two important quantities sensitivity specificity: Also called true positive rate recall. apply spam classifier 100 spam emails, many correctly flagged spam? P(case result | case status) Sensitivity makes sense think problem detecting something subtle. (Like Jedi “force sensitive” Spider-Man’s Spidey sense tingling ’s danger.) Also called true negative rate selectivity. apply spam classifier 100 safe (ham) emails, many correctly ignored? P(control result | control status) Specificity great term; selectivity makes slightly sense. don’t want sensor trip noise: needs specific selective. Suppose diagnostic instrument provides score, choose diagnostic threshold one scores. example, suppose decide scores 60 indicate email probably spam can moved spam folder. threshold specificity attached . can look proportion spam emails equal 60 (sensitivity), can look proportion ham emails 60 (specificity). number choose threshold sensitivity specificity score, ROC curve visualization sensitivity specificity change along range threshold scores. (impenetrable terminology: ROC stands “receiver operating characteristic”, something detections made radar receivers different operating levels.)","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/articles/roc.html","id":"a-worked-example","dir":"Articles","previous_headings":"A primer on ROC curves","what":"A worked example","title":"Tools for ROC curves","text":"can work example ROC curve using pROC package. pROC provides aSAH dataset provides “several clinical one laboratory variable 113 patients aneurysmal subarachnoid hemorrhage” (hence, aSAH). outcome (Good versus Poor) measure called s100b. can see many Good outcomes near 0 Poor outcomes.  point grid points along s100b, can compute proportions patients group threshold. can plot proportions visualize trading relations specificity sensitivity threshold changes.  took 5 tries get plot correct. able convince noting Good outcomes less .51 threshold catch single Good outcome hence specificity 1. Conversely, just Poor outcome 1, threshold 1 going detect 1 Poor outcome hence low sensitivity. ignore threshold visualization, can (finally) plot canonical ROC curve. shows specificity reversing order ideal point top left corner (sensitivity = 1, specificity = 1).  can compare plot one provided pROC package. find perfect match sensitivity specificity values.   Instead computing ROC curves hand, defer calculation ROC curves pROC package easy get confused calculating sensitivity specificity pROC provides tools working ROC curves. Thus, wisclabmisc’s goal ROC curves provide helper functions fit ROC curves pROC return results nice dataframe. contrast two types ROC curves: empirical ROC curve raw data used make jagged ROC curve (smooth) density ROC curve densities two distributions used make smooth ROC curve.","code":"data <- as_tibble(aSAH) data #> # A tibble: 113 × 7 #>    gos6  outcome gender   age wfns  s100b  ndka #>    <ord> <fct>   <fct>  <int> <ord> <dbl> <dbl> #>  1 5     Good    Female    42 1      0.13  3.01 #>  2 5     Good    Female    37 1      0.14  8.54 #>  3 5     Good    Female    42 1      0.1   8.09 #>  4 5     Good    Female    27 1      0.04 10.4  #>  5 1     Poor    Female    42 3      0.13 17.4  #>  6 1     Poor    Male      48 2      0.1  12.8  #>  7 4     Good    Male      57 5      0.47  6    #>  8 1     Poor    Male      41 4      0.16 13.2  #>  9 5     Good    Female    49 1      0.18 15.5  #> 10 4     Good    Female    75 2      0.1   6.01 #> # … with 103 more rows count(data, outcome) #> # A tibble: 2 × 2 #>   outcome     n #>   <fct>   <int> #> 1 Good       72 #> 2 Poor       41  ggplot(data) +    aes(x = s100b, y = outcome) +    geom_point(     position = position_jitter(width = 0, height = .2),     size = 3,     alpha = .2,   ) +   theme_grey(base_size = 12) +   labs(y = NULL) by_outcome <- split(data, data$outcome) smallest_diff <- min(diff(unique(sort(data$s100b)))) grid <- tibble(   threshold = seq(     min(data$s100b) - smallest_diff,      max(data$s100b) + smallest_diff,      length.out = 200   ) )  roc_coordinates <- grid %>%    rowwise() %>%    summarise(     threshold = threshold,     prop_poor_above = mean(by_outcome$Poor$s100b >= threshold),     prop_good_below = mean(by_outcome$Good$s100b < threshold),   )  ggplot(roc_coordinates) +    aes(x = threshold) +    geom_step(aes(y = prop_poor_above)) +    geom_step(aes(y = prop_good_below)) +   annotate(\"text\", x = 2, y = .9, hjust = 1, label = \"specificity\") +    annotate(\"text\", x = 2, y = .1, hjust = 1, label = \"sensitivity\") +   labs(     title = \"Sensitivity and specificity as cumulative proportions\",     x = \"threshold (diagnosis when score >= threshold)\",     y = NULL   ) roc_coordinates <- roc_coordinates %>%    rename(     sensitivities = prop_poor_above,      specificities = prop_good_below   ) %>%   # otherwise the stair-steps look wrong   arrange(sensitivities)  p <- ggplot(roc_coordinates) +    aes(x = specificities, y = sensitivities) +    geom_step() +   scale_x_reverse() +    coord_fixed() +    theme_grey(base_size = 14) p roc <- pROC::roc(data, response = outcome, predictor = s100b) #> Setting levels: control = Good, case = Poor #> Setting direction: controls < cases plot(roc) proc_coordinates <- roc[2:3] %>%    as.data.frame() %>%    arrange(sensitivities)  # Plot the pROC point as a wide semi-transparent blue # band on top of ours p +    geom_step(     data = proc_coordinates,      color = \"blue\",     alpha = .5,     size = 2   )"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/roc.html","id":"emprical-roc-curves","dir":"Articles","previous_headings":"","what":"Emprical ROC curves","title":"Tools for ROC curves","text":"Let’s return example, predicting group label outcome (case: Poor, control: Good) predictor s100b. messages, can see pROC::roc() makes decisions us: Good control level Poor case level, controls lower s100b cases. pROC::roc() returns roc object bundles data model results together. Ultimately, want results dataframe one row provide sensitivity specificity threshold value. can get close dataframe manipulating list using coords(). pROC::coords() additional features allow identify “best” ROC points, strips useful data like direction used. wisclabmisc provides compute_empirical_roc() combines results pROC::roc() pROC::coords() tibble. includes metadata .controls .cases levels, .direction relationship, overall .auc curve. also identifies two “best” coordinates .is_best_youden is_best_closest_topleft. Finally, retains name predictor variable. can still see messages emitted pROC::roc() call use compute_empirical_roc(). can pass arguments direction levels pROC::roc() silence messages. According help page pROC::coords() Youden’s J statistic point farthest vertical distance diagonal line. “best” point point closest upper-left corner. following plot labels distances. Youden’s point topleft point point.","code":"r <- pROC::roc(data, outcome, s100b) #> Setting levels: control = Good, case = Poor #> Setting direction: controls < cases r #>  #> Call: #> roc.data.frame(data = data, response = outcome, predictor = s100b) #>  #> Data: s100b in 72 controls (outcome Good) < 41 cases (outcome Poor). #> Area under the curve: 0.7314 r #>  #> Call: #> roc.data.frame(data = data, response = outcome, predictor = s100b) #>  #> Data: s100b in 72 controls (outcome Good) < 41 cases (outcome Poor). #> Area under the curve: 0.7314 class(r) #> [1] \"roc\" str(r, max.level = 1, give.attr = FALSE) #> List of 15 #>  $ percent           : logi FALSE #>  $ sensitivities     : num [1:51] 1 0.976 0.976 0.976 0.976 ... #>  $ specificities     : num [1:51] 0 0 0.0694 0.1111 0.1389 ... #>  $ thresholds        : num [1:51] -Inf 0.035 0.045 0.055 0.065 ... #>  $ direction         : chr \"<\" #>  $ cases             : num [1:41] 0.13 0.1 0.16 0.12 0.44 0.71 0.49 0.07 0.33 0.09 ... #>  $ controls          : num [1:72] 0.13 0.14 0.1 0.04 0.47 0.18 0.1 0.1 0.04 0.08 ... #>  $ fun.sesp          :function (thresholds, controls, cases, direction)   #>  $ auc               : 'auc' num 0.731 #>  $ call              : language roc.data.frame(data = data, response = outcome, predictor = s100b) #>  $ original.predictor: num [1:113] 0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ... #>  $ original.response : Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ... #>  $ predictor         : num [1:113] 0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ... #>  $ response          : Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ... #>  $ levels            : chr [1:2] \"Good\" \"Poor\" r[1:5] %>%    as.data.frame() %>%    tibble::as_tibble() #> # A tibble: 51 × 5 #>    percent sensitivities specificities thresholds direction #>    <lgl>           <dbl>         <dbl>      <dbl> <chr>     #>  1 FALSE           1            0        -Inf     <         #>  2 FALSE           0.976        0           0.035 <         #>  3 FALSE           0.976        0.0694      0.045 <         #>  4 FALSE           0.976        0.111       0.055 <         #>  5 FALSE           0.976        0.139       0.065 <         #>  6 FALSE           0.902        0.222       0.075 <         #>  7 FALSE           0.878        0.306       0.085 <         #>  8 FALSE           0.829        0.389       0.095 <         #>  9 FALSE           0.780        0.486       0.105 <         #> 10 FALSE           0.756        0.542       0.115 <         #> # … with 41 more rows  pROC::coords(r) %>%    tibble::as_tibble() #> # A tibble: 51 × 3 #>    threshold specificity sensitivity #>        <dbl>       <dbl>       <dbl> #>  1  -Inf          0            1     #>  2     0.035      0            0.976 #>  3     0.045      0.0694       0.976 #>  4     0.055      0.111        0.976 #>  5     0.065      0.139        0.976 #>  6     0.075      0.222        0.902 #>  7     0.085      0.306        0.878 #>  8     0.095      0.389        0.829 #>  9     0.105      0.486        0.780 #> 10     0.115      0.542        0.756 #> # … with 41 more rows compute_empirical_roc(data, outcome, s100b) #> Setting levels: control = Good, case = Poor #> Setting direction: controls < cases #> # A tibble: 51 × 9 #>       s100b .specificities .sensitivities  .auc .direction .controls .cases #>       <dbl>          <dbl>          <dbl> <dbl> <chr>      <chr>     <chr>  #>  1 -Inf             0               1     0.731 <          Good      Poor   #>  2    0.035         0               0.976 0.731 <          Good      Poor   #>  3    0.045         0.0694          0.976 0.731 <          Good      Poor   #>  4    0.055         0.111           0.976 0.731 <          Good      Poor   #>  5    0.065         0.139           0.976 0.731 <          Good      Poor   #>  6    0.075         0.222           0.902 0.731 <          Good      Poor   #>  7    0.085         0.306           0.878 0.731 <          Good      Poor   #>  8    0.095         0.389           0.829 0.731 <          Good      Poor   #>  9    0.105         0.486           0.780 0.731 <          Good      Poor   #> 10    0.115         0.542           0.756 0.731 <          Good      Poor   #> # … with 41 more rows, and 2 more variables: .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl> data_roc <- compute_empirical_roc(   data,    outcome,    s100b,    direction = \"<\",   levels = c(\"Good\", \"Poor\") ) data_roc #> # A tibble: 51 × 9 #>       s100b .specificities .sensitivities  .auc .direction .controls .cases #>       <dbl>          <dbl>          <dbl> <dbl> <chr>      <chr>     <chr>  #>  1 -Inf             0               1     0.731 <          Good      Poor   #>  2    0.035         0               0.976 0.731 <          Good      Poor   #>  3    0.045         0.0694          0.976 0.731 <          Good      Poor   #>  4    0.055         0.111           0.976 0.731 <          Good      Poor   #>  5    0.065         0.139           0.976 0.731 <          Good      Poor   #>  6    0.075         0.222           0.902 0.731 <          Good      Poor   #>  7    0.085         0.306           0.878 0.731 <          Good      Poor   #>  8    0.095         0.389           0.829 0.731 <          Good      Poor   #>  9    0.105         0.486           0.780 0.731 <          Good      Poor   #> 10    0.115         0.542           0.756 0.731 <          Good      Poor   #> # … with 41 more rows, and 2 more variables: .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl> data_roc <- data_roc %>%    arrange(.sensitivities)  p_best <- ggplot(data_roc) +    aes(x = .specificities, y = .sensitivities) +    geom_abline(     slope = 1,      intercept = 1,      linetype = \"dotted\",      color = \"grey20\"   ) +   geom_step() +    geom_segment(     aes(xend = .specificities, yend = 1 - .specificities),     data = . %>% filter(.is_best_youden),     color = \"blue\",     linetype = \"dashed\"   ) +    geom_segment(     aes(xend = 1, yend = 1),     data = . %>% filter(.is_best_closest_topleft),     color = \"maroon\",     linetype = \"dashed\"   ) +    # Basically, finding a point 9/10ths of the way   # along the line   geom_text(     aes(       x = weighted.mean(c(1, .specificities), c(9, 1)),        y = weighted.mean(c(1, .sensitivities), c(9, 1)),      ),     data = . %>% filter(.is_best_closest_topleft),     color = \"maroon\",     label = \"closest to topleft\",     hjust = 0,      nudge_x = .02,     size = 5   ) +    geom_text(     aes(       x = .specificities,        y = weighted.mean(c(1 - .specificities, .sensitivities), c(1, 2)),      ),     data = . %>% filter(.is_best_youden),     color = \"blue\",     label = \"Youden's J\\n(max height above diagonal)\",     hjust = 0,     vjust = .5,     nudge_x = .02,     size = 5   ) +    annotate(     \"text\",     x = .91,     y = .05,     hjust = 0,      size = 5,     label = \"diagonal: random classifier\",     color = \"grey20\"   ) +   scale_x_reverse() +   coord_fixed() +   theme_grey(base_size = 12) p_best"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/roc.html","id":"smooth-density-roc-curves","dir":"Articles","previous_headings":"","what":"(Smooth) density ROC curves","title":"Tools for ROC curves","text":"Instead looking observed data, let’s assume s100b values group drawn normal distribution means scales (standard deviations) different two groups. can compute group’s mean standard deviation plot normal density curves top . Pepe (2003) refers approach “binormal ROC curve”.  various points along x-axis range, stat_function() compute dnorm() (density normal curves). can hand . take full range data, within group, generate set points along range compute group’s density point. Next, pivot wide pivot format comparing two densities point. pROC::roc() can compute ROC curve densities. Note interface different. provide dataframe names columns data frame. Instead, provide two vectors densities, fact, densities lost computing ROC curve.  roc object returns coordinates sensitivity decreasing order, obvious map sensitivities back original densities. terms earlier density plot, don’t know whether sensitivities move x axis x axis. Let’s restate problem , clarity: want map thresholds densities ROC coordinates map ROC coordinates back densities thresholds. pROC::roc(density.controls, density.controls), hit brick wall map backwards ROC coordinates sensitivites may reversed respect densities. Fortunately, compute sensitivities hand, can figure coordinates ordered. try orderings find one best matches one provided pROC::roc(). < direction better matched ROC results, conclude sensitivities follow order densities. compute_smooth_density_roc() uses similar heuristic determine order ROC coordinates respect original densities. result, can map original threshold values sensitivity specificity values. function also lets us use column names directly. compute_smooth_density_roc() also provides coordinates “best” thresholds Youden topleft criteria. consistency two functions, can just replace data used make annotated ROC curve smoothed ROC coordinates. case, Youden topleft points different.  final demonstration, let’s compare smooth empirical ROC sensitivity specificity values along threshold values.","code":"data_stats <- data %>%    group_by(outcome) %>%    summarise(     mean = mean(s100b),     sd = sd(s100b)   )   l_control <- data_stats %>%    filter(outcome == \"Good\") %>%    as.list()  l_case <- data_stats %>%    filter(outcome != \"Good\") %>%    as.list()  ggplot(data) +    aes(x = s100b, color = outcome) +    # include a \"rug\" at the bottom   geom_jitter(aes(y = -.2), width = 0, height = .15, alpha = .4) +   stat_function(     data = . %>% filter(outcome == \"Good\"),     fun = dnorm,      args = list(mean = l_control$mean, sd = l_control$sd)   ) +   stat_function(     data = . %>% filter(outcome != \"Good\"),     fun = dnorm,      args = list(mean = l_case$mean, sd = l_case$sd)   ) +   geom_text(     aes(x = mean, y = dnorm(mean, mean, sd), label = outcome),     data = data_stats,     vjust = \"inward\",     hjust = 0,     nudge_x = .05,     nudge_y = .05,     size = 4   ) +   theme_grey(14) +   theme(legend.position = \"top\", legend.justification = \"left\") +   labs(y = NULL) +   guides(color = \"none\") data_grid <- data %>%    mutate(     xmin = min(s100b),     xmax = max(s100b)   ) %>%    group_by(outcome) %>%    summarise(     x = seq(xmin[1], xmax[1], length.out = 200),     group_mean = mean(s100b),     group_sd = sd(s100b),     density = dnorm(x, group_mean, group_sd),     .groups = \"drop\"   )  data_grid #> # A tibble: 400 × 5 #>    outcome      x group_mean group_sd density #>    <fct>    <dbl>      <dbl>    <dbl>   <dbl> #>  1 Good    0.03        0.162    0.131    1.84 #>  2 Good    0.0403      0.162    0.131    1.98 #>  3 Good    0.0505      0.162    0.131    2.13 #>  4 Good    0.0608      0.162    0.131    2.27 #>  5 Good    0.0710      0.162    0.131    2.40 #>  6 Good    0.0813      0.162    0.131    2.53 #>  7 Good    0.0915      0.162    0.131    2.64 #>  8 Good    0.102       0.162    0.131    2.75 #>  9 Good    0.112       0.162    0.131    2.84 #> 10 Good    0.122       0.162    0.131    2.91 #> # … with 390 more rows data_dens <- data_grid %>%    rename(s100b = x) %>%    select(-group_mean, -group_sd) %>%    pivot_wider(names_from = outcome, values_from = density) data_dens #> # A tibble: 200 × 3 #>     s100b  Good  Poor #>     <dbl> <dbl> <dbl> #>  1 0.03    1.84 0.659 #>  2 0.0403  1.98 0.676 #>  3 0.0505  2.13 0.694 #>  4 0.0608  2.27 0.711 #>  5 0.0710  2.40 0.729 #>  6 0.0813  2.53 0.746 #>  7 0.0915  2.64 0.763 #>  8 0.102   2.75 0.780 #>  9 0.112   2.84 0.797 #> 10 0.122   2.91 0.813 #> # … with 190 more rows data_dens <- arrange(data_dens, s100b) r_dens <- roc(   density.controls = data_dens$Good,    density.cases = data_dens$Poor ) r_dens #>  #> Call: #> roc.default(density.controls = data_dens$Good, density.cases = data_dens$Poor) #>  #> Smoothing: density with controls: data_dens$Good; and cases: data_dens$Poor #> Area under the curve: 0.8299 plot(r_dens) # direction > : Good > threshold >= Poor sens_gt <- rev(cumsum(data_dens$Poor) / sum(data_dens$Poor)) # direction < : Good < threshold <= Poor sens_lt <- 1 - (cumsum(data_dens$Poor) / sum(data_dens$Poor)) # The model did ?? fitted_sensitivities <- r_dens$sensitivities[-c(1, 201)]  mean(fitted_sensitivities - sens_lt) #> [1] 0.004999997 mean(fitted_sensitivities - sens_gt) #> [1] -0.530585 data_smooth <- compute_smooth_density_roc(   data = data_dens,    controls = Good,    cases = Poor,    along = s100b ) data_smooth #> # A tibble: 202 × 10 #>     s100b  Good  Poor .sensitivities .specificities  .auc .roc_row .direction #>     <dbl> <dbl> <dbl>          <dbl>          <dbl> <dbl>    <int> <chr>      #>  1 0.03    1.84 0.659          1             0      0.830        2 <          #>  2 0.0403  1.98 0.676          0.992         0.0221 0.830        3 <          #>  3 0.0505  2.13 0.694          0.984         0.0460 0.830        4 <          #>  4 0.0608  2.27 0.711          0.975         0.0716 0.830        5 <          #>  5 0.0710  2.40 0.729          0.967         0.0989 0.830        6 <          #>  6 0.0813  2.53 0.746          0.958         0.128  0.830        7 <          #>  7 0.0915  2.64 0.763          0.949         0.158  0.830        8 <          #>  8 0.102   2.75 0.780          0.939         0.190  0.830        9 <          #>  9 0.112   2.84 0.797          0.930         0.223  0.830       10 <          #> 10 0.122   2.91 0.813          0.920         0.257  0.830       11 <          #> # … with 192 more rows, and 2 more variables: .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl> p_best + list(data_smooth) ggplot(data_smooth) +    aes(x = s100b) +    geom_line(     aes(color = \"smooth\", linetype = \"smooth\", y = .sensitivities),   ) +    geom_line(     aes(color = \"empirical\", linetype = \"smooth\", y = .sensitivities),     data = data_roc   ) +    geom_line(     aes(color = \"smooth\", linetype = \"empirical\", y = .specificities)   ) +    geom_line(     aes(color = \"empirical\", linetype = \"empirical\", y = .specificities),     data = data_roc   ) +   annotate(\"text\", x = 2, y = .9, hjust = 1, label = \"specificity\") +    annotate(\"text\", x = 2, y = .1, hjust = 1, label = \"sensitivity\") +   labs(     color = \"ROC type\",      linetype = \"ROC type\",     y = NULL   ) +    theme_grey(base_size = 12) +    theme(legend.position = \"top\") #> Warning: Removed 2 row(s) containing missing values (geom_path). #> Removed 2 row(s) containing missing values (geom_path)."},{"path":"https://www.tjmahr.com/wisclabmisc/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Tristan Mahr. Author, maintainer.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Mahr T (2022). wisclabmisc: Tools Support 'WiscLab'. https://github.com/tjmahr/wisclabmisc, https://www.tjmahr.com/wisclabmisc/.","code":"@Manual{,   title = {wisclabmisc: Tools to Support the 'WiscLab'},   author = {Tristan Mahr},   year = {2022},   note = {https://github.com/tjmahr/wisclabmisc, https://www.tjmahr.com/wisclabmisc/}, }"},{"path":"https://www.tjmahr.com/wisclabmisc/index.html","id":"wisclabmisc","dir":"","previous_headings":"","what":"Tools to Support the WiscLab","title":"Tools to Support the WiscLab","text":"goal wisclabmisc reuse analysis functions across WISC Lab project projects provide share analysis code used projects.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Tools to Support the WiscLab","text":"can install development version wisclabmisc GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"tjmahr/wisclabmisc\")"},{"path":"https://www.tjmahr.com/wisclabmisc/index.html","id":"acknowledgments","dir":"","previous_headings":"","what":"Acknowledgments","title":"Tools to Support the WiscLab","text":"wisclabmisc created process data WISC Lab project. Thus, development package supported NIH R01DC009411 NIH R01DC015653.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert age in months to years;months — format_year_month_age","title":"Convert age in months to years;months — format_year_month_age","text":"Convert age months years;months","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert age in months to years;months — format_year_month_age","text":"","code":"format_year_month_age(x)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert age in months to years;months — format_year_month_age","text":"x vector ages months","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert age in months to years;months — format_year_month_age","text":"ages years;months format","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert age in months to years;months — format_year_month_age","text":"Ages NA return \"NA;NA\". format default numerically ordered. means c(\"2;0\", \"10;10\", \"10;9\") sort c(\"10;10\", \"10;9\", \"2;0\"). function stringr::str_sort(..., numeric = TRUE) sort vector correctly.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert age in months to years;months — format_year_month_age","text":"","code":"ages <- c(26, 58, 25, 67, 21, 59, 36, 43, 27, 49) format_year_month_age(ages) #>  [1] \"2;2\"  \"4;10\" \"2;1\"  \"5;7\"  \"1;9\"  \"4;11\" \"3;0\"  \"3;7\"  \"2;3\"  \"4;1\""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"function fits type GAMLSS model used Hustad colleagues (2021) 🔓: beta regression model (via gamlss.dist::()) natural cubic splines mean (mu) scale (sigma). model fitted using package's mem_gamlss() wrapper function.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"","code":"fit_beta_gamlss(data, var_x, var_y, df_mu = 3, df_sigma = 2, control = NULL)  fit_beta_gamlss_se(   data,   name_x,   name_y,   df_mu = 3,   df_sigma = 2,   control = NULL )  predict_beta_gamlss(newdata, model, centiles = c(5, 10, 50, 90, 95))  optimize_beta_gamlss_slope(   model,   centiles = 50,   interval = c(30, 119),   maximum = TRUE )  uniroot_beta_gamlss(model, centiles = 50, targets = 0.5, interval = c(30, 119))"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"Associated article: https://doi.org/10.1044/2021_JSLHR-21-00142","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"data data frame var_x, var_y (unquoted) variable names giving predictor variable (e.g., age) outcome variable (.e.g, intelligibility). df_mu, df_sigma degrees freedom control gamlss::gamlss.control() controller. Defaults NULL uses default settings, except setting trace FALSE silence output gamlss. name_x, name_y quoted variable names giving predictor variable (e.g., \"age\") outcome variable (.e.g, \"intelligibility\"). arguments apply fit_beta_gamlss_se(). newdata one-column dataframe predictions model model fitted fit_beta_gamlss() centiles centiles use prediction. Defaults c(5, 10, 50, 90, 95) predict_beta_gamlss(). Defaults 50 optimize_beta_gamlss_slope() uniroot_beta_gamlss(), although functions support multiple centile values. interval optimize_beta_gamlss_slope(), range x values optimize . uniroot_beta_gamlss(), range x values search roots (target y values) . maximum optimize_beta_gamlss_slope(), whether find maximum slope (TRUE) minimum slope (FALSE). targets uniroot_beta_gamlss(), target y values use roots. default, .5 used, uniroot_beta_gamlss() returns x value y value .5. Multiple targets supported.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"fit_beta_gamlss() fit_beta_gamlss_se(), mem_gamlss()-fitted model. .user data model includes degrees freedom parameter splines::ns() basis parameter. predict_beta_gamlss(), dataframe containing model predictions mu sigma, plus columns centile centiles. optimize_beta_gamlss_slope(), dataframe optimized x values (maximum minimum), gradient x value (objective), quantile (quantile). uniroot_beta_gamlss(), dataframe one row per quantile/target combination results calling stats::uniroot(). root column x value quantile curve crosses target value.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"two versions function. main version fit_beta_gamlss(), works unquoted column names (e.g., age). alternative version fit_beta_gamlss_se(); final \"se\" stands \"Standard Evaluation\". designation means variable names must given strings (, quoted \"age\" instead bare name age). alternative version necessary fit several models using parallel computing furrr::future_map() (using bootstrap resampling). predict_centiles() work function, likely throw warning message. Therefore, predict_beta_gamlss() provides alternative way compute centiles model. function manually computes centiles instead relying gamlss::centiles(). main difference new x values go splines::predict.ns() multiplied model coefficients. optimize_beta_gamlss_slope() computes point (.e., age) rate steepest growth different quantiles. function wraps following process: internal prediction function computes quantile x model coefficients spline bases. another internal function uses numDeriv::grad() get gradient prediction function x. optimize_beta_gamlss_slope() uses stats::optimize() gradient function find x maximum minimum slope. uniroot_beta_gamlss() also uses internal prediction function find quantile growth curve crosses given value. stats::uniroot() finds function crosses 0 (root). modify prediction function always subtract .5 end, root prediction function x value predicted value crosses .5. work, function used find, say, age (root) children 10th percentile (centiles) cross 50% intelligibility (targets).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"gamlss-does-beta-regression-differently","dir":"Reference","previous_headings":"","what":"GAMLSS does beta regression differently","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"part brief note GAMLSS uses different parameterization beta distribution beta family packages. canonical parameterization beta distribution uses shape parameters \\(\\alpha\\) \\(\\beta\\) probability density function: $$f(y;\\alpha,\\beta) = \\frac{1}{B(\\alpha,\\beta)} y^{\\alpha-1}(1-y)^{\\beta-1}$$ \\(B\\) beta function. beta regression, distribution reparameterized mean probability \\(\\mu\\) parameter represents spread around mean. GAMLSS (gamlss.dist::()), use scale parameter \\(\\sigma\\) (larger values mean spread around mean). Everywhere else—betareg::betareg() rstanarm::stan_betareg() vignette(\"betareg\", \"betareg\"), brms::Beta() vignette(\"brms_families\", \"brms\"), mgcv::betar()—precision parameter \\(\\phi\\) (larger values mean precision, less spread around mean). comparison: $$  \\text{betareg, brms, mgcv, etc.} \\\\  \\mu              = \\alpha / (\\alpha + \\beta) \\\\  \\phi             = \\alpha + b \\\\  \\textsf{E}(y)    = \\mu \\\\  \\textsf{VAR}(y)  = \\mu(1-\\mu)/(1 + \\phi) \\\\ $$ $$  \\text{GAMLSS} \\\\  \\mu             = \\alpha / (\\alpha + \\beta) \\\\  \\sigma          = (1 / (\\alpha + \\beta + 1))^.5 \\\\  \\textsf{E}(y)   = \\mu \\\\  \\textsf{VAR}(y) = \\mu(1-\\mu)\\sigma^2 $$","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"","code":"data_fake_intelligibility #> # A tibble: 200 × 2 #>    age_months intelligibility #>         <int>           <dbl> #>  1         28           0.539 #>  2         29           0.375 #>  3         31           0.221 #>  4         31           0.253 #>  5         32           0.276 #>  6         32           0.750 #>  7         32           0.820 #>  8         33           0.325 #>  9         33           0.446 #> 10         33           0.592 #> # … with 190 more rows  m <- fit_beta_gamlss(   data_fake_intelligibility,   age_months,   intelligibility )  # using \"qr\" in summary() just to suppress a warning message summary(m, type = \"qr\") #> ****************************************************************** #> Family:  c(\"BE\", \"Beta\")  #>  #> Call:  gamlss::gamlss(formula = intelligibility ~ ns(age_months, df = 3),   #>     sigma.formula = ~ns(age_months, df = 2), family = BE(), data = ~data_fake_intelligibility,   #>     control = list(c.crit = 0.001, n.cyc = 20, mu.step = 1, sigma.step = 1,   #>         nu.step = 1, tau.step = 1, gd.tol = Inf, iter = 0, trace = FALSE,   #>         autostep = TRUE, save = TRUE))  #>  #> Fitting method: RS()  #>  #> ------------------------------------------------------------------ #> Mu link function:  logit #> Mu Coefficients: #>                         Estimate Std. Error t value Pr(>|t|)     #> (Intercept)              -0.3414     0.2090  -1.634    0.104     #> ns(age_months, df = 3)1   2.4951     0.1896  13.162   <2e-16 *** #> ns(age_months, df = 3)2   5.0171     0.4960  10.116   <2e-16 *** #> ns(age_months, df = 3)3   3.1454     0.1746  18.017   <2e-16 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ------------------------------------------------------------------ #> Sigma link function:  logit #> Sigma Coefficients: #>                         Estimate Std. Error t value Pr(>|t|)     #> (Intercept)              -0.4380     0.1935  -2.264   0.0247 *   #> ns(age_months, df = 2)1  -1.8670     0.4175  -4.472 1.31e-05 *** #> ns(age_months, df = 2)2  -1.6193     0.2120  -7.639 9.29e-13 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ------------------------------------------------------------------ #> No. of observations in the fit:  200  #> Degrees of Freedom for the fit:  7 #>       Residual Deg. of Freedom:  193  #>                       at cycle:  8  #>   #> Global Deviance:     -510.3592  #>             AIC:     -496.3592  #>             SBC:     -473.271  #> ******************************************************************  # Alternative interface d <- data_fake_intelligibility m2 <- fit_beta_gamlss_se(   data = d,   name_x = \"age_months\",   name_y = \"intelligibility\" ) coef(m2) == coef(m) #>             (Intercept) ns(age_months, df = 3)1 ns(age_months, df = 3)2  #>                    TRUE                    TRUE                    TRUE  #> ns(age_months, df = 3)3  #>                    TRUE   # how to use control to change gamlss() behavior m_traced <- fit_beta_gamlss(   data_fake_intelligibility,   age_months,   intelligibility,   control = gamlss::gamlss.control(n.cyc = 15, trace = TRUE) ) #> GAMLSS-RS iteration 1: Global Deviance = -394.1232  #> GAMLSS-RS iteration 2: Global Deviance = -456.3473  #> GAMLSS-RS iteration 3: Global Deviance = -489.514  #> GAMLSS-RS iteration 4: Global Deviance = -506.4194  #> GAMLSS-RS iteration 5: Global Deviance = -510.1204  #> GAMLSS-RS iteration 6: Global Deviance = -510.3517  #> GAMLSS-RS iteration 7: Global Deviance = -510.359  #> GAMLSS-RS iteration 8: Global Deviance = -510.3592   # The `.user` space includes the spline bases, so that we can make accurate # predictions of new xs. names(m$.user) #> [1] \"data\"         \"session_info\" \"call\"         \"df_mu\"        \"df_sigma\"     #> [6] \"basis_mu\"     \"basis_sigma\"   # predict logit(mean) at 55 months: logit_mean_55 <- cbind(1, predict(m$.user$basis_mu, 55)) %*% coef(m) logit_mean_55 #>          [,1] #> [1,] 1.627416 stats::plogis(logit_mean_55) #>           [,1] #> [1,] 0.8358153  # But predict_gen_gamma_gamlss() does this work for us and also provides # centiles new_ages <- data.frame(age_months = 48:71) centiles <- predict_beta_gamlss(new_ages, m) centiles #> # A tibble: 24 × 8 #>    age_months    mu sigma    c5   c10   c50   c90   c95 #>         <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #>  1         48 0.761 0.296 0.527 0.586 0.778 0.912 0.936 #>  2         49 0.773 0.291 0.547 0.605 0.791 0.918 0.941 #>  3         50 0.785 0.286 0.566 0.623 0.803 0.924 0.946 #>  4         51 0.796 0.281 0.584 0.640 0.814 0.929 0.950 #>  5         52 0.807 0.276 0.602 0.656 0.824 0.934 0.953 #>  6         53 0.817 0.271 0.619 0.672 0.834 0.938 0.957 #>  7         54 0.827 0.266 0.636 0.687 0.844 0.943 0.960 #>  8         55 0.836 0.261 0.652 0.702 0.852 0.946 0.963 #>  9         56 0.844 0.256 0.668 0.716 0.861 0.950 0.965 #> 10         57 0.852 0.251 0.683 0.730 0.868 0.953 0.968 #> # … with 14 more rows  # Confirm that the manual prediction matches the automatic one centiles[centiles$age_months == 55, \"mu\"] #> # A tibble: 1 × 1 #>      mu #>   <dbl> #> 1 0.836 stats::plogis(logit_mean_55) #>           [,1] #> [1,] 0.8358153  if(requireNamespace(\"ggplot2\", quietly = TRUE)) {   library(ggplot2)   ggplot(pivot_centiles_longer(centiles)) +     aes(x = age_months, y = .value) +     geom_line(aes(group = .centile, color = .centile_pair)) +     geom_point(       aes(y = intelligibility),       data = subset(         data_fake_intelligibility,         48 <= age_months & age_months <= 71       )     ) }   # Age of steepest growth for each centile optimize_beta_gamlss_slope(   m,   centiles = c(5, 10, 50, 90),   interval = range(data_fake_intelligibility$age_months) ) #> # A tibble: 4 × 3 #>   maximum objective quantile #>     <dbl>     <dbl>    <dbl> #> 1    40.0    0.0222     0.05 #> 2    37.7    0.0225     0.1  #> 3    29.8    0.0217     0.5  #> 4    28.0    0.0153     0.9   # Manual approach: Make fine grid of predictions and find largest jump fine_centiles <- predict_beta_gamlss(   data.frame(age_months = seq(28, 95, length.out = 1000)),   m ) fine_centiles[which.max(diff(fine_centiles$c5)), \"age_months\"] #> # A tibble: 1 × 1 #>   age_months #>        <dbl> #> 1       39.9  # When do children in different centiles reach 50%, 70% intelligibility? uniroot_beta_gamlss(   model,   centiles = c(5, 10, 50),   targets = c(.5, .7) ) #> Error in uniroot_beta_gamlss(model, centiles = c(5, 10, 50), targets = c(0.5,     0.7)): object 'model' not found"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/check_sample_centiles.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the percentage of points under each centile line — check_sample_centiles","title":"Compute the percentage of points under each centile line — check_sample_centiles","text":"Compute percentage points centile line","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/check_sample_centiles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the percentage of points under each centile line — check_sample_centiles","text":"","code":"check_sample_centiles(   data,   model,   var_x,   var_y,   centiles = c(5, 10, 25, 50, 75, 90, 95) )"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/check_sample_centiles.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the percentage of points under each centile line — check_sample_centiles","text":"data dataset used fit model. dataframe grouped dplyr::group_by(), sample centiles computed group. model gamlss model prepared mem_gamlss() var_x, var_y bare column names predictor outcome variables centiles centiles use prediction. Defaults c(5, 10, 25, 50, 75, 90, 95).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/check_sample_centiles.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the percentage of points under each centile line — check_sample_centiles","text":"tibble number points percentage points less equal quantile value.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_empirical_roc.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an ROC curve from observed data — compute_empirical_roc","title":"Create an ROC curve from observed data — compute_empirical_roc","text":"Create ROC curve observed data","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_empirical_roc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an ROC curve from observed data — compute_empirical_roc","text":"","code":"compute_empirical_roc(   data,   response,   predictor,   direction = \"auto\",   best_weights = c(1, 0.5),   ... )"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_empirical_roc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an ROC curve from observed data — compute_empirical_roc","text":"data dataframe containing responses (groupings) predictor variable response bare column name group status (control vs. cases) predictor bare column name predictor use classification direction direction set pROC::roc(). Defaults \"auto\". best_weights weights computing best ROC curve points. Defaults c(1, .5), defaults used pROC::coords(). ... additional arguments passed pROC::roc().","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_empirical_roc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an ROC curve from observed data — compute_empirical_roc","text":"new dataframe ROC coordinates returned columns predictor variable, .sensitivities, .specificities, .auc, .direction, .controls, .cases, .is_best_youden .is_best_closest_topleft.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_empirical_roc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an ROC curve from observed data — compute_empirical_roc","text":"","code":"set.seed(100) x1 <- rnorm(100, 4, 1) x2 <- rnorm(100, 2, .5) both <- c(x1, x2) steps <- seq(min(both), max(both), length.out = 200) d1 <- dnorm(steps, mean(x1), sd(x1)) d2 <- dnorm(steps, mean(x2), sd(x2)) data <- tibble::tibble(   y = steps,   d1 = d1,   d2 = d2,   outcome = rbinom(200, 1, prob = 1 - (d1 / (d1 + d2))),   group = ifelse(outcome, \"case\", \"control\") )  # get an ROC on the fake data compute_empirical_roc(data, outcome, y) #> Setting levels: control = 0, case = 1 #> Setting direction: controls > cases #> # A tibble: 201 × 9 #>         y .specificities .sensitivities  .auc .direction .controls .cases #>     <dbl>          <dbl>          <dbl> <dbl> <chr>      <chr>     <chr>  #>  1 Inf           0                    1 0.979 >          0         1      #>  2   6.57        0.00704              1 0.979 >          0         1      #>  3   6.54        0.0141               1 0.979 >          0         1      #>  4   6.51        0.0211               1 0.979 >          0         1      #>  5   6.48        0.0282               1 0.979 >          0         1      #>  6   6.45        0.0352               1 0.979 >          0         1      #>  7   6.43        0.0423               1 0.979 >          0         1      #>  8   6.40        0.0493               1 0.979 >          0         1      #>  9   6.37        0.0563               1 0.979 >          0         1      #> 10   6.34        0.0634               1 0.979 >          0         1      #> # … with 191 more rows, and 2 more variables: .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl> # this guess the cases and controls from the group name and gets it wrong compute_empirical_roc(data, group, y) #> Setting levels: control = case, case = control #> Setting direction: controls < cases #> # A tibble: 201 × 9 #>           y .specificities .sensitivities  .auc .direction .controls .cases  #>       <dbl>          <dbl>          <dbl> <dbl> <chr>      <chr>     <chr>   #>  1 -Inf             0                   1 0.979 <          case      control #>  2    0.946         0.0172              1 0.979 <          case      control #>  3    0.974         0.0345              1 0.979 <          case      control #>  4    1.00          0.0517              1 0.979 <          case      control #>  5    1.03          0.0690              1 0.979 <          case      control #>  6    1.06          0.0862              1 0.979 <          case      control #>  7    1.09          0.103               1 0.979 <          case      control #>  8    1.12          0.121               1 0.979 <          case      control #>  9    1.14          0.138               1 0.979 <          case      control #> 10    1.17          0.155               1 0.979 <          case      control #> # … with 191 more rows, and 2 more variables: .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl> # better compute_empirical_roc(data, group, y, levels = c(\"control\", \"case\")) #> Setting direction: controls > cases #> # A tibble: 201 × 9 #>         y .specificities .sensitivities  .auc .direction .controls .cases #>     <dbl>          <dbl>          <dbl> <dbl> <chr>      <chr>     <chr>  #>  1 Inf           0                    1 0.979 >          control   case   #>  2   6.57        0.00704              1 0.979 >          control   case   #>  3   6.54        0.0141               1 0.979 >          control   case   #>  4   6.51        0.0211               1 0.979 >          control   case   #>  5   6.48        0.0282               1 0.979 >          control   case   #>  6   6.45        0.0352               1 0.979 >          control   case   #>  7   6.43        0.0423               1 0.979 >          control   case   #>  8   6.40        0.0493               1 0.979 >          control   case   #>  9   6.37        0.0563               1 0.979 >          control   case   #> 10   6.34        0.0634               1 0.979 >          control   case   #> # … with 191 more rows, and 2 more variables: .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl>"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute positive and negative predictive value — compute_predictive_value_from_rates","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"Compute positive negative predictive value","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"","code":"compute_predictive_value_from_rates(sensitivity, specificity, prevalence)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"sensitivity, specificity, prevalence vectors confusion matrix rates","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"tibble columns sensitivity, specificity, prevalence, ppv, npv ppv npv positive predictive value negative predictive value.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"vectors passed function common length /length 1. example, 4 sensitivities, 4 specificities 1 incidence work sensitivities specificities common length can safely recycle (reuse) incidence value. 4 sensitivities, 2 specificities, 1 incidence fail common length.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"","code":"compute_predictive_value_from_rates(   sensitivity = .9,   specificity = .8,   prevalence = .05 ) #> # A tibble: 1 × 5 #>   sensitivity specificity prevalence   ppv   npv #>         <dbl>       <dbl>      <dbl> <dbl> <dbl> #> 1         0.9         0.8       0.05 0.191 0.993  compute_predictive_value_from_rates(   sensitivity = .67,   specificity = .53,   prevalence = c(.15, .3) ) #> # A tibble: 2 × 5 #>   sensitivity specificity prevalence   ppv   npv #>         <dbl>       <dbl>      <dbl> <dbl> <dbl> #> 1        0.67        0.53       0.15 0.201 0.901 #> 2        0.67        0.53       0.3  0.379 0.789"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_smooth_density_roc.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","title":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","text":"Create ROC curve smoothed densities","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_smooth_density_roc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","text":"","code":"compute_smooth_density_roc(   data,   controls,   cases,   along = NULL,   best_weights = c(1, 0.5),   direction = \"auto\",   ... )"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_smooth_density_roc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","text":"data dataframe containing densities controls, cases bare column name densities control group along optional bare column name response values best_weights weights computing best ROC curve points. Defaults c(1, .5), defaults used pROC::coords(). direction direction set pROC::roc(). Defaults \"auto\". ... additional arguments. used currently.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_smooth_density_roc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","text":"dataframe updated new columns .sensitivities, .specificities, .auc, .roc_row, .is_best_youden .is_best_closest_topleft.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_smooth_density_roc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","text":"","code":"set.seed(100) x1 <- rnorm(100, 4, 1) x2 <- rnorm(100, 2, .5) both <- c(x1, x2) steps <- seq(min(both), max(both), length.out = 200) d1 <- dnorm(steps, mean(x1), sd(x1)) d2 <- dnorm(steps, mean(x2), sd(x2)) data <- tibble::tibble(   y = steps,   d1 = d1,   d2 = d2,   outcome = rbinom(200, 1, prob = 1 - (d1 / (d1 + d2))),   group = ifelse(outcome, \"case\", \"control\") ) compute_smooth_density_roc(data, d1, d2) #> # A tibble: 202 × 12 #>        y      d1     d2 outcome group .sensitivities .specificities  .auc #>    <dbl>   <dbl>  <dbl>   <int> <chr>          <dbl>          <dbl> <dbl> #>  1 0.932 0.00423 0.0264       1 case        0.000751          1.00  0.967 #>  2 0.960 0.00460 0.0319       1 case        0.00166           1.00  0.967 #>  3 0.989 0.00499 0.0383       1 case        0.00275           1.00  0.967 #>  4 1.02  0.00542 0.0459       1 case        0.00406           0.999 0.967 #>  5 1.05  0.00587 0.0546       1 case        0.00561           0.999 0.967 #>  6 1.07  0.00636 0.0647       1 case        0.00746           0.999 0.967 #>  7 1.10  0.00689 0.0763       1 case        0.00963           0.999 0.967 #>  8 1.13  0.00745 0.0895       1 case        0.0122            0.999 0.967 #>  9 1.16  0.00806 0.104        1 case        0.0152            0.998 0.967 #> 10 1.19  0.00870 0.121        1 case        0.0186            0.998 0.967 #> # … with 192 more rows, and 4 more variables: .roc_row <int>, .direction <chr>, #> #   .is_best_youden <lgl>, .is_best_closest_topleft <lgl> compute_smooth_density_roc(data, d1, d2, along = y) #> # A tibble: 202 × 12 #>        y      d1     d2 outcome group .sensitivities .specificities  .auc #>    <dbl>   <dbl>  <dbl>   <int> <chr>          <dbl>          <dbl> <dbl> #>  1 0.932 0.00423 0.0264       1 case        0.000751          1.00  0.967 #>  2 0.960 0.00460 0.0319       1 case        0.00166           1.00  0.967 #>  3 0.989 0.00499 0.0383       1 case        0.00275           1.00  0.967 #>  4 1.02  0.00542 0.0459       1 case        0.00406           0.999 0.967 #>  5 1.05  0.00587 0.0546       1 case        0.00561           0.999 0.967 #>  6 1.07  0.00636 0.0647       1 case        0.00746           0.999 0.967 #>  7 1.10  0.00689 0.0763       1 case        0.00963           0.999 0.967 #>  8 1.13  0.00745 0.0895       1 case        0.0122            0.999 0.967 #>  9 1.16  0.00806 0.104        1 case        0.0152            0.998 0.967 #> 10 1.19  0.00870 0.121        1 case        0.0186            0.998 0.967 #> # … with 192 more rows, and 4 more variables: .roc_row <int>, .direction <chr>, #> #   .is_best_youden <lgl>, .is_best_closest_topleft <lgl>  # terrible ROC because the response is not present (just the densities) data_shuffled <- data[sample(seq_len(nrow(data))), ] compute_smooth_density_roc(data_shuffled, d1, d2) #> # A tibble: 202 × 12 #>        y     d1       d2 outcome group   .sensitivities .specificities  .auc #>    <dbl>  <dbl>    <dbl>   <int> <chr>            <dbl>          <dbl> <dbl> #>  1  2.29 0.0963 7.70e- 1       1 case            0.0219          0.997 0.551 #>  2  3.43 0.334  1.66e- 3       0 control         0.0220          0.988 0.551 #>  3  4.40 0.363  1.49e- 8       0 control         0.0220          0.977 0.551 #>  4  6.33 0.0293 2.63e-26       0 control         0.0220          0.976 0.551 #>  5  6.35 0.0275 1.21e-26       0 control         0.0220          0.976 0.551 #>  6  5.59 0.117  2.59e-18       0 control         0.0220          0.972 0.551 #>  7  3.00 0.242  4.30e- 2       0 control         0.0232          0.965 0.551 #>  8  3.26 0.300  7.00e- 3       1 case            0.0234          0.957 0.551 #>  9  3.20 0.288  1.09e- 2       0 control         0.0237          0.949 0.551 #> 10  4.20 0.384  2.64e- 7       0 control         0.0237          0.938 0.551 #> # … with 192 more rows, and 4 more variables: .roc_row <int>, .direction <chr>, #> #   .is_best_youden <lgl>, .is_best_closest_topleft <lgl>  # sorted along response first: correct AUC compute_smooth_density_roc(data_shuffled, d1, d2, along = y) #> # A tibble: 202 × 12 #>        y     d1       d2 outcome group   .sensitivities .specificities  .auc #>    <dbl>  <dbl>    <dbl>   <int> <chr>            <dbl>          <dbl> <dbl> #>  1  2.29 0.0963 7.70e- 1       1 case             0.776        0.952   0.967 #>  2  3.43 0.334  1.66e- 3       0 control          1.00         0.707   0.967 #>  3  4.40 0.363  1.49e- 8       0 control          1.00         0.342   0.967 #>  4  6.33 0.0293 2.63e-26       0 control          1            0.00551 0.967 #>  5  6.35 0.0275 1.21e-26       0 control          1            0.00472 0.967 #>  6  5.59 0.117  2.59e-18       0 control          1            0.0534  0.967 #>  7  3.00 0.242  4.30e- 2       0 control          0.995        0.833   0.967 #>  8  3.26 0.300  7.00e- 3       1 case             0.999        0.762   0.967 #>  9  3.20 0.288  1.09e- 2       0 control          0.999        0.779   0.967 #> 10  4.20 0.384  2.64e- 7       0 control          1.00         0.416   0.967 #> # … with 192 more rows, and 4 more variables: .roc_row <int>, .direction <chr>, #> #   .is_best_youden <lgl>, .is_best_closest_topleft <lgl>"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_intelligibility.html","id":null,"dir":"Reference","previous_headings":"","what":"Fake intelligibility data — data_fake_intelligibility","title":"Fake intelligibility data — data_fake_intelligibility","text":"dataset fake intelligibility scores testing demonstrating modeling functions. created randomly sampling 200 rows intelligibility dataset adding random noise age_months intelligibility variables. values measure real children represent plausible age intelligibility measurements kind work.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_intelligibility.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fake intelligibility data — data_fake_intelligibility","text":"","code":"data_fake_intelligibility"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_intelligibility.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fake intelligibility data — data_fake_intelligibility","text":"data frame 200 rows 2 variables: age_months child's age months intelligibility child's intelligibility (proportion words said child correctly transcribed two listeners)","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_rates.html","id":null,"dir":"Reference","previous_headings":"","what":"Fake speaking rate data — data_fake_rates","title":"Fake speaking rate data — data_fake_rates","text":"dataset fake speaking rate measures testing demonstrating modeling functions. created randomly sampling 200 rows speaking rate dataset adding random noise age_months speaking_sps variables. values measure real children represent plausible age rate measurements kind work.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_rates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fake speaking rate data — data_fake_rates","text":"","code":"data_fake_rates"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_rates.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fake speaking rate data — data_fake_rates","text":"data frame 200 rows 2 variables: age_months child's age months speaking_sps child's speaking rate syllables per second","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"function fits type GAMLSS model used Mahr colleagues (2021) 🔓: generalized gamma regression model (via gamlss.dist::GG()) natural cubic splines mean (mu), scale (sigma), shape (nu) distribution. model fitted using package's mem_gamlss() wrapper function.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"","code":"fit_gen_gamma_gamlss(   data,   var_x,   var_y,   df_mu = 3,   df_sigma = 2,   df_nu = 1,   control = NULL )  fit_gen_gamma_gamlss_se(   data,   name_x,   name_y,   df_mu = 3,   df_sigma = 2,   df_nu = 1,   control = NULL )  predict_gen_gamma_gamlss(newdata, model, centiles = c(5, 10, 50, 90, 95))"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"Associated article: https://doi.org/10.1044/2021_JSLHR-21-00206","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"data data frame var_x, var_y (unquoted) variable names giving predictor variable (e.g., age) outcome variable (.e.g, rate). df_mu, df_sigma, df_nu degrees freedom control gamlss::gamlss.control() controller. Defaults NULL uses default settings, except setting trace FALSE silence output gamlss. name_x, name_y quoted variable names giving predictor variable (e.g., \"age\") outcome variable (.e.g, \"rate\"). arguments apply fit_gen_gamma_gamlss_se(). newdata one-column dataframe predictions model model fitted fit_gen_gamma_gamlss() centiles centiles use prediction. Defaults c(5, 10, 50, 90, 95).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"fit_gen_gamma_gamlss() fit_gen_gamma_gamlss_se(), mem_gamlss()-fitted model. .user data model includes degrees freedom parameter splines::ns() basis parameter. predict_gen_gamma_gamlss(), dataframe containing model predictions mu, sigma, nu, plus columns centile centiles.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"two versions function. main version fit_gen_gamma_gamlss(), works unquoted column names (e.g., age). alternative version fit_gen_gamma_gamlss_se(); final \"se\" stands \"Standard Evaluation\". designation means variable names must given strings (, quoted \"age\" instead bare name age). alternative version necessary fit several models using parallel computing furrr::future_map() (using bootstrap resampling). predict_centiles() work function, likely throw warning message. Therefore, predict_gen_gamma_gamlss() provides alternative way compute centiles model. function manually computes centiles instead relying gamlss::centiles(). main difference new x values go splines::predict.ns() multiplied model coefficients.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"","code":"data_fake_rates #> # A tibble: 200 × 2 #>    age_months speaking_sps #>         <int>        <dbl> #>  1         66         3.76 #>  2         29         2.08 #>  3         90         3.07 #>  4         61         2.64 #>  5         46         3.54 #>  6         61         3.23 #>  7         63         3.55 #>  8         51         2.84 #>  9         48         3.24 #> 10         37         2.39 #> # … with 190 more rows  m <- fit_gen_gamma_gamlss(data_fake_rates, age_months, speaking_sps)  # using \"qr\" in summary() just to suppress a warning message summary(m, type = \"qr\") #> ****************************************************************** #> Family:  c(\"GG\", \"generalised Gamma Lopatatsidis-Green\")  #>  #> Call:  gamlss::gamlss(formula = speaking_sps ~ ns(age_months, df = 3),   #>     sigma.formula = ~ns(age_months, df = 2), nu.formula = ~ns(age_months,   #>         df = 1), family = GG(), data = ~data_fake_rates, control = list(  #>         c.crit = 0.001, n.cyc = 20, mu.step = 1, sigma.step = 1,   #>         nu.step = 1, tau.step = 1, gd.tol = Inf, iter = 0, trace = FALSE,   #>         autostep = TRUE, save = TRUE))  #>  #> Fitting method: RS()  #>  #> ------------------------------------------------------------------ #> Mu link function:  log #> Mu Coefficients: #>                         Estimate Std. Error t value Pr(>|t|)     #> (Intercept)              0.92763    0.04539  20.435  < 2e-16 *** #> ns(age_months, df = 3)1  0.14393    0.03919   3.672 0.000310 *** #> ns(age_months, df = 3)2  0.36779    0.10288   3.575 0.000441 *** #> ns(age_months, df = 3)3  0.20240    0.03780   5.355 2.38e-07 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ------------------------------------------------------------------ #> Sigma link function:  log #> Sigma Coefficients: #>                         Estimate Std. Error t value Pr(>|t|)     #> (Intercept)              -1.7623     0.1597 -11.038   <2e-16 *** #> ns(age_months, df = 2)1  -0.6923     0.3379  -2.049   0.0418 *   #> ns(age_months, df = 2)2  -0.3832     0.2073  -1.848   0.0661 .   #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ------------------------------------------------------------------ #> Nu link function:  identity  #> Nu Coefficients: #>                        Estimate Std. Error t value Pr(>|t|)   #> (Intercept)              -3.438      1.647  -2.088   0.0381 * #> ns(age_months, df = 1)    8.336      4.312   1.933   0.0547 . #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ------------------------------------------------------------------ #> No. of observations in the fit:  200  #> Degrees of Freedom for the fit:  9 #>       Residual Deg. of Freedom:  191  #>                       at cycle:  14  #>   #> Global Deviance:     184.5313  #>             AIC:     202.5313  #>             SBC:     232.2161  #> ******************************************************************  # Alternative interface d <- data_fake_rates m2 <- fit_gen_gamma_gamlss_se(   data = d,   name_x = \"age_months\",   name_y = \"speaking_sps\" ) coef(m2) == coef(m) #>             (Intercept) ns(age_months, df = 3)1 ns(age_months, df = 3)2  #>                    TRUE                    TRUE                    TRUE  #> ns(age_months, df = 3)3  #>                    TRUE   # how to use control to change gamlss() behavior m_traced <- fit_gen_gamma_gamlss(   data_fake_rates,   age_months,   speaking_sps,   control = gamlss::gamlss.control(n.cyc = 15, trace = TRUE) ) #> GAMLSS-RS iteration 1: Global Deviance = 185.9307  #> GAMLSS-RS iteration 2: Global Deviance = 185.2313  #> GAMLSS-RS iteration 3: Global Deviance = 184.9111  #> GAMLSS-RS iteration 4: Global Deviance = 184.7408  #> GAMLSS-RS iteration 5: Global Deviance = 184.6482  #> GAMLSS-RS iteration 6: Global Deviance = 184.5971  #> GAMLSS-RS iteration 7: Global Deviance = 184.5691  #> GAMLSS-RS iteration 8: Global Deviance = 184.553  #> GAMLSS-RS iteration 9: Global Deviance = 184.5436  #> GAMLSS-RS iteration 10: Global Deviance = 184.5381  #> GAMLSS-RS iteration 11: Global Deviance = 184.5348  #> GAMLSS-RS iteration 12: Global Deviance = 184.5329  #> GAMLSS-RS iteration 13: Global Deviance = 184.5319  #> GAMLSS-RS iteration 14: Global Deviance = 184.5313   # The `.user` space includes the spline bases, so that we can make accurate # predictions of new xs. names(m$.user) #> [1] \"data\"         \"session_info\" \"call\"         \"df_mu\"        \"df_sigma\"     #> [6] \"df_nu\"        \"basis_mu\"     \"basis_sigma\"  \"basis_nu\"      # predict log(mean) at 55 months: log_mean_55 <- cbind(1, predict(m$.user$basis_mu, 55)) %*% coef(m) log_mean_55 #>         [,1] #> [1,] 1.07022 exp(log_mean_55) #>          [,1] #> [1,] 2.916022  # But predict_gen_gamma_gamlss() does this work for us and also provides # centiles new_ages <- data.frame(age_months = 48:71) centiles <- predict_gen_gamma_gamlss(new_ages, m) centiles #> # A tibble: 24 × 9 #>    age_months    mu sigma     nu    c5   c10   c50   c90   c95 #>         <int> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #>  1         48  2.83 0.142 -1.60   2.29  2.40  2.86  3.47  3.68 #>  2         49  2.84 0.141 -1.50   2.30  2.41  2.87  3.48  3.68 #>  3         50  2.86 0.140 -1.40   2.32  2.43  2.88  3.48  3.68 #>  4         51  2.87 0.138 -1.31   2.33  2.44  2.89  3.48  3.68 #>  5         52  2.88 0.137 -1.21   2.34  2.45  2.90  3.49  3.68 #>  6         53  2.89 0.136 -1.11   2.35  2.46  2.91  3.49  3.68 #>  7         54  2.91 0.135 -1.02   2.36  2.47  2.92  3.49  3.68 #>  8         55  2.92 0.134 -0.920  2.37  2.48  2.93  3.50  3.68 #>  9         56  2.93 0.132 -0.823  2.38  2.49  2.94  3.50  3.68 #> 10         57  2.94 0.131 -0.726  2.39  2.50  2.95  3.50  3.68 #> # … with 14 more rows  # Confirm that the manual prediction matches the automatic one centiles[centiles$age_months == 55, \"mu\"] #> # A tibble: 1 × 1 #>      mu #>   <dbl> #> 1  2.92 exp(log_mean_55) #>          [,1] #> [1,] 2.916022  if(requireNamespace(\"ggplot2\", quietly = TRUE)) {   library(ggplot2)   ggplot(pivot_centiles_longer(centiles)) +     aes(x = age_months, y = .value) +     geom_line(aes(group = .centile, color = .centile_pair)) +     geom_point(       aes(y = speaking_sps),       data = subset(         data_fake_rates,         48 <= age_months & age_months <= 71       )     ) }"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/join_to_split.html","id":null,"dir":"Reference","previous_headings":"","what":"Join data onto resampled IDs — join_to_split","title":"Join data onto resampled IDs — join_to_split","text":"Join data onto resampled IDs","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/join_to_split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Join data onto resampled IDs — join_to_split","text":"","code":"join_to_split(x, y, by, validate = FALSE)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/join_to_split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Join data onto resampled IDs — join_to_split","text":"x rset object created rsample::bootstraps() y y dataframe column id values resampled create x name column y data validate whether validate join counting number rows associated id. Defaults FALSE.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/join_to_split.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Join data onto resampled IDs — join_to_split","text":"original rset object x$data updated join y row numbers x$in_id updated work expanded dataset.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/join_to_split.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Join data onto resampled IDs — join_to_split","text":"","code":"library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union data_trees <- tibble::as_tibble(datasets::Orange)  data_tree_ids <- distinct(data_trees, Tree)  # Resample ids data_bootstraps <- data_tree_ids %>%   rsample::bootstraps(times = 20) %>%   rename(splits_id = splits) %>%   # Attach data to resampled ids   mutate(     data_splits = splits_id %>% purrr::map(       join_to_split,       data_trees,       by = \"Tree\",       validate = TRUE     )   )  data_bootstraps #> # A tibble: 20 × 3 #>    splits_id     id          data_splits     #>    <list>        <chr>       <list>          #>  1 <split [5/2]> Bootstrap01 <split [35/14]> #>  2 <split [5/1]> Bootstrap02 <split [35/7]>  #>  3 <split [5/1]> Bootstrap03 <split [35/7]>  #>  4 <split [5/1]> Bootstrap04 <split [35/7]>  #>  5 <split [5/2]> Bootstrap05 <split [35/14]> #>  6 <split [5/2]> Bootstrap06 <split [35/14]> #>  7 <split [5/2]> Bootstrap07 <split [35/14]> #>  8 <split [5/3]> Bootstrap08 <split [35/21]> #>  9 <split [5/3]> Bootstrap09 <split [35/21]> #> 10 <split [5/2]> Bootstrap10 <split [35/14]> #> 11 <split [5/2]> Bootstrap11 <split [35/14]> #> 12 <split [5/1]> Bootstrap12 <split [35/7]>  #> 13 <split [5/2]> Bootstrap13 <split [35/14]> #> 14 <split [5/1]> Bootstrap14 <split [35/7]>  #> 15 <split [5/1]> Bootstrap15 <split [35/7]>  #> 16 <split [5/3]> Bootstrap16 <split [35/21]> #> 17 <split [5/2]> Bootstrap17 <split [35/14]> #> 18 <split [5/2]> Bootstrap18 <split [35/14]> #> 19 <split [5/2]> Bootstrap19 <split [35/14]> #> 20 <split [5/2]> Bootstrap20 <split [35/14]>"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/mem_gamlss.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a gamlss model but store user data — mem_gamlss","title":"Fit a gamlss model but store user data — mem_gamlss","text":"Think gamlss model memories (mem. gamlss).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/mem_gamlss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a gamlss model but store user data — mem_gamlss","text":"","code":"mem_gamlss(...)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/mem_gamlss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a gamlss model but store user data — mem_gamlss","text":"... arguments passed gamlss::gamlss()","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/mem_gamlss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a gamlss model but store user data — mem_gamlss","text":"fitted model object updated include user information model$.user. Includes dataset used fit model model$.user$data, session info model$.user$session_info call used fit model model$.user$call. model$call updated match","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::[\\%>\\%][magrittr::pipe] details.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/predict_centiles.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict and tidy centiles from a GAMLSS model — predict_centiles","title":"Predict and tidy centiles from a GAMLSS model — predict_centiles","text":"gamlss trouble predictions without original training data.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/predict_centiles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict and tidy centiles from a GAMLSS model — predict_centiles","text":"","code":"predict_centiles(newdata, model, centiles = c(5, 10, 50, 90, 95), ...)  pivot_centiles_longer(data)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/predict_centiles.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict and tidy centiles from a GAMLSS model — predict_centiles","text":"newdata one-column dataframe predictions model gamlss model prepared mem_gamlss() centiles centiles use prediction. Defaults c(5, 10, 50, 90, 95). ... arguments passed gamlss::centiles.pred() data centile predictions reshape pivot_centiles_longer()","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/predict_centiles.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict and tidy centiles from a GAMLSS model — predict_centiles","text":"tibble fitted centiles predict_centiles() long-format tibble one centile value per row pivot_centiles_longer()","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/tidyeval.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy eval helpers — tidyeval","title":"Tidy eval helpers — tidyeval","text":"rlang::sym() creates symbol string syms() creates list symbols character vector. enquo() enquos() delay execution one several function arguments. enquo() returns single quoted expression, like blueprint delayed computation. enquos() returns list quoted expressions. expr() quotes new expression locally. mostly useful build new expressions around arguments captured enquo() enquos(): expr(mean(!!enquo(arg), na.rm = TRUE)). rlang::as_name() transforms quoted variable name string. Supplying something else quoted variable name error. unlike rlang::as_label() also returns single string supports kind R object input, including quoted function calls vectors. purpose summarise object single label. label often suitable default name. know quoted expression contains (instance expressions captured enquo() variable name, call function, unquoted constant), use as_label(). know quoted simple variable name, like enforce , use as_name(). learn tidy eval use tools, visit https://tidyeval.tidyverse.org Metaprogramming section Advanced R.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/trapezoid_auc.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute AUCs using the trapezoid method — trapezoid_auc","title":"Compute AUCs using the trapezoid method — trapezoid_auc","text":"Compute AUCs using trapezoid method","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/trapezoid_auc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute AUCs using the trapezoid method — trapezoid_auc","text":"","code":"trapezoid_auc(xs, ys)  partial_trapezoid_auc(xs, ys, xlim)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/trapezoid_auc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute AUCs using the trapezoid method — trapezoid_auc","text":"xs, ys x y positions xlim two-element vector (range) xs sum ","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/trapezoid_auc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute AUCs using the trapezoid method — trapezoid_auc","text":"area curve computed using trapezoid method. partial_trapezoid_auc(), partial area curve computed.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/trapezoid_auc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute AUCs using the trapezoid method — trapezoid_auc","text":"","code":"if (requireNamespace(\"rstanarm\", quietly = TRUE)) {   wells <- rstanarm::wells   r <- pROC::roc(switch ~ arsenic, wells)   pROC::auc(r)   trapezoid_auc(r$specificities, r$sensitivities)    pROC::auc(r, partial.auc = c(.9, 1), partial.auc.focus = \"sp\")   partial_trapezoid_auc(r$specificities, r$sensitivities, c(.9, 1))    pROC::auc(r, partial.auc = c(.9, 1), partial.auc.focus = \"se\")   partial_trapezoid_auc(r$sensitivities, r$specificities, c(.9, 1))    pROC::auc(r, partial.auc = c(.1, .9), partial.auc.focus = \"sp\")   partial_trapezoid_auc(r$specificities, r$sensitivities, c(.1, .9))    pROC::auc(r, partial.auc = c(.1, .9), partial.auc.focus = \"se\")   partial_trapezoid_auc(r$sensitivities, r$specificities, c(.1, .9)) } #> Setting levels: control = 0, case = 1 #> Setting direction: controls < cases #> [1] 0.5086048"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/wisclabmisc-package.html","id":null,"dir":"Reference","previous_headings":"","what":"wisclabmisc: Tools to Support the 'WiscLab' — wisclabmisc-package","title":"wisclabmisc: Tools to Support the 'WiscLab' — wisclabmisc-package","text":"collection 'R' functions use (re-use) across 'WiscLab' projects. analysis presentation oriented functions--, data reading data cleaning.","code":""},{"path":[]},{"path":"https://www.tjmahr.com/wisclabmisc/reference/wisclabmisc-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"wisclabmisc: Tools to Support the 'WiscLab' — wisclabmisc-package","text":"Maintainer: Tristan Mahr tristan.mahr@wisc.edu (ORCID)","code":""}]
