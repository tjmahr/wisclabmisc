[{"path":"https://www.tjmahr.com/wisclabmisc/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2020 Tristan Mahr Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/articles/brms.html","id":"context-a-set-of-models","dir":"Articles","previous_headings":"","what":"Context: A set of models","title":"A Workflow for brms","text":"Suppose dataset age intelligibility. want fit set related models. “related”, loosely mean models outcome variable modeling family. can vary model model formulas priors. example, suppose want compare models different kinds age trends using splines: Given model formulas, write brm() one, ’s good plan. models arise start analyze critique results. might need set adapt_delta higher value models. Aside formulas, sometimes priors, everything model-fitting generally model model. solution problem make function fitting models set. function provides two wins: can standardize model fitting across models. can indicate similarity models organization code. function tells us “define possible age models”. workflow looks like high level. helper function retrieving priors, helper function computing LOO scores, helper function default/shared arguments brm(), main function fitting models set.","code":"data <- wisclabmisc::data_fake_intelligibility data #> # A tibble: 200 × 2 #>    age_months intelligibility #>         <int>           <dbl> #>  1         28           0.539 #>  2         29           0.375 #>  3         31           0.221 #>  4         31           0.253 #>  5         32           0.276 #>  6         32           0.750 #>  7         32           0.820 #>  8         33           0.325 #>  9         33           0.446 #> 10         33           0.592 #> # ℹ 190 more rows library(brms) library(splines) f1 <- bf(   intelligibility ~ age_months,    phi ~ age_months,    family = Beta ) f2 <- bf(   intelligibility ~ ns(age_months, 2),    phi ~ ns(age_months, 2),   family = Beta ) f3 <- bf(   intelligibility ~ ns(age_months, 3),    phi ~ ns(age_months, 3),   family = Beta ) lookup_prior <- function(prior_slug) {   # ... }   add_loo_criterion <- function(x, ..., use_reloo = FALSE) {    # ... }  brm_args <- wisclabmisc::brms_args_create()  fit_age_model <- function(     data,     model_slug,     prior_slug = \"default\",     seed = NA,     use_reloo = FALSE,     ... ) {   # 1. [looking up priors]    # 2. [mapping from model_slug to formula]      # 3. [creating a filename]      # 4. [fitting model]      # 5. [adding LOO score] }"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/brms.html","id":"priors","dir":"Articles","previous_headings":"","what":"Priors","title":"A Workflow for brms","text":"look priors using helper function. Right now, don’t different set priors want compare, just two options: use weak priors logit-scale mu log-scale phi, use brms defaults. Priors strict (good way): can’t set prior parameter doesn’t exist model formula. example, can’t set prior random effect variance formula doesn’t include random effects: flipside strictness need toggle priors, example, fitting random-intercept model random-slope model, fitting model regression splines ns() model smoothing spline s(). recent project, following lookup function: case, two sets models: one binomial outcome used logit_ri logit_rs one beta outcome needed additional priors precision parameter phi via logit_phi_ri logit_phi_rs. _ri _rs different whether include prior correlation random effects.","code":"lookup_prior <- function(prior_slug) {   l <- list(     default = c(       prior(normal(0, 1), class = b),       prior(normal(0, 2), class = Intercept),       prior(normal(0, 2), class = Intercept, dpar = phi),       prior(normal(0, 2), class = b, dpar = phi)     ),     brms_default = NULL   )   l[[prior_slug]] } validate_prior(   c(lookup_prior(\"default\"), prior(normal(0, 1), class = \"sd\")),   formula = f1,   data = data ) #> Error: The following priors do not correspond to any model parameter:  #> sd ~ normal(0, 1) #> Function 'default_prior' might be helpful to you. lookup_brms_priors <- function(prior_slug = \"logit_ri\") {   l <- list(     logit_ri = c(       prior(normal(0, 1), class = b),       prior(normal(0, 2), class = sds),       prior(normal(0, 2), class = Intercept),       prior(normal(0, 1), class = sd)     ),     logit_rs = c(       prior(normal(0, 1), class = b),       prior(normal(0, 1), class = sd),       prior(normal(0, 2), class = sds),       prior(normal(0, 2), class = Intercept),       prior(lkj(2), class = cor)     ),     logit_phi_ri = c(       prior(normal(0, 1), class = b),       prior(normal(0, 2), class = sds),       prior(normal(0, 2), class = Intercept),       prior(normal(0, 1), class = sd),       prior(normal(0, 2), class = b, dpar = phi)     ),     logit_phi_rs = c(       prior(normal(0, 1), class = b),       prior(normal(0, 2), class = sds),       prior(normal(0, 2), class = Intercept),       prior(normal(0, 1), class = sd),       prior(lkj(2), class = cor),       prior(normal(0, 2), class = b, dpar = phi)     )   )   l[[prior_slug]] }"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/brms.html","id":"a-helper-function-for-loo-computation","dir":"Articles","previous_headings":"","what":"A helper function for LOO computation","title":"A Workflow for brms","text":"like compute LOO score default. operation can time-consuming, fortunately gets cached model file just make part model fitting. function doesn’t anything noteworthy making easy toggle exact LOO scores needed. reloo argument gets forwarded brms::loo(). use_reloo = TRUE, brms refits model compute exact leave-one-scores problematic observations. One thing like figure workflow smart way describe fit LOGO (leave one group ) models repeated-measures models.","code":"add_loo_criterion <- function(x, ..., use_reloo = FALSE) {   if (use_reloo) {     brms::add_criterion(       x,       criterion = \"loo\",       reloo = TRUE,       recompile = FALSE,       ...     )   } else {     brms::add_criterion(       x,       criterion = \"loo\",       ...     )   } }"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/brms.html","id":"model-fitting-defaults","dir":"Articles","previous_headings":"","what":"Model-fitting defaults","title":"A Workflow for brms","text":"usually want knobs dials model fitting—number chains, number iterations, Stan backend—model--model. create function called brm_args() prints list default arguments brm(). create function using wisclabmisc::brms_args_create(). package-provided defaults. (defaults just preferred settings.) interesting ones might file_refit, threads backend. provide brm() file like \"-model\" save fitted model -model.rds. file_refit = \"on_change\" tells brms refit model model’s Stan code data change. default file_refit = \"never\", meaning never refits model. set threads backend try make model sample compile quickly. set refresh small number like see MCMC chains sampling. defaults troublesome (refresh), can generate new brm_args() function different defaults. brm_args() function lets us set default brm() argument values. Importantly, can deviate defaults need . can overwrite number chains set value adapt_delta.","code":"brm_args <- wisclabmisc::brms_args_create() brm_args() #> List of 9 #>  $ backend   : chr \"cmdstanr\" #>  $ threads   : num 2 #>  $ chains    : num 4 #>  $ cores     : num 4 #>  $ iter      : num 2000 #>  $ silent    : num 0 #>  $ file_refit: chr \"on_change\" #>  $ refresh   : num 25 #>  $ control   : list() #>  - attr(*, \"class\")= chr [1:2] \"brm_args\" \"list\" brm_args <- wisclabmisc::brms_args_create(iter = 5000, refresh = 100) brm_args() #> List of 9 #>  $ backend   : chr \"cmdstanr\" #>  $ threads   : num 2 #>  $ chains    : num 4 #>  $ cores     : num 4 #>  $ iter      : num 5000 #>  $ silent    : num 0 #>  $ file_refit: chr \"on_change\" #>  $ refresh   : num 100 #>  $ control   : list() #>  - attr(*, \"class\")= chr [1:2] \"brm_args\" \"list\" brm_args(chains = 2, adapt_delta = .98) #> List of 9 #>  $ backend   : chr \"cmdstanr\" #>  $ threads   : num 2 #>  $ chains    : num 2 #>  $ cores     : num 4 #>  $ iter      : num 5000 #>  $ silent    : num 0 #>  $ file_refit: chr \"on_change\" #>  $ refresh   : num 100 #>  $ control   :List of 1 #>   ..$ adapt_delta: num 0.98 #>  - attr(*, \"class\")= chr [1:2] \"brm_args\" \"list\""},{"path":"https://www.tjmahr.com/wisclabmisc/articles/brms.html","id":"model-fitting-function","dir":"Articles","previous_headings":"","what":"Model fitting function","title":"A Workflow for brms","text":"Now, can fit model. steps function pretty simple. generating executing call brm() (via .call()) look prior formula use, create filename file argument. Let’s fit models compare .","code":"# Use rstan for the article page brm_args <- wisclabmisc::brms_args_create(backend = \"rstan\")  fit_age_model <- function(     data,     model_slug,     prior_slug = \"default\",     seed = NA,     use_reloo = FALSE,      ... ) {   # 1. [looking up priors]   prior <- lookup_prior(prior_slug)    # 2. [mapping from model_slug to formula]   ns <- splines::ns      formulas <- list(     linear = bf(       intelligibility ~ age_months,        phi ~ age_months,        family = Beta     ),     spline_2df = bf(       intelligibility ~ ns(age_months, 2),        phi ~ ns(age_months, 2),       family = Beta     ),     spline_3df = bf(       intelligibility ~ ns(age_months, 3),        phi ~ ns(age_months, 3),       family = Beta     )   )      formula <- formulas[[model_slug]]      # 3. [creating a filename]   loo_slug <- ifelse(use_reloo, \"_reloo\", \"\")   file <- file.path(     \"models\", paste0(model_slug, \"_\", prior_slug, loo_slug)   )      # 4. [fitting model]   args <- brm_args(     formula = formula,     data = data,     prior = prior,     file = file,     seed = seed,     ...   )   model <- do.call(brm, args)    # 5. [adding LOO score]   add_loo_criterion(model, use_reloo) } # I'm only using rstan to get the demo to work on GitHub pages library(rstan) #> Loading required package: StanHeaders #>  #> rstan version 2.32.7 (Stan version 2.32.2) #> For execution on a local, multicore CPU with excess RAM we recommend calling #> options(mc.cores = parallel::detectCores()). #> To avoid recompilation of unchanged Stan programs, we recommend calling #> rstan_options(auto_write = TRUE) #> For within-chain threading using `reduce_sum()` or `map_rect()` Stan functions, #> change `threads_per_chain` option: #> rstan_options(threads_per_chain = 1) m1 <- fit_age_model(data, \"linear\", seed = 20241023) m2 <- fit_age_model(data, \"spline_2df\", seed = 20241023) m3 <- fit_age_model(data, \"spline_3df\", seed = 20241023)  loo_compare(m1, m2, m3) |>    print(simplify = FALSE) #>    elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic #> m1    0.0       0.0   248.6     14.8         3.2    0.4   -497.3   29.7   #> m2   -0.6       2.1   248.0     14.7         4.8    0.6   -496.1   29.4   #> m3   -4.1       2.5   244.5     14.9         6.5    0.7   -489.0   29.8"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/brms.html","id":"the-full-workflow","dir":"Articles","previous_headings":"","what":"The full workflow","title":"A Workflow for brms","text":"full workflow , easy copy-pasting boilerplate new projects. Putting code chunk easy grab place motivation writing article. 🤓","code":"lookup_prior <- function(prior_slug) {   l <- list(     default = c(       prior(normal(0, 1), class = b),       prior(normal(0, 2), class = Intercept),       prior(normal(0, 2), class = Intercept, dpar = phi),       prior(normal(0, 2), class = b, dpar = phi)     ),     brms_default = NULL   )   l[[prior_slug]] }  add_loo_criterion <- function(x, ..., use_reloo = FALSE) {   if (use_reloo) {     brms::add_criterion(       x,       criterion = \"loo\",       reloo = TRUE,       recompile = FALSE,       ...     )   } else {     brms::add_criterion(       x,       criterion = \"loo\",       ...     )   } }  brm_args <- wisclabmisc::brms_args_create()  fit_age_model <- function(     data,     model_slug,     prior_slug = \"default\",     seed = NA,     use_reloo = FALSE,      ... ) {   # 1. [looking up priors]   prior <- lookup_prior(prior_slug)    # 2. [mapping from model_slug to formula]   ns <- splines::ns      formulas <- list(     linear = bf(       intelligibility ~ age_months,        phi ~ age_months,        family = Beta     ),     spline_2df = bf(       intelligibility ~ ns(age_months, 2),        phi ~ ns(age_months, 2),       family = Beta     ),     spline_3df = bf(       intelligibility ~ ns(age_months, 3),        phi ~ ns(age_months, 3),       family = Beta     )   )      formula <- formulas[[model_slug]]      # 3. [creating a filename]   loo_slug <- ifelse(use_reloo, \"_reloo\", \"\")   file <- file.path(     \"models\", paste0(model_slug, \"_\", prior_slug, loo_slug)   )      # 4. [fitting model]   args <- brm_args(     formula = formula,     data = data,     prior = prior,     file = file,     seed = seed,     ...   )   model <- do.call(brm, args)    # 5. [adding LOO score]   add_loo_criterion(model, use_reloo) }"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/gamlss-tools.html","id":"a-gamlss-that-remembers-the-data","dir":"Articles","previous_headings":"","what":"A gamlss() that remembers the data","title":"Tools for GAMLSS models","text":"mem_gamlss() (memory gamlss) provides drop-replacement gamlss() function. difference mem_gamlss() gamlss() modified version includes bundle data .user records original dataset, session information call used fit model. gamlss store data part model object, need dataset prediction centile prediction often fails without dataset: including original dataset works: (“Centile prediction” means predicting percentiles data along single variable. ’s function just needs single xname: single predictor variable used. use centile prediction compute growth curves can look smooth changes percentiles age.)","code":"library(wisclabmisc) library(gamlss) library(tidyverse)  data <- as.data.frame(nlme::Orthodont) model <- mem_gamlss(distance ~ age, data = data) #> GAMLSS-RS iteration 1: Global Deviance = 505.577  #> GAMLSS-RS iteration 2: Global Deviance = 505.577 str(model$.user, max.level = 1) #> List of 3 #>  $ data        :'data.frame':    108 obs. of  4 variables: #>  $ session_info:List of 2 #>   ..- attr(*, \"class\")= chr [1:2] \"session_info\" \"list\" #>  $ call        : language mem_gamlss(distance ~ age, data = data) newdata <- distinct(data, age) centiles.pred(   model,    cent = c(25, 50, 75),   xname = \"age\",    xvalues = newdata$age,    plot = FALSE ) #> Error in data.frame(data, source = namelist): arguments imply differing number of rows: 4, 5 centiles.pred(   model,    cent = c(25, 50, 75),   xname = \"age\",    xvalues = newdata$age,    plot = FALSE,   data = model$.user$data ) #>    x       25       50       75 #> 1  8 20.34723 22.04259 23.73796 #> 2 10 21.66760 23.36296 25.05833 #> 3 12 22.98797 24.68333 26.37870 #> 4 14 24.30834 26.00370 27.69907"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/gamlss-tools.html","id":"centile-prediction-and-tidying","dir":"Articles","previous_headings":"","what":"Centile prediction and tidying","title":"Tools for GAMLSS models","text":"package provides predict_centiles() streamlined version code, : assumes model fitted mem_gamlss() returns tibble keeps predictor name (, age instead x) prefixes centiles q (quantile) predicted centiles wide format. can tidy long format pivot_centiles_longer(). also includes .pair column helps mark commonly paired quantiles 25:75, 10:90, 5:95.","code":"centiles <- predict_centiles(   newdata,   model,    cent = c(25, 50, 75) ) centiles #> # A tibble: 4 × 4 #>     age   c25   c50   c75 #>   <dbl> <dbl> <dbl> <dbl> #> 1     8  20.3  22.0  23.7 #> 2    10  21.7  23.4  25.1 #> 3    12  23.0  24.7  26.4 #> 4    14  24.3  26.0  27.7 pivot_centiles_longer(centiles) #> # A tibble: 12 × 4 #>      age .centile .value .centile_pair   #>    <dbl>    <dbl>  <dbl> <chr>           #>  1     8       25   20.3 centiles 25, 75 #>  2     8       50   22.0 median          #>  3     8       75   23.7 centiles 25, 75 #>  4    10       25   21.7 centiles 25, 75 #>  5    10       50   23.4 median          #>  6    10       75   25.1 centiles 25, 75 #>  7    12       25   23.0 centiles 25, 75 #>  8    12       50   24.7 median          #>  9    12       75   26.4 centiles 25, 75 #> 10    14       25   24.3 centiles 25, 75 #> 11    14       50   26.0 median          #> 12    14       75   27.7 centiles 25, 75"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/gamlss-tools.html","id":"sample-centiles-checks","dir":"Articles","previous_headings":"Centile prediction and tidying","what":"Sample centiles checks","title":"Tools for GAMLSS models","text":"Half data 50% centile line half 50% centile line. holds centile lines. check_model_centiles() performs check computing percentages observations less equal centile line. matches gamlss package’s output: function also supports grouped data check centile performance different subsets data. output also matches output provide gamlss’s centile.split() function:","code":"check_model_centiles(data, model, age, distance) #> # A tibble: 7 × 4 #>   .centile     n n_under_centile percent_under_centile #>      <dbl> <int>           <int>                 <dbl> #> 1        5   108               6                  5.56 #> 2       10   108               9                  8.33 #> 3       25   108              25                 23.1  #> 4       50   108              61                 56.5  #> 5       75   108              85                 78.7  #> 6       90   108              95                 88.0  #> 7       95   108             100                 92.6 centiles(   model,    model$.user$data$age,    data = model$.user$data,    cent = c(5, 10,25, 50, 75, 90, 95),    plot = FALSE ) #> % of cases below  5 centile is  5.555556  #> % of cases below  10 centile is  8.333333  #> % of cases below  25 centile is  23.14815  #> % of cases below  50 centile is  56.48148  #> % of cases below  75 centile is  78.7037  #> % of cases below  90 centile is  87.96296  #> % of cases below  95 centile is  92.59259 data %>%    mutate(age_bin = ntile(age, 2)) %>%    group_by(age_bin) %>%    check_model_centiles(model, age, distance) #> # A tibble: 14 × 5 #>    age_bin .centile     n n_under_centile percent_under_centile #>      <int>    <dbl> <int>           <int>                 <dbl> #>  1       1        5    54               3                  5.56 #>  2       1       10    54               4                  7.41 #>  3       1       25    54              13                 24.1  #>  4       1       50    54              29                 53.7  #>  5       1       75    54              44                 81.5  #>  6       1       90    54              49                 90.7  #>  7       1       95    54              51                 94.4  #>  8       2        5    54               3                  5.56 #>  9       2       10    54               5                  9.26 #> 10       2       25    54              12                 22.2  #> 11       2       50    54              32                 59.3  #> 12       2       75    54              41                 75.9  #> 13       2       90    54              46                 85.2  #> 14       2       95    54              49                 90.7 centiles.split(   model,    model$.user$data$age,    data = model$.user$data,    n.inter = 2,   cent = c(5, 10,25, 50, 75, 90, 95),    plot = FALSE ) #>      7 to 11  11 to 15 #> 5   5.555556  5.555556 #> 10  7.407407  9.259259 #> 25 24.074074 22.222222 #> 50 53.703704 59.259259 #> 75 81.481481 75.925926 #> 90 90.740741 85.185185 #> 95 94.444444 90.740741"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/roc.html","id":"a-primer-on-roc-curves","dir":"Articles","previous_headings":"","what":"A primer on ROC curves","title":"Tools for ROC curves","text":"wisclabmisc provides functions tidying results ROC curves. curves arise diagnostic classification settings want use test score determine whether individual belongs control group versus case group. binary classification normal versus clinical status, regular email versus spam status, . use terminology control case follow pROC package’s interface. classification literature, tons tons statistics describe classifier performance. ROC curve centers around two important quantities sensitivity specificity: Also called true positive rate recall. apply spam classifier 100 spam emails, many correctly flagged spam? P(case result | case status) Sensitivity makes sense think problem detecting something subtle. (Like Jedi “force sensitive” Spider-Man’s Spidey sense tingling ’s danger.) Also called true negative rate selectivity. apply spam classifier 100 safe (ham) emails, many correctly ignored? P(control result | control status) Specificity great term; selectivity makes slightly sense. don’t want sensor trip noise: needs specific selective. Suppose diagnostic instrument provides score, choose diagnostic threshold one scores. example, suppose decide scores 60 indicate email probably spam can moved spam folder. threshold specificity attached . can look proportion spam emails equal 60 (sensitivity), can look proportion ham emails 60 (specificity). number choose threshold sensitivity specificity score, ROC curve visualization sensitivity specificity change along range threshold scores. (impenetrable terminology: ROC stands “receiver operating characteristic”, something detections made radar receivers different operating levels.)","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/articles/roc.html","id":"a-worked-example","dir":"Articles","previous_headings":"A primer on ROC curves","what":"A worked example","title":"Tools for ROC curves","text":"can work example ROC curve using pROC package. pROC provides aSAH dataset provides “several clinical one laboratory variable 113 patients aneurysmal subarachnoid hemorrhage” (hence, aSAH). outcome (Good versus Poor) measure called s100b. can see many Good outcomes near 0 Poor outcomes.  point grid points along s100b, can compute proportions patients group threshold. can plot proportions visualize trading relations specificity sensitivity threshold changes.  took 5 tries get plot correct. able convince noting Good outcomes less .51 threshold catch single Good outcome hence specificity 1. Conversely, just Poor outcome 1, threshold 1 going detect 1 Poor outcome hence low sensitivity. ignore threshold visualization, can (finally) plot canonical ROC curve. shows specificity reversing order ideal point top left corner (sensitivity = 1, specificity = 1).  can compare plot one provided pROC package. find perfect match sensitivity specificity values.   Instead computing ROC curves hand, defer calculation ROC curves pROC package easy get confused calculating sensitivity specificity pROC provides tools working ROC curves. Thus, wisclabmisc’s goal ROC curves provide helper functions fit ROC curves pROC return results nice dataframe. contrast two types ROC curves: empirical ROC curve raw data used make jagged ROC curve (smooth) density ROC curve densities two distributions used make smooth ROC curve.","code":"data <- as_tibble(aSAH) data #> # A tibble: 113 × 7 #>    gos6  outcome gender   age wfns  s100b  ndka #>    <ord> <fct>   <fct>  <int> <ord> <dbl> <dbl> #>  1 5     Good    Female    42 1      0.13  3.01 #>  2 5     Good    Female    37 1      0.14  8.54 #>  3 5     Good    Female    42 1      0.1   8.09 #>  4 5     Good    Female    27 1      0.04 10.4  #>  5 1     Poor    Female    42 3      0.13 17.4  #>  6 1     Poor    Male      48 2      0.1  12.8  #>  7 4     Good    Male      57 5      0.47  6    #>  8 1     Poor    Male      41 4      0.16 13.2  #>  9 5     Good    Female    49 1      0.18 15.5  #> 10 4     Good    Female    75 2      0.1   6.01 #> # ℹ 103 more rows count(data, outcome) #> # A tibble: 2 × 2 #>   outcome     n #>   <fct>   <int> #> 1 Good       72 #> 2 Poor       41  ggplot(data) +    aes(x = s100b, y = outcome) +    geom_point(     position = position_jitter(width = 0, height = .2),     size = 3,     alpha = .2,   ) +   theme_grey(base_size = 12) +   labs(y = NULL) by_outcome <- split(data, data$outcome) smallest_diff <- min(diff(unique(sort(data$s100b)))) grid <- tibble(   threshold = seq(     min(data$s100b) - smallest_diff,      max(data$s100b) + smallest_diff,      length.out = 200   ) )  roc_coordinates <- grid %>%    rowwise() %>%    summarise(     threshold = threshold,     prop_poor_above = mean(by_outcome$Poor$s100b >= threshold),     prop_good_below = mean(by_outcome$Good$s100b < threshold),   )  ggplot(roc_coordinates) +    aes(x = threshold) +    geom_step(aes(y = prop_poor_above)) +    geom_step(aes(y = prop_good_below)) +   annotate(\"text\", x = 2, y = .9, hjust = 1, label = \"specificity\") +    annotate(\"text\", x = 2, y = .1, hjust = 1, label = \"sensitivity\") +   labs(     title = \"Sensitivity and specificity as cumulative proportions\",     x = \"threshold (diagnosis when score >= threshold)\",     y = NULL   ) roc_coordinates <- roc_coordinates %>%    rename(     sensitivities = prop_poor_above,      specificities = prop_good_below   ) %>%   # otherwise the stair-steps look wrong   arrange(sensitivities)  p <- ggplot(roc_coordinates) +    aes(x = specificities, y = sensitivities) +    geom_step() +   scale_x_reverse() +    coord_fixed() +    theme_grey(base_size = 14) p roc <- pROC::roc(data, response = outcome, predictor = s100b) #> Setting levels: control = Good, case = Poor #> Setting direction: controls < cases plot(roc) proc_coordinates <- roc[2:3] %>%    as.data.frame() %>%    arrange(sensitivities)  # Plot the pROC point as a wide semi-transparent blue # band on top of ours p +    geom_step(     data = proc_coordinates,      color = \"blue\",     alpha = .5,     size = 2   ) #> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. #> ℹ Please use `linewidth` instead. #> This warning is displayed once every 8 hours. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated."},{"path":"https://www.tjmahr.com/wisclabmisc/articles/roc.html","id":"emprical-roc-curves","dir":"Articles","previous_headings":"","what":"Emprical ROC curves","title":"Tools for ROC curves","text":"Let’s return example, predicting group label outcome (case: Poor, control: Good) predictor s100b. messages, can see pROC::roc() makes decisions us: Good control level Poor case level, controls lower s100b cases. pROC::roc() returns roc object bundles data model results together. Ultimately, want results dataframe one row provide sensitivity specificity threshold value. can get close dataframe manipulating list using coords(). pROC::coords() additional features allow identify “best” ROC points, strips useful data like direction used. wisclabmisc provides compute_empirical_roc() combines results pROC::roc() pROC::coords() tibble. includes metadata .controls .cases levels, .direction relationship, overall .auc curve. also identifies two “best” coordinates .is_best_youden is_best_closest_topleft. Finally, retains name predictor variable. can still see messages emitted pROC::roc() call use compute_empirical_roc(). can pass arguments direction levels pROC::roc() silence messages. According help page pROC::coords() Youden’s J statistic point farthest vertical distance diagonal line. “best” point point closest upper-left corner. following plot labels distances. Youden’s point topleft point point.","code":"r <- pROC::roc(data, outcome, s100b) #> Setting levels: control = Good, case = Poor #> Setting direction: controls < cases r #>  #> Call: #> roc.data.frame(data = data, response = outcome, predictor = s100b) #>  #> Data: s100b in 72 controls (outcome Good) < 41 cases (outcome Poor). #> Area under the curve: 0.7314 r #>  #> Call: #> roc.data.frame(data = data, response = outcome, predictor = s100b) #>  #> Data: s100b in 72 controls (outcome Good) < 41 cases (outcome Poor). #> Area under the curve: 0.7314 class(r) #> [1] \"roc\" str(r, max.level = 1, give.attr = FALSE) #> List of 15 #>  $ percent           : logi FALSE #>  $ sensitivities     : num [1:51] 1 0.976 0.976 0.976 0.976 ... #>  $ specificities     : num [1:51] 0 0 0.0694 0.1111 0.1389 ... #>  $ thresholds        : num [1:51] -Inf 0.035 0.045 0.055 0.065 ... #>  $ direction         : chr \"<\" #>  $ cases             : num [1:41] 0.13 0.1 0.16 0.12 0.44 0.71 0.49 0.07 0.33 0.09 ... #>  $ controls          : num [1:72] 0.13 0.14 0.1 0.04 0.47 0.18 0.1 0.1 0.04 0.08 ... #>  $ fun.sesp          :function (thresholds, controls, cases, direction)   #>  $ auc               : 'auc' num 0.731 #>  $ call              : language roc.data.frame(data = data, response = outcome, predictor = s100b) #>  $ original.predictor: num [1:113] 0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ... #>  $ original.response : Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ... #>  $ predictor         : num [1:113] 0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ... #>  $ response          : Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ... #>  $ levels            : chr [1:2] \"Good\" \"Poor\" r[1:5] %>%    as.data.frame() %>%    tibble::as_tibble() #> # A tibble: 51 × 5 #>    percent sensitivities specificities thresholds direction #>    <lgl>           <dbl>         <dbl>      <dbl> <chr>     #>  1 FALSE           1            0        -Inf     <         #>  2 FALSE           0.976        0           0.035 <         #>  3 FALSE           0.976        0.0694      0.045 <         #>  4 FALSE           0.976        0.111       0.055 <         #>  5 FALSE           0.976        0.139       0.065 <         #>  6 FALSE           0.902        0.222       0.075 <         #>  7 FALSE           0.878        0.306       0.085 <         #>  8 FALSE           0.829        0.389       0.095 <         #>  9 FALSE           0.780        0.486       0.105 <         #> 10 FALSE           0.756        0.542       0.115 <         #> # ℹ 41 more rows  pROC::coords(r) %>%    tibble::as_tibble() #> # A tibble: 51 × 3 #>    threshold specificity sensitivity #>        <dbl>       <dbl>       <dbl> #>  1  -Inf          0            1     #>  2     0.035      0            0.976 #>  3     0.045      0.0694       0.976 #>  4     0.055      0.111        0.976 #>  5     0.065      0.139        0.976 #>  6     0.075      0.222        0.902 #>  7     0.085      0.306        0.878 #>  8     0.095      0.389        0.829 #>  9     0.105      0.486        0.780 #> 10     0.115      0.542        0.756 #> # ℹ 41 more rows compute_empirical_roc(data, outcome, s100b) #> Setting levels: control = Good, case = Poor #> Setting direction: controls < cases data_roc <- compute_empirical_roc(   data,    outcome,    s100b,    direction = \"<\",   levels = c(\"Good\", \"Poor\") ) data_roc #> # A tibble: 51 × 11 #>       s100b .specificities .sensitivities  .auc .direction .controls .cases #>       <dbl>          <dbl>          <dbl> <dbl> <chr>      <chr>     <chr>  #>  1 -Inf             0               1     0.731 <          Good      Poor   #>  2    0.035         0               0.976 0.731 <          Good      Poor   #>  3    0.045         0.0694          0.976 0.731 <          Good      Poor   #>  4    0.055         0.111           0.976 0.731 <          Good      Poor   #>  5    0.065         0.139           0.976 0.731 <          Good      Poor   #>  6    0.075         0.222           0.902 0.731 <          Good      Poor   #>  7    0.085         0.306           0.878 0.731 <          Good      Poor   #>  8    0.095         0.389           0.829 0.731 <          Good      Poor   #>  9    0.105         0.486           0.780 0.731 <          Good      Poor   #> 10    0.115         0.542           0.756 0.731 <          Good      Poor   #> # ℹ 41 more rows #> # ℹ 4 more variables: .n_controls <int>, .n_cases <int>, .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl> data_roc <- data_roc %>%    arrange(.sensitivities)  p_best <- ggplot(data_roc) +    aes(x = .specificities, y = .sensitivities) +    geom_abline(     slope = 1,      intercept = 1,      linetype = \"dotted\",      color = \"grey20\"   ) +   geom_step() +    geom_segment(     aes(xend = .specificities, yend = 1 - .specificities),     data = . %>% filter(.is_best_youden),     color = \"blue\",     linetype = \"dashed\"   ) +    geom_segment(     aes(xend = 1, yend = 1),     data = . %>% filter(.is_best_closest_topleft),     color = \"maroon\",     linetype = \"dashed\"   ) +    # Basically, finding a point 9/10ths of the way   # along the line   geom_text(     aes(       x = weighted.mean(c(1, .specificities), c(9, 1)),        y = weighted.mean(c(1, .sensitivities), c(9, 1)),      ),     data = . %>% filter(.is_best_closest_topleft),     color = \"maroon\",     label = \"closest to topleft\",     hjust = 0,      nudge_x = .02,     size = 5   ) +    geom_text(     aes(       x = .specificities,        y = weighted.mean(c(1 - .specificities, .sensitivities), c(1, 2)),      ),     data = . %>% filter(.is_best_youden),     color = \"blue\",     label = \"Youden's J\\n(max height above diagonal)\",     hjust = 0,     vjust = .5,     nudge_x = .02,     size = 5   ) +    annotate(     \"text\",     x = .91,     y = .05,     hjust = 0,      size = 5,     label = \"diagonal: random classifier\",     color = \"grey20\"   ) +   scale_x_reverse() +   coord_fixed() +   theme_grey(base_size = 12) p_best"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/roc.html","id":"smooth-density-roc-curves","dir":"Articles","previous_headings":"","what":"(Smooth) density ROC curves","title":"Tools for ROC curves","text":"Instead looking observed data, let’s assume s100b values group drawn normal distribution means scales (standard deviations) different two groups. can compute group’s mean standard deviation plot normal density curves top . Pepe (2003) refers approach “binormal ROC curve”.  various points along x-axis range, stat_function() compute dnorm() (density normal curves). can hand . take full range data, within group, generate set points along range compute group’s density point. Next, pivot wide pivot format comparing two densities point. pROC::roc() can compute ROC curve densities. Note interface different. provide dataframe names columns data frame. Instead, provide two vectors densities, fact, densities lost computing ROC curve.  roc object returns coordinates sensitivity decreasing order, obvious map sensitivities back original densities. terms earlier density plot, don’t know whether sensitivities move x axis x axis. Let’s restate problem , clarity: want map thresholds densities ROC coordinates map ROC coordinates back densities thresholds. pROC::roc(density.controls, density.controls), hit brick wall map backwards ROC coordinates sensitivites may reversed respect densities. Fortunately, compute sensitivities hand, can figure coordinates ordered. try orderings find one best matches one provided pROC::roc(). < direction better matched ROC results, conclude sensitivities follow order densities. compute_smooth_density_roc() uses similar heuristic determine order ROC coordinates respect original densities. result, can map original threshold values sensitivity specificity values. function also lets us use column names directly. compute_smooth_density_roc() also provides coordinates “best” thresholds Youden topleft criteria. consistency two functions, can just replace data used make annotated ROC curve smoothed ROC coordinates. case, Youden topleft points different.  final demonstration, let’s compare smooth empirical ROC sensitivity specificity values along threshold values.","code":"data_stats <- data %>%    group_by(outcome) %>%    summarise(     mean = mean(s100b),     sd = sd(s100b)   )   l_control <- data_stats %>%    filter(outcome == \"Good\") %>%    as.list()  l_case <- data_stats %>%    filter(outcome != \"Good\") %>%    as.list()  ggplot(data) +    aes(x = s100b, color = outcome) +    # include a \"rug\" at the bottom   geom_jitter(aes(y = -.2), width = 0, height = .15, alpha = .4) +   stat_function(     data = . %>% filter(outcome == \"Good\"),     fun = dnorm,      args = list(mean = l_control$mean, sd = l_control$sd)   ) +   stat_function(     data = . %>% filter(outcome != \"Good\"),     fun = dnorm,      args = list(mean = l_case$mean, sd = l_case$sd)   ) +   geom_text(     aes(x = mean, y = dnorm(mean, mean, sd), label = outcome),     data = data_stats,     vjust = \"inward\",     hjust = 0,     nudge_x = .05,     nudge_y = .05,     size = 4   ) +   theme_grey(14) +   theme(legend.position = \"top\", legend.justification = \"left\") +   labs(y = NULL) +   guides(color = \"none\") data_grid <- data %>%    mutate(     xmin = min(s100b),     xmax = max(s100b)   ) %>%    group_by(outcome) %>%    summarise(     x = seq(xmin[1], xmax[1], length.out = 200),     group_mean = mean(s100b),     group_sd = sd(s100b),     density = dnorm(x, group_mean, group_sd),     .groups = \"drop\"   )  #> Warning: Returning more (or less) than 1 row per `summarise()` group was deprecated in #> dplyr 1.1.0. #> ℹ Please use `reframe()` instead. #> ℹ When switching from `summarise()` to `reframe()`, remember that `reframe()` #>   always returns an ungrouped data frame and adjust accordingly. #> Call `lifecycle::last_lifecycle_warnings()` to see where this warning was #> generated. data_grid #> # A tibble: 400 × 5 #>    outcome      x group_mean group_sd density #>    <fct>    <dbl>      <dbl>    <dbl>   <dbl> #>  1 Good    0.03        0.162    0.131    1.84 #>  2 Good    0.0403      0.162    0.131    1.98 #>  3 Good    0.0505      0.162    0.131    2.13 #>  4 Good    0.0608      0.162    0.131    2.27 #>  5 Good    0.0710      0.162    0.131    2.40 #>  6 Good    0.0813      0.162    0.131    2.53 #>  7 Good    0.0915      0.162    0.131    2.64 #>  8 Good    0.102       0.162    0.131    2.75 #>  9 Good    0.112       0.162    0.131    2.84 #> 10 Good    0.122       0.162    0.131    2.91 #> # ℹ 390 more rows data_dens <- data_grid %>%    rename(s100b = x) %>%    select(-group_mean, -group_sd) %>%    pivot_wider(names_from = outcome, values_from = density) data_dens #> # A tibble: 200 × 3 #>     s100b  Good  Poor #>     <dbl> <dbl> <dbl> #>  1 0.03    1.84 0.659 #>  2 0.0403  1.98 0.676 #>  3 0.0505  2.13 0.694 #>  4 0.0608  2.27 0.711 #>  5 0.0710  2.40 0.729 #>  6 0.0813  2.53 0.746 #>  7 0.0915  2.64 0.763 #>  8 0.102   2.75 0.780 #>  9 0.112   2.84 0.797 #> 10 0.122   2.91 0.813 #> # ℹ 190 more rows data_dens <- arrange(data_dens, s100b) r_dens <- roc(   density.controls = data_dens$Good,    density.cases = data_dens$Poor ) r_dens #>  #> Call: #> roc.default(density.controls = data_dens$Good, density.cases = data_dens$Poor) #>  #> Data: (unknown) in 0 controls ((unknown) )  0 cases ((unknown) ). #> Smoothing: density with controls: data_dens$Good; and cases: data_dens$Poor #> Area under the curve: 0.8299 plot(r_dens) # direction > : Good > threshold >= Poor sens_gt <- rev(cumsum(data_dens$Poor) / sum(data_dens$Poor)) # direction < : Good < threshold <= Poor sens_lt <- 1 - (cumsum(data_dens$Poor) / sum(data_dens$Poor)) # The model did ?? fitted_sensitivities <- r_dens$sensitivities[-c(1, 201)]  mean(fitted_sensitivities - sens_lt) #> [1] 0.004999997 mean(fitted_sensitivities - sens_gt) #> [1] -0.530585 data_smooth <- compute_smooth_density_roc(   data = data_dens,    controls = Good,    cases = Poor,    along = s100b ) data_smooth #> # A tibble: 202 × 10 #>     s100b  Good  Poor .sensitivities .specificities  .auc .roc_row .direction #>     <dbl> <dbl> <dbl>          <dbl>          <dbl> <dbl>    <int> <chr>      #>  1 0.03    1.84 0.659          1             0      0.830        2 <          #>  2 0.0403  1.98 0.676          0.992         0.0221 0.830        3 <          #>  3 0.0505  2.13 0.694          0.984         0.0460 0.830        4 <          #>  4 0.0608  2.27 0.711          0.975         0.0716 0.830        5 <          #>  5 0.0710  2.40 0.729          0.967         0.0989 0.830        6 <          #>  6 0.0813  2.53 0.746          0.958         0.128  0.830        7 <          #>  7 0.0915  2.64 0.763          0.949         0.158  0.830        8 <          #>  8 0.102   2.75 0.780          0.939         0.190  0.830        9 <          #>  9 0.112   2.84 0.797          0.930         0.223  0.830       10 <          #> 10 0.122   2.91 0.813          0.920         0.257  0.830       11 <          #> # ℹ 192 more rows #> # ℹ 2 more variables: .is_best_youden <lgl>, .is_best_closest_topleft <lgl> p_best + list(data_smooth) ggplot(data_smooth) +    aes(x = s100b) +    geom_line(     aes(color = \"smooth\", linetype = \"smooth\", y = .sensitivities),   ) +    geom_line(     aes(color = \"empirical\", linetype = \"smooth\", y = .sensitivities),     data = data_roc   ) +    geom_line(     aes(color = \"smooth\", linetype = \"empirical\", y = .specificities)   ) +    geom_line(     aes(color = \"empirical\", linetype = \"empirical\", y = .specificities),     data = data_roc   ) +   annotate(\"text\", x = 2, y = .9, hjust = 1, label = \"specificity\") +    annotate(\"text\", x = 2, y = .1, hjust = 1, label = \"sensitivity\") +   labs(     color = \"ROC type\",      linetype = \"ROC type\",     y = NULL   ) +    theme_grey(base_size = 12) +    theme(legend.position = \"top\") #> Warning: Removed 2 rows containing missing values or values outside the scale range #> (`geom_line()`). #> Removed 2 rows containing missing values or values outside the scale range #> (`geom_line()`)."},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2020-mahr-growth.html","id":"data-overview","dir":"Articles","previous_headings":"","what":"Data overview","title":"Analysis code: 'Longitudinal growth in intelligibility of connected speech  from 2 to 8 years in children with cerebral palsy: A novel Bayesian approach'\n","text":"Let’s take note data. first 10 rows data. : sid uniquely identifies child age child’s age months slpg child’s speech-language profile group intel child’s intelligibility measurement Children nested speech-language profile groups visits nested children. Intelligibility proportion 0 1. Age ranges 24 months 96 months.","code":"library(tidyverse) data #> # A tibble: 513 x 4 #>    sid     age slpg    intel #>    <chr> <dbl> <fct>   <dbl> #>  1 c65      53 NSMI    0.890 #>  2 c65      60 NSMI    0.892 #>  3 c65      65 NSMI    0.951 #>  4 c65      70 NSMI    0.975 #>  5 c65      76 NSMI    0.977 #>  6 c65      81 NSMI    0.974 #>  7 c65      87 NSMI    0.990 #>  8 c65      96 NSMI    0.988 #>  9 c45      45 SMI-LCT 0.417 #> 10 c45      51 SMI-LCT 0.706 #> # ... with 503 more rows data %>%    group_by(slpg, sid) %>%    summarise(     n_visits_by_child = n()   ) #> # A tibble: 65 x 3 #> # Groups:   slpg [3] #>    slpg  sid   n_visits_by_child #>    <fct> <chr>             <int> #>  1 NSMI  c12                  10 #>  2 NSMI  c13                  10 #>  3 NSMI  c15                   7 #>  4 NSMI  c23                   4 #>  5 NSMI  c26                  10 #>  6 NSMI  c28                  11 #>  7 NSMI  c32                  10 #>  8 NSMI  c37                  11 #>  9 NSMI  c4                    8 #> 10 NSMI  c40                   8 #> # ... with 55 more rows  data %>%    group_by(slpg) %>%    summarise(     n_visits_in_group = n(),     n_children_in_group = n_distinct(sid)   ) #> # A tibble: 3 x 3 #>   slpg    n_visits_in_group n_children_in_group #>   <fct>               <int>               <int> #> 1 NSMI                  191                  22 #> 2 SMI-LCT               248                  31 #> 3 SMI-LCI                74                  12 summary(data$intel, digits = 2)  #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>  0.0058  0.3000  0.6000  0.5500  0.8300  0.9900  summary(data$age, digits = 3)  #>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max.  #>    24.0    51.0    64.0    64.3    78.0    96.0"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2020-mahr-growth.html","id":"building-the-brms-model","dir":"Articles","previous_headings":"","what":"Building the BRMS model","title":"Analysis code: 'Longitudinal growth in intelligibility of connected speech  from 2 to 8 years in children with cerebral palsy: A novel Bayesian approach'\n","text":"fit model, need specify model family, model formula parameters, priors parameters. brms rich formula-based language building regression models. resembles usual syntax making linear mixed models R lme4, elaborates syntax number ways. bf(``) function (short brms formula) used build model specifications. ordinary linear model regressing y onto x set follows.","code":"library(brms)  bf(   y ~ x,    family = gaussian() )"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2020-mahr-growth.html","id":"response-family","dir":"Articles","previous_headings":"Building the BRMS model","what":"Response family","title":"Analysis code: 'Longitudinal growth in intelligibility of connected speech  from 2 to 8 years in children with cerebral palsy: A novel Bayesian approach'\n","text":"outcome proportion 0 1, use beta regression model parameterized using mean parameter precision parameter (phi). precision strictly positive modeled log-link function. fit ‘submodel’ parameters: nonlinear model mean linear model precision. model formula looks point, using placeholders formulas. nl = TRUE signals nonlinear model inside formula.","code":"bf(   intel ~ `some nonlinear formula for the mean`,   phi ~ `some linear formula for the precision`,   nl = TRUE,    family = Beta(link = \"identity\", link_phi = \"log\") )"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2020-mahr-growth.html","id":"nonlinear-formula-for-the-mean","dir":"Articles","previous_headings":"Building the BRMS model","what":"Nonlinear formula for the mean","title":"Analysis code: 'Longitudinal growth in intelligibility of connected speech  from 2 to 8 years in children with cerebral palsy: A novel Bayesian approach'\n","text":"Let’s start overall model mean. fitting logistic curve. usually form: asymptote, mid scale parameters estimated. model assumes children start 0 intelligibility, show accelerating decelerating growth, eventually plateau mature level performance (asymptote). point growth steepest midpoint (mid), scale feature controls steep growth curve . Later , features get linear model. Thus, model formula expands : make two alterations logistic function. First, exponentiate scale term, slope always positive. operation rules degenerate growth curves negative slopes. Second, apply inverse-logit function asymptote. Intelligibility proportion scale, asymptote needs proportion scale . later going incorporate group child variation asymptote feature means linear model, need constrain asymptote proportion scale. handle estimating asymptote real-valued logit (log-odds) scale converting proportion using inverse logit function. updated formula follows:","code":"f(y) = asymptote / (1 + exp((mid - x) * scale)) bf(   intel ~ asymptote / (1 + exp((mid - age) * scale)),   asymptote ~ `some linear formula`,   mid ~ `some linear formula`,   scale ~ `some linear formula`,   phi ~ `some linear formula for the precision`,   nl = TRUE,    family = Beta(link = \"identity\", link_phi = \"log\") ) inv_logit <- function(x) 1 / (1 + exp(-x))  bf(   intel ~ inv_logit(asymlogit) / (1 + exp((mid - age) * exp(scale))),   asymlogit ~ `some linear formula`,   mid ~ `some linear formula`,   scale ~ `some linear formula`,   phi ~ `some linear formula for the precision`,   nl = TRUE,    family = Beta(link = \"identity\", link_phi = \"log\") )"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2020-mahr-growth.html","id":"linear-formulas-for-the-curve-parameters","dir":"Articles","previous_headings":"Building the BRMS model","what":"Linear formulas for the curve parameters","title":"Analysis code: 'Longitudinal growth in intelligibility of connected speech  from 2 to 8 years in children with cerebral palsy: A novel Bayesian approach'\n","text":"Now nonlinear model mean sketched , specify linear model curve parameters. linear model used one: Let’s work parts formula step step. first two terms population average group: 1 - fit intercept (average NSMI group) + slpg - fit group (slpg) differences intercept (NSMI vs SMI-LCT NSMI vs SMI-LCT) remaining terms population variation group: + ``( ...`` | sid) - include random effects. Observations nested within sid variable. + ``( ...`` | ID | ...) - include correlation random effects formulas. ID works identifier saying formulas correlations . case, correlated. (0 + slpg | ...) - estimate separate random intercept variance term group. 0 means suppress intercept. model estimates correlation among random effect terms. example, NSMI group, model estimates correlations among midpoints, asymptotes, scale features, estimates analogous correlations SMI-LCT SMI-LCI groups. also estimates correlations among midpoints, asymptotes, scale features groups; example, correlation NSMI asymptotes SMI-LCT midpoints estimated. Removing cross-group correlations provide parsimonious model, model without correlations converge (due divergent iterations). Therefore, allow cross-group correlations.","code":"asymlogit ~ 1 + slpg + (0 + slpg | ID | sid) mid       ~ 1 + slpg + (0 + slpg | ID | sid) scale     ~ 1 + slpg + (0 + slpg | ID | sid) inv_logit <- function(x) 1 / (1 + exp(-x))  bf(   intel ~ inv_logit(asymlogit) / (1 + exp((mid - age) * exp(scale))),   asymlogit ~ 1 + slpg + (0 + slpg | ID | sid),   mid ~ 1 + slpg + (0 + slpg | ID | sid),   scale ~ 1 + slpg + (0 + slpg | ID | sid),   phi ~ `some linear formula for the precision`,   nl = TRUE,    family = Beta(link = \"identity\", link_phi = \"log\") )"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2020-mahr-growth.html","id":"linear-formula-for-the-precision","dir":"Articles","previous_headings":"Building the BRMS model","what":"Linear formula for the precision","title":"Analysis code: 'Longitudinal growth in intelligibility of connected speech  from 2 to 8 years in children with cerebral palsy: A novel Bayesian approach'\n","text":"allow precision change linearly age allow average precision change group:","code":"phi ~ 1 + age + slpg"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2020-mahr-growth.html","id":"the-full-formula","dir":"Articles","previous_headings":"Building the BRMS model","what":"The full formula","title":"Analysis code: 'Longitudinal growth in intelligibility of connected speech  from 2 to 8 years in children with cerebral palsy: A novel Bayesian approach'\n","text":"full model formula.","code":"inv_logit <- function(x) 1 / (1 + exp(-x))  full_formula <- bf(   intel ~ inv_logit(asymlogit) / (1 + exp((mid - age) * exp(scale))),   asymlogit ~ 1 + slpg + (0 + slpg | ID | sid),   mid       ~ 1 + slpg + (0 + slpg | ID | sid),   scale     ~ 1 + slpg + (0 + slpg | ID | sid),   phi ~ 1 + age + slpg,   nl = TRUE,    family = Beta(link = \"identity\", link_phi = \"log\") )"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2020-mahr-growth.html","id":"model-priors","dir":"Articles","previous_headings":"Building the BRMS model","what":"Model priors","title":"Analysis code: 'Longitudinal growth in intelligibility of connected speech  from 2 to 8 years in children with cerebral palsy: A novel Bayesian approach'\n","text":"provide three sets priors: Priors population averages group (fixed effects), priors population variation (random effects), priors precision. population average priors, specify prior distributions NSMI group (coef = \"Intercept\") group differences (class = \"b\", b beta regression equation.) Let’s work one example priors saying. midpoint intercept parameter (nlpar = \"mid\"), prior information says following: seeing data, think plausible set values average age steepest growth NSMI group normal distribution mean 50 months standard deviation 6 months, result, 99% plausible NSMI-average midpoints fall 34.5 66.5 months. group differences, tell model group differences order 12 months plausible. prior less informative one reference group: centered 0, meaning earlier--NSMI later--NSMI midpoints plausible group averages. Group differences 24 months plausible. priors computational devices: supply information available data help model sample space parameter values ruling priori implausible parameter values. priors population variation given terms standard deviations LKJ prior correlation matrix. , midpoints, prior says -child variability (standard deviation) 5 15 months plausible group. LKJ prior specifies correlations plausible. example, 2x2 matrix, LKJ(1) puts uniform distribution correlations whereas LKJ(2) rules correlations -1 1:  use LKJ(2) prior provides weakly informative prior: Enough information rule degenerate correlations. Finally, precision parameter, use weakly informative priors: prior selection, used combined subject matter knowledge evaluation prior predictive distribution. , things like midpoint feature, good sense children’s speech develops, selected priors encompassed age range. computational features, particular scale parameter, model simulate fake data. fake data implausible, like implying 100% intelligibility 18 months changing 20% intelligible 80% intelligible month, tuned priors fake data became plausible.","code":"prior_fixef <- c(   prior(normal(1.25, .5), nlpar = \"asymlogit\", coef = \"Intercept\"),   prior(normal(-.5, .5), nlpar = \"asymlogit\", class = \"b\"),   prior(normal(50, 6), nlpar = \"mid\", coef = \"Intercept\"),   prior(normal(0, 12), nlpar = \"mid\", class = \"b\"),   prior(normal(-2, 1), nlpar = \"scale\", coef = \"Intercept\"),   prior(normal(0, .5), nlpar = \"scale\", class = \"b\") ) prior_ranef <- c(   prior(normal(0, 1.25), class = \"sd\", nlpar = \"asymlogit\"),   prior(normal(10, 2.5), class = \"sd\", nlpar = \"mid\"),   prior(normal(0, .5), class = \"sd\", nlpar = \"scale\"),   prior(lkj_corr_cholesky(2), class = \"L\") ) prior_phi <- c(   prior(normal(2, 1), dpar = \"phi\", class = \"Intercept\"),   prior(normal(0, 1), dpar = \"phi\", class = \"b\") )"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2020-mahr-growth.html","id":"full-model-fitting-code","dir":"Articles","previous_headings":"Building the BRMS model","what":"Full model fitting code","title":"Analysis code: 'Longitudinal growth in intelligibility of connected speech  from 2 to 8 years in children with cerebral palsy: A novel Bayesian approach'\n","text":"completeness, exact code used fit model manuscript, include sampling settings. (slight variables name changes syntax changes compared exposition).","code":"fit_model <- function(data, chains = 4, cores = 4, sample_prior = \"no\") {   inv_logit <- function(x) 1 / (1 + exp(-x))    formula_beta <- bf(     multiword_intel2 ~       inv_logit(asymlogit) * inv(1 + exp((mid - age) * exp(scale))),     asymlogit ~ 1 + slpg + (0 + slpg | ID | sid),     mid ~ 1 + slpg + (0 + slpg | ID | sid),     scale ~ 1 + slpg + (0 + slpg | ID | sid),     phi ~ 1 + age + slpg,     nl = TRUE   )    prior_fixef <- c(     prior(normal(1.25, .5), nlpar = \"asymlogit\", coef = \"Intercept\"),     prior(normal(-.5, .5), nlpar = \"asymlogit\", class = \"b\"),     prior(normal(50, 6), nlpar = \"mid\", coef = \"Intercept\"),     prior(normal(0, 12), nlpar = \"mid\", class = \"b\"),     prior(normal(-2, 1), nlpar = \"scale\", coef = \"Intercept\"),     prior(normal(0, .5), nlpar = \"scale\", class = \"b\")   )    prior_phi <- c(     prior(normal(2, 1), dpar = \"phi\", class = \"Intercept\"),     prior(normal(0, 1), dpar = \"phi\", class = \"b\")   )    prior_ranef <- c(     prior(normal(0, 1.25), class = \"sd\", nlpar = \"asymlogit\"),     prior(normal(10, 2.5), class = \"sd\", nlpar = \"mid\"),     prior(normal(0, .5), class = \"sd\", nlpar = \"scale\"),     prior(lkj_corr_cholesky(2), class = \"L\")   )    fit_beta <- brm(     formula_beta,     data = data,     prior = c(prior_fixef, prior_phi, prior_ranef),     family = Beta(link = identity, link_phi = \"log\"),     iter = 2000,     chains = chains,     refresh = 25,     sample_prior = sample_prior,     cores = cores,     control = list(adapt_delta = 0.92, max_treedepth = 15)   )    fit_beta }  fit <- fit_model(data, cores = 4, chains = 4)"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2020-mahr-growth.html","id":"model-summary","dir":"Articles","previous_headings":"","what":"Model summary","title":"Analysis code: 'Longitudinal growth in intelligibility of connected speech  from 2 to 8 years in children with cerebral palsy: A novel Bayesian approach'\n","text":"model output (posterior median, SD, 95% intervals): diagnostic output. Rhat < 1.05. Effective sample size > 400. Stan’s internal diagonistic check.","code":"#>  Family: beta  #>   Links: mu = identity; phi = log  #> Formula: multiword_intel2 ~ inv_logit(asymlogit) * inv(1 + exp((mid - age) * exp(scale)))  #>          asymlogit ~ 1 + slpg + (0 + slpg | ID | sid) #>          mid ~ 1 + slpg + (0 + slpg | ID | sid) #>          scale ~ 1 + slpg + (0 + slpg | ID | sid) #>          phi ~ 1 + age + slpg #>    Data: data (Number of observations: 513)  #> Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #>          total post-warmup samples = 4000 #>  #> Group-Level Effects:  #> ~sid (Number of levels: 65)  #>                                                  Estimate Est.Error l-95% CI u-95% CI #> sd(asymlogit_slpgNSMI)                               0.71      0.25     0.26     1.29 #> sd(asymlogit_slpgSMIMLCT)                            2.44      0.41     1.76     3.37 #> sd(asymlogit_slpgSMIMLCI)                            2.23      0.52     1.33     3.35 #> sd(mid_slpgNSMI)                                     6.80      1.45     4.30     9.96 #> sd(mid_slpgSMIMLCT)                                 10.63      1.59     7.68    13.88 #> sd(mid_slpgSMIMLCI)                                 11.01      2.60     5.86    16.00 #> sd(scale_slpgNSMI)                                   0.19      0.12     0.01     0.43 #> sd(scale_slpgSMIMLCT)                                0.23      0.12     0.02     0.47 #> sd(scale_slpgSMIMLCI)                                0.46      0.31     0.03     1.15 #> cor(asymlogit_slpgNSMI,asymlogit_slpgSMIMLCT)       -0.00      0.29    -0.54     0.54 #> cor(asymlogit_slpgNSMI,asymlogit_slpgSMIMLCI)       -0.00      0.29    -0.56     0.54 #> cor(asymlogit_slpgSMIMLCT,asymlogit_slpgSMIMLCI)    -0.01      0.29    -0.57     0.56 #> cor(asymlogit_slpgNSMI,mid_slpgNSMI)                -0.06      0.24    -0.51     0.42 #> cor(asymlogit_slpgSMIMLCT,mid_slpgNSMI)             -0.00      0.29    -0.57     0.55 #> cor(asymlogit_slpgSMIMLCI,mid_slpgNSMI)             -0.01      0.29    -0.55     0.54 #> cor(asymlogit_slpgNSMI,mid_slpgSMIMLCT)              0.01      0.29    -0.55     0.55 #> cor(asymlogit_slpgSMIMLCT,mid_slpgSMIMLCT)           0.05      0.23    -0.42     0.48 #> cor(asymlogit_slpgSMIMLCI,mid_slpgSMIMLCT)          -0.01      0.29    -0.56     0.52 #> cor(mid_slpgNSMI,mid_slpgSMIMLCT)                    0.00      0.29    -0.55     0.57 #> cor(asymlogit_slpgNSMI,mid_slpgSMIMLCI)              0.01      0.28    -0.54     0.55 #> cor(asymlogit_slpgSMIMLCT,mid_slpgSMIMLCI)           0.00      0.29    -0.55     0.55 #> cor(asymlogit_slpgSMIMLCI,mid_slpgSMIMLCI)          -0.06      0.26    -0.57     0.45 #> cor(mid_slpgNSMI,mid_slpgSMIMLCI)                   -0.00      0.29    -0.55     0.57 #> cor(mid_slpgSMIMLCT,mid_slpgSMIMLCI)                 0.00      0.29    -0.54     0.55 #> cor(asymlogit_slpgNSMI,scale_slpgNSMI)               0.14      0.27    -0.41     0.63 #> cor(asymlogit_slpgSMIMLCT,scale_slpgNSMI)           -0.00      0.29    -0.55     0.56 #> cor(asymlogit_slpgSMIMLCI,scale_slpgNSMI)            0.00      0.29    -0.56     0.57 #> cor(mid_slpgNSMI,scale_slpgNSMI)                     0.09      0.27    -0.46     0.57 #> cor(mid_slpgSMIMLCT,scale_slpgNSMI)                  0.01      0.28    -0.55     0.55 #> cor(mid_slpgSMIMLCI,scale_slpgNSMI)                  0.00      0.29    -0.56     0.55 #> cor(asymlogit_slpgNSMI,scale_slpgSMIMLCT)            0.01      0.29    -0.56     0.56 #> cor(asymlogit_slpgSMIMLCT,scale_slpgSMIMLCT)        -0.03      0.27    -0.56     0.50 #> cor(asymlogit_slpgSMIMLCI,scale_slpgSMIMLCT)         0.01      0.28    -0.54     0.56 #> cor(mid_slpgNSMI,scale_slpgSMIMLCT)                 -0.01      0.29    -0.56     0.56 #> cor(mid_slpgSMIMLCT,scale_slpgSMIMLCT)               0.02      0.25    -0.46     0.50 #> cor(mid_slpgSMIMLCI,scale_slpgSMIMLCT)              -0.00      0.29    -0.56     0.56 #> cor(scale_slpgNSMI,scale_slpgSMIMLCT)               -0.00      0.29    -0.55     0.55 #> cor(asymlogit_slpgNSMI,scale_slpgSMIMLCI)           -0.01      0.29    -0.56     0.55 #> cor(asymlogit_slpgSMIMLCT,scale_slpgSMIMLCI)         0.00      0.29    -0.54     0.55 #> cor(asymlogit_slpgSMIMLCI,scale_slpgSMIMLCI)         0.02      0.28    -0.53     0.56 #> cor(mid_slpgNSMI,scale_slpgSMIMLCI)                  0.00      0.29    -0.54     0.55 #> cor(mid_slpgSMIMLCT,scale_slpgSMIMLCI)              -0.01      0.29    -0.56     0.54 #> cor(mid_slpgSMIMLCI,scale_slpgSMIMLCI)              -0.00      0.28    -0.54     0.54 #> cor(scale_slpgNSMI,scale_slpgSMIMLCI)                0.00      0.29    -0.57     0.57 #> cor(scale_slpgSMIMLCT,scale_slpgSMIMLCI)             0.01      0.29    -0.55     0.56 #>  #> Population-Level Effects:  #>                       Estimate Est.Error l-95% CI u-95% CI #> phi_Intercept             2.44      0.31     1.83     3.05 #> asymlogit_Intercept       2.53      0.24     2.05     2.99 #> asymlogit_slpgSMIMLCT    -0.69      0.40    -1.45     0.09 #> asymlogit_slpgSMIMLCI    -1.48      0.51    -2.46    -0.44 #> mid_Intercept            39.06      1.70    35.75    42.49 #> mid_slpgSMIMLCT          15.36      2.97     9.64    21.25 #> mid_slpgSMIMLCI          12.76      9.56    -4.94    32.77 #> scale_Intercept          -2.38      0.10    -2.58    -2.19 #> scale_slpgSMIMLCT        -0.23      0.13    -0.47     0.04 #> scale_slpgSMIMLCI        -0.71      0.35    -1.38     0.02 #> phi_age                   0.02      0.00     0.01     0.03 #> phi_slpgSMIMLCT          -0.35      0.16    -0.67    -0.04 #> phi_slpgSMIMLCI          -0.72      0.22    -1.17    -0.30 #>  #> Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS #> and Tail_ESS are effective sample size measures, and Rhat is the potential #> scale reduction factor on split chains (at convergence, Rhat = 1). #>  Family: beta  #>   Links: mu = identity; phi = log  #> Formula: multiword_intel2 ~ inv_logit(asymlogit) * inv(1 + exp((mid - age) * exp(scale)))  #>          asymlogit ~ 1 + slpg + (0 + slpg | ID | sid) #>          mid ~ 1 + slpg + (0 + slpg | ID | sid) #>          scale ~ 1 + slpg + (0 + slpg | ID | sid) #>          phi ~ 1 + age + slpg #>    Data: data (Number of observations: 513)  #> Samples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; #>          total post-warmup samples = 4000 #>  #> Group-Level Effects:  #> ~sid (Number of levels: 65)  #>                                                  Rhat Bulk_ESS Tail_ESS #> sd(asymlogit_slpgNSMI)                           1.01     1065     1133 #> sd(asymlogit_slpgSMIMLCT)                        1.00     2382     2888 #> sd(asymlogit_slpgSMIMLCI)                        1.00     2361     2354 #> sd(mid_slpgNSMI)                                 1.00     2821     3467 #> sd(mid_slpgSMIMLCT)                              1.00     3461     2928 #> sd(mid_slpgSMIMLCI)                              1.00     3563     2215 #> sd(scale_slpgNSMI)                               1.01      819     1793 #> sd(scale_slpgSMIMLCT)                            1.00      711     1421 #> sd(scale_slpgSMIMLCI)                            1.00     2471     2559 #> cor(asymlogit_slpgNSMI,asymlogit_slpgSMIMLCT)    1.01      531     1256 #> cor(asymlogit_slpgNSMI,asymlogit_slpgSMIMLCI)    1.00     1289     2426 #> cor(asymlogit_slpgSMIMLCT,asymlogit_slpgSMIMLCI) 1.00     1483     2456 #> cor(asymlogit_slpgNSMI,mid_slpgNSMI)             1.00     2291     2801 #> cor(asymlogit_slpgSMIMLCT,mid_slpgNSMI)          1.00     1422     2305 #> cor(asymlogit_slpgSMIMLCI,mid_slpgNSMI)          1.00     1205     2459 #> cor(asymlogit_slpgNSMI,mid_slpgSMIMLCT)          1.00      807     1685 #> cor(asymlogit_slpgSMIMLCT,mid_slpgSMIMLCT)       1.00     2315     2767 #> cor(asymlogit_slpgSMIMLCI,mid_slpgSMIMLCT)       1.00      990     1565 #> cor(mid_slpgNSMI,mid_slpgSMIMLCT)                1.01     1013     2102 #> cor(asymlogit_slpgNSMI,mid_slpgSMIMLCI)          1.00     5526     3134 #> cor(asymlogit_slpgSMIMLCT,mid_slpgSMIMLCI)       1.00     5714     3001 #> cor(asymlogit_slpgSMIMLCI,mid_slpgSMIMLCI)       1.00     6011     3265 #> cor(mid_slpgNSMI,mid_slpgSMIMLCI)                1.00     4993     2953 #> cor(mid_slpgSMIMLCT,mid_slpgSMIMLCI)             1.00     5288     3442 #> cor(asymlogit_slpgNSMI,scale_slpgNSMI)           1.00     5699     3479 #> cor(asymlogit_slpgSMIMLCT,scale_slpgNSMI)        1.00     3546     3424 #> cor(asymlogit_slpgSMIMLCI,scale_slpgNSMI)        1.00     3386     3296 #> cor(mid_slpgNSMI,scale_slpgNSMI)                 1.00     4435     3313 #> cor(mid_slpgSMIMLCT,scale_slpgNSMI)              1.00     3615     3411 #> cor(mid_slpgSMIMLCI,scale_slpgNSMI)              1.00     2886     3688 #> cor(asymlogit_slpgNSMI,scale_slpgSMIMLCT)        1.00     2975     3159 #> cor(asymlogit_slpgSMIMLCT,scale_slpgSMIMLCT)     1.00     4081     3102 #> cor(asymlogit_slpgSMIMLCI,scale_slpgSMIMLCT)     1.00     2787     2935 #> cor(mid_slpgNSMI,scale_slpgSMIMLCT)              1.00     3010     3387 #> cor(mid_slpgSMIMLCT,scale_slpgSMIMLCT)           1.00     3610     3096 #> cor(mid_slpgSMIMLCI,scale_slpgSMIMLCT)           1.00     2942     3543 #> cor(scale_slpgNSMI,scale_slpgSMIMLCT)            1.00     3011     3519 #> cor(asymlogit_slpgNSMI,scale_slpgSMIMLCI)        1.00     6844     3092 #> cor(asymlogit_slpgSMIMLCT,scale_slpgSMIMLCI)     1.00     6062     3319 #> cor(asymlogit_slpgSMIMLCI,scale_slpgSMIMLCI)     1.00     7379     2966 #> cor(mid_slpgNSMI,scale_slpgSMIMLCI)              1.00     5616     3328 #> cor(mid_slpgSMIMLCT,scale_slpgSMIMLCI)           1.00     4729     3314 #> cor(mid_slpgSMIMLCI,scale_slpgSMIMLCI)           1.00     3467     3394 #> cor(scale_slpgNSMI,scale_slpgSMIMLCI)            1.00     3156     3755 #> cor(scale_slpgSMIMLCT,scale_slpgSMIMLCI)         1.00     3877     3563 #>  #> Population-Level Effects:  #>                       Rhat Bulk_ESS Tail_ESS #> phi_Intercept         1.00     4462     3025 #> asymlogit_Intercept   1.00     1064     1577 #> asymlogit_slpgSMIMLCT 1.00     3681     2909 #> asymlogit_slpgSMIMLCI 1.00     4619     3164 #> mid_Intercept         1.00     2555     2794 #> mid_slpgSMIMLCT       1.00     2745     2814 #> mid_slpgSMIMLCI       1.00     1519     1828 #> scale_Intercept       1.00     2419     2549 #> scale_slpgSMIMLCT     1.00     2893     2855 #> scale_slpgSMIMLCI     1.00     2979     2831 #> phi_age               1.00     4518     3435 #> phi_slpgSMIMLCT       1.00     2845     3405 #> phi_slpgSMIMLCI       1.00     4498     3681 #>  #> Samples were drawn using sampling(NUTS). For each parameter, Bulk_ESS #> and Tail_ESS are effective sample size measures, and Rhat is the potential #> scale reduction factor on split chains (at convergence, Rhat = 1). rstan::check_hmc_diagnostics(fit$fit) #>  #> Divergences: #> 0 of 4000 iterations ended with a divergence. #>  #> Tree depth: #> 0 of 4000 iterations saturated the maximum tree depth of 15. #>  #> Energy: #> E-BFMI indicated no pathological behavior."},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2021-mahr-aligners.html","id":"question-1-accuracy-by-aligner","dir":"Articles","previous_headings":"","what":"Question 1 (Accuracy by aligner)","title":"Analysis code: 'Performance of Forced-Alignment Algorithms on Children's  Speech'\n","text":"get marginal means probability scale (type = \"response\") aligner (~ aligner). model used included aligner--class interactions, ignoring estimate averaging sound classes. software prints NOTE remind fact. means: prob estimated probability matching alignment, asymp.LCL asymp.UCL 95% interval estimated probability. marginal means pretty close means child-level proportions: Now, can compare means . look pairwise differences aligners (pairwise ~ aligner). emmeans handles contrast-coding p-value adjustment automatically. use Bonferroni method. Differences odds ratios like X / Y. Values greater 1 indicate advantage X Y, values less 1 indicate advantage Y X, values 1 indicates difference odds. Notably, significance difference Kaldi MFA (SAT). two similar aligners comparison, priori, MFA built top Kaldi uses acoustic model behind scenes.","code":"library(emmeans) #> Warning: package 'emmeans' was built under R version 4.0.3  means_by_aligner <- emmeans(   m_class_int,    spec = ~ aligner,     type = \"response\",    bias.adjust = TRUE,    sigma = sigma ) #> NOTE: Results may be misleading due to involvement in interactions means_by_aligner #>  aligner       prob      SE  df asymp.LCL asymp.UCL #>  Kaldi        0.757 0.00932 Inf     0.738     0.775 #>  MFA (SAT)    0.856 0.00638 Inf     0.843     0.868 #>  MFA (No SAT) 0.769 0.00902 Inf     0.750     0.786 #>  P2FA         0.669 0.01109 Inf     0.647     0.691 #>  Prosodylab   0.607 0.01191 Inf     0.583     0.630 #>  #> Results are averaged over the levels of: class  #> Confidence level used: 0.95  #> Intervals are back-transformed from the logit scale  #> Bias adjustment applied based on sigma = 0.32093 model_data %>%    group_by(aligner) %>%    summarise(     empirical_proportion = mean(prop_matches),     .groups = \"drop\"   ) %>%    left_join(     broom::tidy(means_by_aligner, conf.int = TRUE),      by = c(\"aligner\")   ) %>%    select(     aligner,      empirical_proportion,      marginal_probability = prob,      lower_ci = asymp.LCL,      upper_ci = asymp.UCL   ) #> # A tibble: 5 x 5 #>   aligner      empirical_proportion marginal_probability lower_ci upper_ci #>   <chr>                       <dbl>                <dbl>    <dbl>    <dbl> #> 1 Kaldi                       0.752                0.757    0.738    0.775 #> 2 MFA (SAT)                   0.854                0.856    0.843    0.868 #> 3 MFA (No SAT)                0.762                0.769    0.750    0.786 #> 4 P2FA                        0.667                0.669    0.647    0.691 #> 5 Prosodylab                  0.599                0.607    0.583    0.630 by_aligner <- emmeans(   m_class_int,    spec = pairwise ~ aligner,     type = \"response\",    bias.adjust = TRUE,    sigma = sigma,   adjust = \"bonferroni\" ) #> NOTE: Results may be misleading due to involvement in interactions by_aligner$contrasts #>  contrast                  odds.ratio     SE  df z.ratio p.value #>  Kaldi / MFA (SAT)              0.544 0.0290 Inf -12.386 <.0001  #>  Kaldi / MFA (No SAT)           0.984 0.0516 Inf  -1.261 1.0000  #>  Kaldi / P2FA                   1.632 0.0849 Inf   8.447 <.0001  #>  Kaldi / Prosodylab             2.151 0.1118 Inf  13.779 <.0001  #>  MFA (SAT) / MFA (No SAT)       1.904 0.1015 Inf  11.136 <.0001  #>  MFA (SAT) / P2FA               3.156 0.1672 Inf  20.756 <.0001  #>  MFA (SAT) / Prosodylab         4.162 0.2201 Inf  26.012 <.0001  #>  MFA (No SAT) / P2FA            1.743 0.0908 Inf   9.708 <.0001  #>  MFA (No SAT) / Prosodylab      2.298 0.1195 Inf  15.037 <.0001  #>  P2FA / Prosodylab              1.386 0.0716 Inf   5.354 <.0001  #>  #> Results are averaged over the levels of: class  #> P value adjustment: bonferroni method for 10 tests  #> Tests are performed on the log odds ratio scale  #> Bias adjustment applied based on sigma = 0.32093"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2021-mahr-aligners.html","id":"question-2-accuracy-by-sound-class","dir":"Articles","previous_headings":"","what":"Question 2 (Accuracy by sound class)","title":"Analysis code: 'Performance of Forced-Alignment Algorithms on Children's  Speech'\n","text":"question 2, change aggregating aligner (averaging class) aggregating class (averaging aligner). otherwise use procedure. vowels consistent advantage classes. question 2, also looked class--aligner effects. use effect coding (eff ~ ...) compare phone--class mean average phone--class means. , emmeans handles contrast p-value adjustment automatically. uses \"fdr\" adjustment (false discovery rate, Benjamini & Hochberg, 1995). First, means. importantly, contrasts. manuscript, report classes aligner largest smallest effects. Vowels largest effect aligner. smallest effects sounds Kaldi MFA aligners, plosives fricatives P2FA plosives Prosodylab.","code":"by_class <- emmeans(   m_class_int,    spec = pairwise ~ class,     type = \"response\",    bias.adjust = TRUE,    sigma = sigma,   adjust = \"bonferroni\" ) #> NOTE: Results may be misleading due to involvement in interactions by_class$emmeans #>  class       prob      SE  df asymp.LCL asymp.UCL #>  fricatives 0.724 0.00806 Inf     0.708     0.740 #>  others     0.691 0.00869 Inf     0.674     0.708 #>  plosives   0.705 0.00826 Inf     0.688     0.721 #>  vowels     0.827 0.00573 Inf     0.815     0.838 #>  #> Results are averaged over the levels of: aligner  #> Confidence level used: 0.95  #> Intervals are back-transformed from the logit scale  #> Bias adjustment applied based on sigma = 0.32093 by_class$contrasts #>  contrast              odds.ratio      SE  df z.ratio p.value #>  fricatives / others        1.240 0.02415 Inf   8.461 <.0001  #>  fricatives / plosives      1.160 0.01986 Inf   5.738 <.0001  #>  fricatives / vowels        0.574 0.00979 Inf -35.509 <.0001  #>  others / plosives          0.984 0.01774 Inf  -3.691 0.0013  #>  others / vowels            0.486 0.00875 Inf -42.847 <.0001  #>  plosives / vowels          0.520 0.00801 Inf -45.721 <.0001  #>  #> Results are averaged over the levels of: aligner  #> P value adjustment: bonferroni method for 6 tests  #> Tests are performed on the log odds ratio scale  #> Bias adjustment applied based on sigma = 0.32093 by_aligner_class <- emmeans(   m_class_int,    spec = eff ~ class * aligner,     type = \"response\",    bias.adjust = TRUE,    sigma = sigma ) by_aligner_class$emmeans #>  class      aligner       prob      SE  df asymp.LCL asymp.UCL #>  fricatives Kaldi        0.749 0.01062 Inf     0.728     0.770 #>  others     Kaldi        0.711 0.01191 Inf     0.687     0.734 #>  plosives   Kaldi        0.749 0.01023 Inf     0.728     0.768 #>  vowels     Kaldi        0.811 0.00827 Inf     0.794     0.826 #>  fricatives MFA (SAT)    0.861 0.00729 Inf     0.846     0.875 #>  others     MFA (SAT)    0.801 0.00968 Inf     0.781     0.819 #>  plosives   MFA (SAT)    0.849 0.00734 Inf     0.834     0.862 #>  vowels     MFA (SAT)    0.901 0.00514 Inf     0.890     0.910 #>  fricatives MFA (No SAT) 0.747 0.01068 Inf     0.725     0.767 #>  others     MFA (No SAT) 0.696 0.01220 Inf     0.671     0.719 #>  plosives   MFA (No SAT) 0.742 0.01037 Inf     0.721     0.762 #>  vowels     MFA (No SAT) 0.863 0.00657 Inf     0.849     0.875 #>  fricatives P2FA         0.614 0.01294 Inf     0.589     0.640 #>  others     P2FA         0.676 0.01254 Inf     0.651     0.700 #>  plosives   P2FA         0.618 0.01249 Inf     0.593     0.642 #>  vowels     P2FA         0.757 0.00971 Inf     0.738     0.776 #>  fricatives Prosodylab   0.592 0.01317 Inf     0.566     0.618 #>  others     Prosodylab   0.544 0.01392 Inf     0.517     0.571 #>  plosives   Prosodylab   0.502 0.01315 Inf     0.476     0.528 #>  vowels     Prosodylab   0.766 0.00951 Inf     0.747     0.784 #>  #> Confidence level used: 0.95  #> Intervals are back-transformed from the logit scale  #> Bias adjustment applied based on sigma = 0.32093 by_aligner_class$contrasts #>  contrast                       odds.ratio     SE  df z.ratio p.value #>  fricatives Kaldi effect             1.100 0.0461 Inf   1.087 0.3193  #>  others Kaldi effect                 0.902 0.0395 Inf  -3.501 0.0006  #>  plosives Kaldi effect               1.096 0.0428 Inf   1.064 0.3193  #>  vowels Kaldi effect                 1.584 0.0604 Inf  10.757 <.0001  #>  fricatives MFA (SAT) effect         2.305 0.1076 Inf  16.816 <.0001  #>  others MFA (SAT) effect             1.485 0.0692 Inf   7.405 <.0001  #>  plosives MFA (SAT) effect           2.082 0.0876 Inf  16.235 <.0001  #>  vowels MFA (SAT) effect             3.387 0.1431 Inf  27.699 <.0001  #>  fricatives MFA (No SAT) effect      1.085 0.0453 Inf   0.751 0.4762  #>  others MFA (No SAT) effect          0.837 0.0363 Inf  -5.265 <.0001  #>  plosives MFA (No SAT) effect        1.059 0.0412 Inf   0.186 0.8527  #>  vowels MFA (No SAT) effect          2.335 0.0931 Inf  20.001 <.0001  #>  fricatives P2FA effect              0.578 0.0230 Inf -15.025 <.0001  #>  others P2FA effect                  0.762 0.0327 Inf  -7.491 <.0001  #>  plosives P2FA effect                0.587 0.0220 Inf -15.546 <.0001  #>  vowels P2FA effect                  1.149 0.0425 Inf   2.397 0.0207  #>  fricatives Prosodylab effect        0.526 0.0209 Inf -17.450 <.0001  #>  others Prosodylab effect            0.430 0.0180 Inf -21.395 <.0001  #>  plosives Prosodylab effect          0.361 0.0134 Inf -28.709 <.0001  #>  vowels Prosodylab effect            1.204 0.0448 Inf   3.648 0.0004  #>  #> P value adjustment: fdr method for 20 tests  #> Tests are performed on the log odds ratio scale  #> Bias adjustment applied based on sigma = 0.32093"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2021-mahr-aligners.html","id":"question-3-age-effects","dir":"Articles","previous_headings":"","what":"Question 3 (Age effects)","title":"Analysis code: 'Performance of Forced-Alignment Algorithms on Children's  Speech'\n","text":"now augment model include age. consider model just 2-way interactions model full 3-way interactions. disregard convergence warning diagnosticmax|grad| = 0.00225522 barely threshold tol = 0.002. Model comparison (lowest AIC, chi-square tests) supports full, 3-way model. want compare age-slopes aligners. emtrends() compute age trend (var = \"age_scale\") aligner using effect coding (eff ~ aligner). compare slopes 60 months (= lis``t(age_scale = 0)). values logit (log-odds) scale. Exponentiating produces odds ratios. MFA-SAT Prosodylab 95% intervals overlap 1 (odds ratio) conclude statistically clear age trend. describe effect size response scale, compare match probabilities aligner steepest age effect (Kaldi). calculate match probability age 0 age 1 (= list(age_scale = c(0, 1))) aligner compute pairwise differences probabilities get age 1 advantage age 0 (revpairwise ~ age_scale | aligner computed reverse-pairwise differences—age 1 / age 0 odds ratio scale—use .) Now, can converse aggregation averaging: Look age trend sound class averaging aligners. likewise, look odds ratios. Finally, can examine Aligner x Class age trends.","code":"# Add age, age x aligner, age x class effects m_class_int_age_2way <- glmer(   cbind(n_matches, n_misses) ~      class * aligner + age_scale +      age_scale:class + aligner:age_scale +      (1 | child/aligner),    family = binomial,    model_data )  # Add full 3-way interaction m_class_int_age_3way <- glmer(   cbind(n_matches, n_misses) ~      class * aligner * age_scale +      (1 | child/aligner),    family = binomial,    model_data ) #> Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : #> Model failed to converge with max|grad| = 0.00225522 (tol = 0.002, component 1) format(   anova(m_class_int, m_class_int_age_2way, m_class_int_age_3way),    digits = 3 ) #>                      npar  AIC  BIC logLik deviance Chisq Df Pr(>Chisq) #> m_class_int            22 6848 6952  -3402     6804    NA NA         NA #> m_class_int_age_2way   30 6555 6697  -3248     6495 308.3  8   7.20e-62 #> m_class_int_age_3way   42 6520 6719  -3218     6436  59.2 12   3.11e-08 variances2 <- broom.mixed::tidy(   m_class_int_age_3way,    \"ran_pars\",    scales = \"vcov\" ) sigma2 <- sqrt(sum(variances2$estimate))  aligner_age_trends <- emtrends(   m_class_int_age_3way,    eff ~ aligner,    var = \"age_scale\",    bias.adjust = TRUE,    sigma = sigma2,    at = list(age_scale = 0) ) #> NOTE: Results may be misleading due to involvement in interactions aligner_age_trends #> $emtrends #>  aligner      age_scale.trend     SE  df asymp.LCL asymp.UCL #>  Kaldi                 0.2326 0.0418 Inf    0.1506    0.3146 #>  MFA (SAT)             0.0319 0.0430 Inf   -0.0523    0.1161 #>  MFA (No SAT)          0.1731 0.0418 Inf    0.0910    0.2551 #>  P2FA                  0.1746 0.0414 Inf    0.0934    0.2557 #>  Prosodylab           -0.0357 0.0413 Inf   -0.1168    0.0453 #>  #> Results are averaged over the levels of: class  #> Confidence level used: 0.95  #>  #> $contrasts #>  contrast            estimate     SE  df z.ratio p.value #>  Kaldi effect          0.1173 0.0259 Inf  4.533  <.0001  #>  MFA (SAT) effect     -0.0834 0.0270 Inf -3.094  0.0033  #>  MFA (No SAT) effect   0.0578 0.0259 Inf  2.232  0.0256  #>  P2FA effect           0.0593 0.0255 Inf  2.327  0.0249  #>  Prosodylab effect    -0.1510 0.0254 Inf -5.945  <.0001  #>  #> Results are averaged over the levels of: class  #> P value adjustment: fdr method for 5 tests aligner_age_trends$emtrends %>%    broom::tidy(conf.int = TRUE) %>%    mutate(     or_trend = exp(age_scale.trend),     or_lcl = exp(asymp.LCL),     or_ucl = exp(asymp.UCL),     trend = age_scale.trend,     se = std.error,     z = z.ratio,     p = p.value   ) %>%    select(aligner, starts_with(\"or_\"), trend, se, z, p) #> # A tibble: 5 x 8 #>   aligner      or_trend or_lcl or_ucl   trend     se      z            p #>   <chr>           <dbl>  <dbl>  <dbl>   <dbl>  <dbl>  <dbl>        <dbl> #> 1 Kaldi           1.26   1.16    1.37  0.233  0.0418  5.56  0.0000000269 #> 2 MFA (SAT)       1.03   0.949   1.12  0.0319 0.0430  0.743 0.458        #> 3 MFA (No SAT)    1.19   1.10    1.29  0.173  0.0418  4.14  0.0000354    #> 4 P2FA            1.19   1.10    1.29  0.175  0.0414  4.22  0.0000249    #> 5 Prosodylab      0.965  0.890   1.05 -0.0357 0.0413 -0.865 0.387 aligner_age_trends_1_vs_0 <- emmeans(   m_class_int_age_3way,    revpairwise ~ age_scale | aligner ,    bias.adjust = TRUE,    sigma = sigma2,   type = \"response\",   at = list(age_scale = c(0, 1)) )  #> NOTE: Results may be misleading due to involvement in interactions  aligner_age_trends_1_vs_0$emmeans %>%    broom::tidy(conf.int = TRUE) %>%    filter(aligner == \"Kaldi\") %>%    select(age_scale, aligner, prob, asymp.LCL, asymp.UCL) #> # A tibble: 2 x 5 #>   age_scale aligner  prob asymp.LCL asymp.UCL #>       <dbl> <chr>   <dbl>     <dbl>     <dbl> #> 1         0 Kaldi   0.758     0.741     0.773 #> 2         1 Kaldi   0.797     0.777     0.816 class_age_trends <- emtrends(   m_class_int_age_3way,    eff ~ class,    var = \"age_scale\",    bias.adjust = TRUE,    sigma = sigma2,    at = list(age_scale = 0) ) #> NOTE: Results may be misleading due to involvement in interactions class_age_trends #> $emtrends #>  class      age_scale.trend     SE  df asymp.LCL asymp.UCL #>  fricatives         0.25805 0.0349 Inf   0.18970    0.3264 #>  others             0.13961 0.0353 Inf   0.07042    0.2088 #>  plosives           0.05992 0.0340 Inf  -0.00671    0.1266 #>  vowels             0.00356 0.0341 Inf  -0.06318    0.0703 #>  #> Results are averaged over the levels of: aligner  #> Confidence level used: 0.95  #>  #> $contrasts #>  contrast          estimate      SE  df z.ratio p.value #>  fricatives effect   0.1428 0.01112 Inf  12.844 <.0001  #>  others effect       0.0243 0.01174 Inf   2.072 0.0383  #>  plosives effect    -0.0554 0.00968 Inf  -5.721 <.0001  #>  vowels effect      -0.1117 0.00973 Inf -11.483 <.0001  #>  #> Results are averaged over the levels of: aligner  #> P value adjustment: fdr method for 4 tests class_age_trends$emtrends %>%    broom::tidy(conf.int = TRUE) %>%    mutate(     or_trend = exp(age_scale.trend),     or_lcl = exp(asymp.LCL),     or_ucl = exp(asymp.UCL),     trend = age_scale.trend,     se = std.error,     z = z.ratio,     p = p.value   ) %>%    select(class, starts_with(\"or_\"), trend, se, z, p) #> # A tibble: 4 x 8 #>   class      or_trend or_lcl or_ucl   trend     se     z        p #>   <chr>         <dbl>  <dbl>  <dbl>   <dbl>  <dbl> <dbl>    <dbl> #> 1 fricatives     1.29  1.21    1.39 0.258   0.0349 7.40  1.36e-13 #> 2 others         1.15  1.07    1.23 0.140   0.0353 3.95  7.66e- 5 #> 3 plosives       1.06  0.993   1.13 0.0599  0.0340 1.76  7.80e- 2 #> 4 vowels         1.00  0.939   1.07 0.00356 0.0341 0.105 9.17e- 1 class_aligner_trends <- emtrends(   m_class_int_age_3way,     ~ class | aligner,    var = \"age_scale\",   bias.adjust = TRUE,    sigma = sigma2,   type = \"response\",   at = list(age_scale = c(0, 1)),  )   class_aligner_trends %>%    broom::tidy(conf.int = TRUE) %>%    mutate(     or_trend = exp(age_scale.trend),     or_lcl = exp(asymp.LCL),     or_ucl = exp(asymp.UCL),     trend = age_scale.trend,     se = std.error,     z = z.ratio,     p = p.value   ) %>%    select(aligner, class, starts_with(\"or_\"), trend, se, z, p) #> # A tibble: 20 x 9 #>    aligner      class     or_trend or_lcl or_ucl    trend     se      z        p #>    <chr>        <chr>        <dbl>  <dbl>  <dbl>    <dbl>  <dbl>  <dbl>    <dbl> #>  1 Kaldi        fricativ~    1.46   1.32   1.61   0.376   0.0494  7.62  2.46e-14 #>  2 Kaldi        others       1.23   1.11   1.36   0.207   0.0509  4.07  4.64e- 5 #>  3 Kaldi        plosives     1.25   1.14   1.37   0.224   0.0462  4.84  1.32e- 6 #>  4 Kaldi        vowels       1.13   1.03   1.24   0.123   0.0455  2.71  6.76e- 3 #>  5 MFA (SAT)    fricativ~    1.30   1.17   1.45   0.264   0.0536  4.93  8.10e- 7 #>  6 MFA (SAT)    others       1.01   0.914  1.13   0.0146  0.0535  0.273 7.84e- 1 #>  7 MFA (SAT)    plosives     0.991  0.900  1.09  -0.00925 0.0489 -0.189 8.50e- 1 #>  8 MFA (SAT)    vowels       0.867  0.787  0.956 -0.142   0.0496 -2.87  4.17e- 3 #>  9 MFA (No SAT) fricativ~    1.37   1.25   1.51   0.317   0.0490  6.47  9.82e-11 #> 10 MFA (No SAT) others       1.19   1.08   1.31   0.174   0.0503  3.46  5.44e- 4 #> 11 MFA (No SAT) plosives     1.13   1.03   1.24   0.123   0.0459  2.67  7.49e- 3 #> 12 MFA (No SAT) vowels       1.08   0.986  1.19   0.0784  0.0470  1.67  9.54e- 2 #> 13 P2FA         fricativ~    1.29   1.18   1.42   0.258   0.0471  5.47  4.39e- 8 #> 14 P2FA         others       1.23   1.12   1.36   0.211   0.0500  4.21  2.50e- 5 #> 15 P2FA         plosives     1.09   1.00   1.19   0.0883  0.0447  1.98  4.82e- 2 #> 16 P2FA         vowels       1.15   1.06   1.26   0.141   0.0444  3.18  1.46e- 3 #> 17 Prosodylab   fricativ~    1.08   0.983  1.18   0.0746  0.0468  1.59  1.11e- 1 #> 18 Prosodylab   others       1.10   0.996  1.21   0.0913  0.0489  1.87  6.16e- 2 #> 19 Prosodylab   plosives     0.882  0.808  0.962 -0.126   0.0445 -2.83  4.73e- 3 #> 20 Prosodylab   vowels       0.833  0.763  0.909 -0.183   0.0446 -4.11  4.04e- 5"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2021-mahr-aligners.html","id":"computing-environment","dir":"Articles","previous_headings":"","what":"Computing environment","title":"Analysis code: 'Performance of Forced-Alignment Algorithms on Children's  Speech'\n","text":"Benjamini, Y., Hochberg, Y. (1995). Controlling false discovery rate: practical powerful approach multiple testing. Journal Royal Statistical Society Series B, 57, 289–300. http://www.jstor.org/stable/2346101.","code":"sessioninfo::session_info() #> - Session info --------------------------------------------------------------- #>  setting  value                        #>  version  R version 4.0.2 (2020-06-22) #>  os       Windows 10 x64               #>  system   x86_64, mingw32              #>  ui       RTerm                        #>  language (EN)                         #>  collate  English_United States.1252   #>  ctype    English_United States.1252   #>  tz       America/Chicago              #>  date     2020-10-30                   #>  #> - Packages ------------------------------------------------------------------- #>  ! package      * version date       lib source         #>    assertthat     0.2.1   2019-03-21 [1] CRAN (R 4.0.2) #>    backports      1.1.10  2020-09-15 [1] CRAN (R 4.0.2) #>    base64url      1.4     2018-05-14 [1] CRAN (R 4.0.2) #>    blob           1.2.1   2020-01-20 [1] CRAN (R 4.0.2) #>    boot           1.3-25  2020-04-26 [1] CRAN (R 4.0.2) #>    broom          0.7.2   2020-10-20 [1] CRAN (R 4.0.2) #>    broom.mixed    0.2.6   2020-05-17 [1] CRAN (R 4.0.2) #>    cellranger     1.1.0   2016-07-27 [1] CRAN (R 4.0.2) #>    cli            2.1.0   2020-10-12 [1] CRAN (R 4.0.2) #>    coda           0.19-4  2020-09-30 [1] CRAN (R 4.0.2) #>    codetools      0.2-16  2018-12-24 [1] CRAN (R 4.0.0) #>    colorspace     1.4-1   2019-03-18 [1] CRAN (R 4.0.2) #>    crayon         1.3.4   2017-09-16 [1] CRAN (R 4.0.2) #>    DBI            1.1.0   2019-12-15 [1] CRAN (R 4.0.2) #>    dbplyr         1.4.4   2020-05-27 [1] CRAN (R 4.0.2) #>    digest         0.6.27  2020-10-24 [1] CRAN (R 4.0.3) #>    dplyr        * 1.0.2   2020-08-18 [1] CRAN (R 4.0.2) #>    drake          7.12.6  2020-10-10 [1] CRAN (R 4.0.3) #>    ellipsis       0.3.1   2020-05-15 [1] CRAN (R 4.0.2) #>    emmeans      * 1.5.2-1 2020-10-25 [1] CRAN (R 4.0.3) #>    estimability   1.3     2018-02-11 [1] CRAN (R 4.0.0) #>    evaluate       0.14    2019-05-28 [1] CRAN (R 4.0.2) #>    fansi          0.4.1   2020-01-08 [1] CRAN (R 4.0.2) #>    filelock       1.0.2   2018-10-05 [1] CRAN (R 4.0.2) #>    forcats      * 0.5.0   2020-03-01 [1] CRAN (R 4.0.2) #>    fs             1.5.0   2020-07-31 [1] CRAN (R 4.0.2) #>    generics       0.0.2   2018-11-29 [1] CRAN (R 4.0.2) #>    ggplot2      * 3.3.2   2020-06-19 [1] CRAN (R 4.0.2) #>    glue           1.4.2   2020-08-27 [1] CRAN (R 4.0.2) #>    gtable         0.3.0   2019-03-25 [1] CRAN (R 4.0.2) #>    haven          2.3.1   2020-06-01 [1] CRAN (R 4.0.2) #>    hms            0.5.3   2020-01-08 [1] CRAN (R 4.0.2) #>    htmltools      0.5.0   2020-06-16 [1] CRAN (R 4.0.2) #>    httr           1.4.2   2020-07-20 [1] CRAN (R 4.0.2) #>    igraph         1.2.6   2020-10-06 [1] CRAN (R 4.0.2) #>    jsonlite       1.7.1   2020-09-07 [1] CRAN (R 4.0.2) #>    knitr          1.30    2020-09-22 [1] CRAN (R 4.0.2) #>    lattice        0.20-41 2020-04-02 [1] CRAN (R 4.0.2) #>    lifecycle      0.2.0   2020-03-06 [1] CRAN (R 4.0.2) #>    lme4         * 1.1-25  2020-10-23 [1] CRAN (R 4.0.3) #>    lubridate      1.7.9   2020-06-08 [1] CRAN (R 4.0.2) #>    magrittr       1.5     2014-11-22 [1] CRAN (R 4.0.2) #>    MASS           7.3-53  2020-09-09 [1] CRAN (R 4.0.2) #>    Matrix       * 1.2-18  2019-11-27 [1] CRAN (R 4.0.2) #>    minqa          1.2.4   2014-10-09 [1] CRAN (R 4.0.2) #>    modelr         0.1.8   2020-05-19 [1] CRAN (R 4.0.2) #>    multcomp       1.4-14  2020-09-23 [1] CRAN (R 4.0.2) #>    munsell        0.5.0   2018-06-12 [1] CRAN (R 4.0.2) #>    mvtnorm        1.1-1   2020-06-09 [1] CRAN (R 4.0.0) #>    nlme           3.1-150 2020-10-24 [1] CRAN (R 4.0.3) #>    nloptr         1.2.2.2 2020-07-02 [1] CRAN (R 4.0.2) #>    pillar         1.4.6   2020-07-10 [1] CRAN (R 4.0.2) #>    pkgconfig      2.0.3   2019-09-22 [1] CRAN (R 4.0.2) #>    plyr           1.8.6   2020-03-03 [1] CRAN (R 4.0.2) #>    prettyunits    1.1.1   2020-01-24 [1] CRAN (R 4.0.2) #>    progress       1.2.2   2019-05-16 [1] CRAN (R 4.0.2) #>    purrr        * 0.3.4   2020-04-17 [1] CRAN (R 4.0.2) #>    R6             2.5.0   2020-10-28 [1] CRAN (R 4.0.2) #>    Rcpp           1.0.5   2020-07-06 [1] CRAN (R 4.0.2) #>    readr        * 1.4.0   2020-10-05 [1] CRAN (R 4.0.2) #>    readxl         1.3.1   2019-03-13 [1] CRAN (R 4.0.2) #>    reprex         0.3.0   2019-05-16 [1] CRAN (R 4.0.2) #>    reshape2       1.4.4   2020-04-09 [1] CRAN (R 4.0.2) #>    rlang          0.4.8   2020-10-08 [1] CRAN (R 4.0.3) #>    rmarkdown      2.5     2020-10-21 [1] CRAN (R 4.0.2) #>    rstudioapi     0.11    2020-02-07 [1] CRAN (R 4.0.2) #>    rvest          0.3.6   2020-07-25 [1] CRAN (R 4.0.2) #>    sandwich       3.0-0   2020-10-02 [1] CRAN (R 4.0.2) #>    scales         1.1.1   2020-05-11 [1] CRAN (R 4.0.2) #>    sessioninfo    1.1.1   2018-11-05 [1] CRAN (R 4.0.2) #>    statmod        1.4.35  2020-10-19 [1] CRAN (R 4.0.3) #>    storr          1.2.4   2020-10-12 [1] CRAN (R 4.0.3) #>    stringi        1.5.3   2020-09-09 [1] CRAN (R 4.0.2) #>    stringr      * 1.4.0   2019-02-10 [1] CRAN (R 4.0.2) #>    survival       3.2-7   2020-09-28 [1] CRAN (R 4.0.2) #>    TH.data        1.0-10  2019-01-21 [1] CRAN (R 4.0.2) #>    tibble       * 3.0.4   2020-10-12 [1] CRAN (R 4.0.3) #>    tidyr        * 1.1.2   2020-08-27 [1] CRAN (R 4.0.2) #>    tidyselect     1.1.0   2020-05-11 [1] CRAN (R 4.0.2) #>    tidyverse    * 1.3.0   2019-11-21 [1] CRAN (R 4.0.2) #>  D TMB            1.7.18  2020-07-27 [1] CRAN (R 4.0.2) #>    txtq           0.2.3   2020-06-23 [1] CRAN (R 4.0.2) #>    utf8           1.1.4   2018-05-24 [1] CRAN (R 4.0.2) #>    vctrs          0.3.4   2020-08-29 [1] CRAN (R 4.0.2) #>    withr          2.3.0   2020-09-22 [1] CRAN (R 4.0.2) #>    xfun           0.18    2020-09-29 [1] CRAN (R 4.0.2) #>    xml2           1.3.2   2020-04-23 [1] CRAN (R 4.0.2) #>    xtable         1.8-4   2019-04-21 [1] CRAN (R 4.0.2) #>    yaml           2.2.1   2020-02-01 [1] CRAN (R 4.0.0) #>    zoo            1.8-8   2020-05-02 [1] CRAN (R 4.0.2) #>  #> [1] C:/Users/Tristan/Documents/R/win-library/4.0 #> [2] C:/Program Files/R/R-4.0.2/library #>  #>  D -- DLL MD5 mismatch, broken installation."},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2022-long-vss.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Analysis code: 'Longitudinal change in speech classification between 4 and  10 years in children with cerebral palsy'\n","text":"data features repeated measurements VSS levels rating. first rows data set : child uniquely identifies child, age_group child’s age years, age_group_4 age years minus 4 (model intercepts age-4 estimates), vss_rating ordered categorical variable levels 1, 2, 3, 4.","code":"library(tidyverse) data #> # A tibble: 328 x 4 #>    child age_group age_group_4 vss_rating #>    <chr>     <dbl>       <dbl> <ord>      #>  1 p001          4           0 4          #>  2 p001          6           2 3          #>  3 p002          4           0 4          #>  4 p002          6           2 3          #>  5 p002          8           4 2          #>  6 p002         10           6 2          #>  7 p003          4           0 2          #>  8 p003          6           2 2          #>  9 p003         10           6 1          #> 10 p004          4           0 4          #> # ... with 318 more rows  unique(data$vss_rating) #> [1] 4 3 2 1 #> Levels: 1 < 2 < 3 < 4"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2022-long-vss.html","id":"model-for-research-question-1","dir":"Articles","previous_headings":"","what":"Model for research question 1","title":"Analysis code: 'Longitudinal change in speech classification between 4 and  10 years in children with cerebral palsy'\n","text":"use brms package (vers. 2.16.1, Bürkner, 2017) fit Bayesian ordinal regression model Stan (vers. 2.27.0, Carpenter et al., 2017). describe syntax model .","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2022-long-vss.html","id":"preliminaries","dir":"Articles","previous_headings":"Model for research question 1","what":"Preliminaries","title":"Analysis code: 'Longitudinal change in speech classification between 4 and  10 years in children with cerebral palsy'\n","text":"convention, need three things generalized linear model R: model family specifying link function response distribution model formula specifying response predictor variables data already seen data model, review family formula arguments. use cumulative(link = \"logit\") family perform ordinal regression estimating rating thresholds using cumulative probabilities log-odds (logit) scale. ordinal models brms, see tutorial Bürkner & Vuorre (2019). Broadly speaking, assume latent continuous speech severity scale ratings carve variable different regions image : Diagram model intuition. model’s job estimate thresholds ratings change age within children. thresholds estimated logit (log-odds) scale, represent cumulative rating probabilities logit scale. brms uses R’s modeling formula syntax order specify mixed effect model. illustrate syntax elaborating intercept-model fully specified model: using Bayesian model, need one additional thing model besides data, family formula: priors model parameters. priors setup plausible values model parameters see data. model updates prior distributions using data obtain posterior distribution. use default flat weakly informative priors models, ruling parameter values based domain knowledge. respect, prior distributions computational devices: use get model machinery going. default priors: Many rows redundant, describing default prior class parameters (like b fixed effects) priors member class (like age_group_4 class b). general, flat priors fixed effects terms, wide-tailed Student distributions three degrees freedom population average rating thresholds (class Intercept) random-effect variances (class sd), uniform prior correlation matrices (lkj(1) class cor) correlation random effect parameters.","code":"# estimate three thresholds: 1|2, 2|3, 3|4 vss_rating ~ 1  # estimate thresholds at x = 0 and how they change with age vss_rating ~ 1 + age_group_4  # estimate thresholds at x = 0 for an average child and how the  # average thresholds change with age. allow each child to have their  # own thresholds by computing an adjustment away from the average  # threshold (by-child random intercepts).  vss_rating ~ 1 + age_group_4 + (1 | child)  # do the same as above but also estimate by-child adjustments for  # age effect (by child random age slopes) vss_rating ~ 1 + age_group_4 + (1 + age_group_4 | child) library(brms) priors <- get_prior(   formula = vss_rating ~ 1 + age_group_4 + (1 + age_group_4 | child),    family = cumulative(),   data = data,  )  priors #>                 prior     class        coef group  #>                (flat)         b                    #>                (flat)         b age_group_4        #>                lkj(1)       cor                    #>                lkj(1)       cor             child  #>  student_t(3, 0, 2.5) Intercept                    #>  student_t(3, 0, 2.5) Intercept           1        #>  student_t(3, 0, 2.5) Intercept           2        #>  student_t(3, 0, 2.5) Intercept           3        #>  student_t(3, 0, 2.5)        sd                    #>  student_t(3, 0, 2.5)        sd             child  #>  student_t(3, 0, 2.5)        sd age_group_4 child  #>  student_t(3, 0, 2.5)        sd   Intercept child"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2022-long-vss.html","id":"model-code-and-output","dir":"Articles","previous_headings":"Model for research question 1","what":"Model code and output","title":"Analysis code: 'Longitudinal change in speech classification between 4 and  10 years in children with cerebral palsy'\n","text":"actual code use fit model. ’s stored custom function, additional options specified including random number seed (reproducibility) options run Monte Carlo sampling Stan: model computes runs 2000 sampling iterations 8 sampling chains. first 1000 warm-iterations sampler adapts model discarded, leaving us posterior distribution 8,000 draws model parameters: model passes Hamiltonian Monte Carlo diagnostics: model parameters following estimates: Group-Level Effects describes random effects variances correlations. Population-Level effects describe rating thresholds average child age 4 Intercept[] terms age slope average child age_group_4. Family Specific Parameters relevant hard code shape latent severity scale. Within , summary diagnostic statistics model parameters: Estimate median posterior distribution Est.Error median absolute deviation posterior distribution l-95% CI, u-95% CI provide 95% posterior interval Rhat, Bulk_ESS, Tail_ESS diagnostic measures described output. acceptable values. Exponentiating -2 times age coefficient gives odds ratio reported manuscript two-year-increase age rating probabilities: Posterior expectations model expected probability VSS rating level. can get predictions model’s average child creating “fake” child asking prediction conditioning child variable (re_formula = NA). 8000 posterior samples age rating level: can two things now: Compute quantiles probabilities compute expected ratings. .epred column posterior median probability rating level (.category) .lower .upper 95% quantiles. probabilities shift favoring level 4 age 4 favoring level 2 age 10. multiple rating’s numerical level probability sum results, get average expected rating. compute quantiles expected ratings: problem estimates far conditional expectations. expectations average child, child whose random effect adjustments zeroed . describe average children. need compute marginal mean.","code":"model <- brm(   vss_rating ~ 1 + age_group_4 + (1 + age_group_4 | child),   data = data,   family = cumulative(),   backend = \"cmdstanr\",   cores = 8,   chains = 8,   seed = 20210929,   control = list(adapt_delta = .9) ) nchains(model) #> [1] 8 niterations(model) #> [1] 1000 ndraws(model) #> [1] 8000 rstan::check_hmc_diagnostics(model$fit) #>  #> Divergences: #> 0 of 8000 iterations ended with a divergence. #>  #> Tree depth: #> 0 of 8000 iterations saturated the maximum tree depth of 10. #>  #> Energy: #> E-BFMI indicated no pathological behavior. summary(model, robust = TRUE) #>  Family: cumulative  #>   Links: mu = logit; disc = identity  #> Formula: vss_rating ~ age_group_4 + (age_group_4 | child)  #>    Data: data (Number of observations: 328)  #>   Draws: 8 chains, each with iter = 1000; warmup = 0; thin = 1; #>          total post-warmup draws = 8000 #>  #> Group-Level Effects:  #> ~child (Number of levels: 101)  #>                            Estimate Est.Error l-95% CI u-95% CI Rhat #> sd(Intercept)                  5.90      1.04     4.21     8.40 1.00 #> sd(age_group_4)                0.49      0.22     0.10     0.97 1.01 #> cor(Intercept,age_group_4)     0.65      0.27    -0.08     0.98 1.00 #>                            Bulk_ESS Tail_ESS #> sd(Intercept)                  1325     2682 #> sd(age_group_4)                1105     1594 #> cor(Intercept,age_group_4)     1941     2386 #>  #> Population-Level Effects:  #>              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS #> Intercept[1]   -10.57      1.45   -13.98    -8.14 1.00     1461 #> Intercept[2]    -4.98      0.98    -7.20    -3.33 1.00     1372 #> Intercept[3]    -1.01      0.74    -2.64     0.36 1.00     1468 #> age_group_4     -0.87      0.16    -1.22    -0.58 1.00     2603 #>              Tail_ESS #> Intercept[1]     2435 #> Intercept[2]     2537 #> Intercept[3]     2891 #> age_group_4      3288 #>  #> Family Specific Parameters:  #>      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #> disc     1.00      0.00     1.00     1.00   NA       NA       NA #>  #> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS #> and Tail_ESS are effective sample size measures, and Rhat is the potential #> scale reduction factor on split chains (at convergence, Rhat = 1). b_age <- fixef(model, robust = TRUE)[\"age_group_4\", ] exp(-2 * b_age) |> round(1) #>  Estimate Est.Error      Q2.5     Q97.5  #>       5.7       0.7      11.6       3.2 library(tidybayes) #>  #> Attaching package: 'tidybayes' #> The following objects are masked from 'package:brms': #>  #>     dstudent_t, pstudent_t, qstudent_t, rstudent_t  one_new_child <- data.frame(   child = \"fake\",   age_group_4 = c(0, 2, 4, 6)  )   draws_average_child <- one_new_child %>%    add_epred_draws(     model,      allow_new_levels = TRUE,      re_formula = NA   ) count(draws_average_child, child, age_group_4, .category) #> # A tibble: 16 x 5 #> # Groups:   child, age_group_4, .row, .category [16] #>    child age_group_4  .row .category     n #>    <chr>       <dbl> <int> <fct>     <int> #>  1 fake            0     1 1          8000 #>  2 fake            0     1 2          8000 #>  3 fake            0     1 3          8000 #>  4 fake            0     1 4          8000 #>  5 fake            2     2 1          8000 #>  6 fake            2     2 2          8000 #>  7 fake            2     2 3          8000 #>  8 fake            2     2 4          8000 #>  9 fake            4     3 1          8000 #> 10 fake            4     3 2          8000 #> 11 fake            4     3 3          8000 #> 12 fake            4     3 4          8000 #> 13 fake            6     4 1          8000 #> 14 fake            6     4 2          8000 #> 15 fake            6     4 3          8000 #> 16 fake            6     4 4          8000 draws_average_child %>%    ggdist::median_qi() %>%    select(child:.upper) #> # A tibble: 16 x 7 #>    child age_group_4  .row .category    .epred      .lower   .upper #>    <chr>       <dbl> <int> <fct>         <dbl>       <dbl>    <dbl> #>  1 fake            0     1 1         0.0000256 0.000000848 0.000292 #>  2 fake            0     1 2         0.00679   0.000743    0.0342   #>  3 fake            0     1 3         0.259     0.0649      0.564    #>  4 fake            0     1 4         0.734     0.411       0.934    #>  5 fake            2     2 1         0.000147  0.00000676  0.00129  #>  6 fake            2     2 2         0.0374    0.00538     0.151    #>  7 fake            2     2 3         0.622     0.300       0.818    #>  8 fake            2     2 4         0.329     0.102       0.688    #>  9 fake            4     3 1         0.000838  0.0000492   0.00616  #> 10 fake            4     3 2         0.181     0.0320      0.511    #> 11 fake            4     3 3         0.704     0.449       0.849    #> 12 fake            4     3 4         0.0806    0.0134      0.311    #> 13 fake            6     4 1         0.00481   0.000304    0.0332   #> 14 fake            6     4 2         0.551     0.141       0.873    #> 15 fake            6     4 3         0.425     0.103       0.774    #> 16 fake            6     4 4         0.0151    0.00141     0.104 draws_expected_ratings <- draws_average_child %>%   ungroup() %>%    mutate(     weight = as.numeric(.category) * .epred   ) %>%    group_by(child, age_group_4, .draw) %>%    summarise(     expected_rating = sum(weight),     .groups = \"drop\"   ) draws_expected_ratings #> # A tibble: 32,000 x 4 #>    child age_group_4 .draw expected_rating #>    <chr>       <dbl> <int>           <dbl> #>  1 fake            0     1            3.67 #>  2 fake            0     2            3.76 #>  3 fake            0     3            3.62 #>  4 fake            0     4            3.84 #>  5 fake            0     5            3.82 #>  6 fake            0     6            3.73 #>  7 fake            0     7            3.75 #>  8 fake            0     8            3.55 #>  9 fake            0     9            3.53 #> 10 fake            0    10            3.59 #> # ... with 31,990 more rows draws_expected_ratings %>%   group_by(child, age_group_4) %>%    ggdist::median_qi(expected_rating) #> # A tibble: 4 x 8 #>   child age_group_4 expected_rating .lower .upper .width .point .interval #>   <chr>       <dbl>           <dbl>  <dbl>  <dbl>  <dbl> <chr>  <chr>     #> 1 fake            0            3.73   3.39   3.93   0.95 median qi        #> 2 fake            2            3.29   2.98   3.68   0.95 median qi        #> 3 fake            4            2.90   2.50   3.26   0.95 median qi        #> 4 fake            6            2.45   2.09   2.95   0.95 median qi"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2022-long-vss.html","id":"computation-of-marginal-means","dir":"Articles","previous_headings":"Model for research question 1","what":"Computation of marginal means","title":"Analysis code: 'Longitudinal change in speech classification between 4 and  10 years in children with cerebral palsy'\n","text":"model’s fixed effects estimates ignore -child variability thresholds age-slopes compute (marginal) average rating probabilities, ’d need average variability. used following procedure. Create 100 new unobserved children. Generate posterior expectations children. posterior draw, average 100 children together. now posterior distribution 8,000 means average variability 100 children. new chidren: compute expectations : average children within draw together: code provides marginal VSS level rating probabilities reported manuscript: Marginal expected ratings computed taking expected ratings average probabilities within draw.","code":"data_100_new_kids #> # A tibble: 400 x 3 #>    child age_group_4 age_group #>    <chr>       <dbl>     <dbl> #>  1 .c001           0         4 #>  2 .c001           2         6 #>  3 .c001           4         8 #>  4 .c001           6        10 #>  5 .c002           0         4 #>  6 .c002           2         6 #>  7 .c002           4         8 #>  8 .c002           6        10 #>  9 .c003           0         4 #> 10 .c003           2         6 #> # ... with 390 more rows draws_posterior_epred_100_new_children <- data_100_new_kids %>%   tidybayes::add_epred_draws(     model,      # include all random effects     re_formula = NULL,     allow_new_levels = TRUE,     sample_new_levels = \"gaussian\"   ) draws_posterior_epred_100_new_children_means <-   draws_posterior_epred_100_new_children %>%     group_by(.draw, age_group, age_group_4, .category) %>%     summarise(       .epred = mean(.epred),       .groups = \"drop\"     ) draws_posterior_epred_100_new_children_means draws_posterior_epred_100_new_children_means #> # A tibble: 128,000 x 5 #>    .draw age_group_4 age_group .category .epred #>    <int>       <dbl>     <dbl> <fct>      <dbl> #>  1     1           0         4 1         0.0417 #>  2     1           0         4 2         0.138  #>  3     1           0         4 3         0.282  #>  4     1           0         4 4         0.539  #>  5     1           2         6 1         0.0937 #>  6     1           2         6 2         0.225  #>  7     1           2         6 3         0.260  #>  8     1           2         6 4         0.421  #>  9     1           4         8 1         0.184  #> 10     1           4         8 2         0.278  #> # ... with 127,990 more rows #> # A tibble: 16 x 6 #>    age_group age_group_4 .category .epred  .lower .upper #>        <dbl>       <dbl> <fct>      <dbl>   <dbl>  <dbl> #>  1         4           0 1         0.0417 0.00791  0.108 #>  2         4           0 2         0.163  0.0977   0.243 #>  3         4           0 3         0.223  0.150    0.310 #>  4         4           0 4         0.565  0.443    0.685 #>  5         6           2 1         0.0952 0.0380   0.180 #>  6         6           2 2         0.216  0.140    0.308 #>  7         6           2 3         0.222  0.153    0.306 #>  8         6           2 4         0.457  0.338    0.579 #>  9         8           4 1         0.171  0.0916   0.274 #> 10         8           4 2         0.245  0.166    0.338 #> 11         8           4 3         0.206  0.138    0.288 #> 12         8           4 4         0.372  0.256    0.493 #> 13        10           6 1         0.257  0.158    0.374 #> 14        10           6 2         0.250  0.167    0.344 #> 15        10           6 3         0.181  0.116    0.262 #> 16        10           6 4         0.304  0.190    0.437"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2022-long-vss.html","id":"child-specific-age-trajectories","dir":"Articles","previous_headings":"Model for research question 1","what":"Child-specific age trajectories","title":"Analysis code: 'Longitudinal change in speech classification between 4 and  10 years in children with cerebral palsy'\n","text":"reported manuscript, model estimate growth trajectories child. can visualize estimates observed data. can see model learned/predicted children partial data (less four observations). First note 4 possible ratings 4 timepoints many duplicated patterns data data. frequent patterns: 14 children 4’s estimated trajectory need visualize data . leads following plot one panel per pattern: Child-specific trajectories. Points represent observed ratings. Bands 95% posterior intervals expected rating. numbers panels number children pattern data. Colors represent age-4 rating levels.","code":"data_vss_wide %>%    count(     vss_at_4,      vss_at_6,      vss_at_8,      vss_at_10,      name = \"num_children\"   ) %>%    arrange(desc(num_children))"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2022-long-vss.html","id":"research-question-2-model","dir":"Articles","previous_headings":"","what":"Research question 2 model","title":"Analysis code: 'Longitudinal change in speech classification between 4 and  10 years in children with cerebral palsy'\n","text":"research question 2, include initial (age-4) ratings categorical predictor variable called starting_rating. also provide continuous version called starting_rating_cont. use continuous version compute age initial rating interaction. model, centered age age 10 years occurs 0 (model intercept), using variable age_group_10. data following structure:","code":"data2 %>%    distinct(starting_rating, starting_rating_cont) #> # A tibble: 4 x 2 #>   starting_rating starting_rating_cont #>   <fct>                          <dbl> #> 1 4                                  3 #> 2 2                                  1 #> 3 3                                  2 #> 4 1                                  0 data2 #> # A tibble: 222 x 5 #>    child vss_rating starting_rating age_group_10 starting_rating_cont #>    <chr> <ord>      <fct>                  <dbl>                <dbl> #>  1 p001  3          4                         -4                    3 #>  2 p002  3          4                         -4                    3 #>  3 p002  2          4                         -2                    3 #>  4 p002  2          4                          0                    3 #>  5 p003  2          2                         -4                    1 #>  6 p003  1          2                          0                    1 #>  7 p004  4          4                         -4                    3 #>  8 p004  4          4                         -2                    3 #>  9 p004  4          4                          0                    3 #> 10 p005  2          2                         -4                    1 #> # ... with 212 more rows"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2022-long-vss.html","id":"model-code-and-output-1","dir":"Articles","previous_headings":"Research question 2 model","what":"Model code and output","title":"Analysis code: 'Longitudinal change in speech classification between 4 and  10 years in children with cerebral palsy'\n","text":"model estimates age trajectories ages 6 8 10 using age-4 VSS level rating predictor. Thresholds age-slopes adjusted using starting rating values using starting_rating predictor age_group_10:starting_rating_cont interaction. model passes model diagnostics: model following output, although inferences driven solely estimated marginal means:","code":"model_t <- brm(   vss_rating ~      1 + age_group_10 + starting_rating +      age_group_10:starting_rating_cont +      (1 + age_group_10 | child),   data = data2,   family = cumulative(),   backend = \"cmdstanr\",   cores = 8,   chains = 8,   seed = 20211014,   control = list(adapt_delta = .95) ) rstan::check_hmc_diagnostics(model_t$fit) #>  #> Divergences: #> 0 of 8000 iterations ended with a divergence. #>  #> Tree depth: #> 0 of 8000 iterations saturated the maximum tree depth of 10. #>  #> Energy: #> E-BFMI indicated no pathological behavior. summary(model_t, robust = TRUE) #>  Family: cumulative  #>   Links: mu = logit; disc = identity  #> Formula: vss_rating ~ age_group_10 + starting_rating + age_group_10:starting_rating_cont + (age_group_10 | child)  #>    Data: data2 (Number of observations: 222)  #>   Draws: 8 chains, each with iter = 1000; warmup = 0; thin = 1; #>          total post-warmup draws = 8000 #>  #> Group-Level Effects:  #> ~child (Number of levels: 98)  #>                             Estimate Est.Error l-95% CI u-95% CI Rhat #> sd(Intercept)                   6.57      1.61     4.07    10.55 1.00 #> sd(age_group_10)                0.99      0.35     0.41     1.87 1.00 #> cor(Intercept,age_group_10)     0.97      0.03     0.78     1.00 1.00 #>                             Bulk_ESS Tail_ESS #> sd(Intercept)                   1847     2904 #> sd(age_group_10)                2071     3070 #> cor(Intercept,age_group_10)     3469     5136 #>  #> Population-Level Effects:  #>                                   Estimate Est.Error l-95% CI u-95% CI #> Intercept[1]                         14.05      4.36     6.76    24.95 #> Intercept[2]                         22.29      5.16    13.71    35.19 #> Intercept[3]                         27.36      5.58    18.00    41.20 #> age_group_10                         -4.02      1.07    -6.70    -2.34 #> starting_rating2                      6.45      2.83     1.44    12.65 #> starting_rating3                     14.38      4.22     7.22    24.51 #> starting_rating4                     29.37      6.22    19.10    45.15 #> age_group_10:starting_rating_cont     1.32      0.42     0.65     2.37 #>                                   Rhat Bulk_ESS Tail_ESS #> Intercept[1]                      1.00     2553     2963 #> Intercept[2]                      1.00     2215     2742 #> Intercept[3]                      1.00     2088     2722 #> age_group_10                      1.00     2397     2900 #> starting_rating2                  1.00     3687     4017 #> starting_rating3                  1.00     2484     2946 #> starting_rating4                  1.00     1984     2788 #> age_group_10:starting_rating_cont 1.00     2399     3142 #>  #> Family Specific Parameters:  #>      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #> disc     1.00      0.00     1.00     1.00   NA       NA       NA #>  #> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS #> and Tail_ESS are effective sample size measures, and Rhat is the potential #> scale reduction factor on split chains (at convergence, Rhat = 1)."},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2022-long-vss.html","id":"marginal-means","dir":"Articles","previous_headings":"Research question 2 model","what":"Marginal means","title":"Analysis code: 'Longitudinal change in speech classification between 4 and  10 years in children with cerebral palsy'\n","text":"made inferences using marginal means using procedure . adjustment simulate children starting (age-4) rating. Simulate 400 new children (100 per initial starting rating). Compute posterior expectations children get rating probabilities. posterior draw, compute average rating probability batch 100 children per initial rating levels. Keep just age-10 probabilities. now posterior distribution average age-10 outcomes (rating probabilities) age-4 rating level.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2022-long-vss.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Analysis code: 'Longitudinal change in speech classification between 4 and  10 years in children with cerebral palsy'\n","text":"Bürkner, P.-C. (2017). brms: R package Bayesian multilevel models using Stan. Journal Statistical Software, 80(1), 1–28. doi:[10.18637/jss.v080.i01](https://doi.org/10.18637/jss.v080.i01) Bürkner, P.-C., & Vuorre, M. (2019). Ordinal regression models psychology: tutorial. Advances Methods Practices Psychological Science, 2(1), 77–101. doi:[10.1177/2515245918823199](https://doi.org/10.1177/2515245918823199) Carpenter, B., Gelman, ., Hoffman, M., Lee, D., Goodrich, B., Betancourt, M., … Allen Riddell, . (2017). Stan: probabilistic programming language. Journal Statistical Software, 76(1), 1–32. doi:[10.18637/jss.v076.i01](https://doi.org/10.18637/jss.v076.i01) R Core Team. (2021). R: language environment statistical computing. Vienna, Austria: R Foundation Statistical Computing. Retrieved https://www.R-project.org/","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2025-mahr-intel-rate.html","id":"data","dir":"Articles","previous_headings":"","what":"Data","title":"Analysis code: 'Speech development between 30 and 119 months in typical  children III: Interaction between speaking rate and intelligibility'\n","text":"first set analyses considered child’s mean speaking rate intelligibility utterance length. child: child identifier speaking_sps, artic_sps, speaking_sps_3, artic_sps_3: speaking rate articulation rate syllables per second. _3 versions 3 subtracted values centered 3 sps. age_months, age_bin, age_48: child age months, rough age group child (plotting), age months minus 48 (values centered 48 months) tocs_level, tocs_length, tocs_level_3: utterance length number, factor, number centered 3 word. length_longest: longest utterance reached child. intelligibility, mwi: observed intelligibility intelligibility compressed range .001–.999 beta regression.","code":"library(tidyverse) data_model <- targets::tar_read(\"data_model_anon\") data_model #> # A tibble: 2,196 × 14 #>    child speaking_sps artic_sps age_months tocs_level length_longest age_bin #>    <chr>        <dbl>     <dbl>      <dbl>      <dbl>          <dbl>   <dbl> #>  1 c464          2.71      2.71         30          3              4      36 #>  2 c464          2.55      2.62         30          4              4      36 #>  3 c017          2.04      2.08         30          3              3      36 #>  4 c315          2.03      2.20         30          3              3      36 #>  5 c468          1.87      1.90         30          3              3      36 #>  6 c154          2.25      2.41         30          3              3      36 #>  7 c338          2.59      2.69         31          3              3      36 #>  8 c110          2.69      2.79         31          3              4      36 #>  9 c110          2.66      2.74         31          4              4      36 #> 10 c431          3.30      3.30         31          3              3      36 #> # ℹ 2,186 more rows #> # ℹ 7 more variables: intelligibility <dbl>, mwi <dbl>, speaking_sps_3 <dbl>, #> #   artic_sps_3 <dbl>, age_48 <dbl>, tocs_length <fct>, tocs_level_3 <dbl>"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2025-mahr-intel-rate.html","id":"correlations","dir":"Articles","previous_headings":"","what":"Correlations","title":"Analysis code: 'Speech development between 30 and 119 months in typical  children III: Interaction between speaking rate and intelligibility'\n","text":"","code":"data_for_cors <- data_model |>    group_by(tocs_length) |>    mutate(     # age-residualized     r_intel = residuals(lm(intelligibility ~ splines::ns(age_months, 3))),     r_rate = residuals(lm(speaking_sps_3 ~ splines::ns(age_months, 3))),   ) |>    ungroup()  data_for_cors |>    select(tocs_length, age_months, intelligibility, speaking_sps, r_intel, r_rate) |>    group_by(tocs_length) |>    reframe(     pick(everything()) |> as.matrix() |> Hmisc::rcorr() |> broom::tidy()   ) |>    mutate(     estimate = round(estimate, 3),     p.value = scales::label_pvalue()(p.value)   ) |>    knitr::kable(align = \"rllrrr\")"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2025-mahr-intel-rate.html","id":"beta-regression-model","dir":"Articles","previous_headings":"","what":"Beta regression model","title":"Analysis code: 'Speech development between 30 and 119 months in typical  children III: Interaction between speaking rate and intelligibility'\n","text":"Analyses carried R programming language (vers. 4.4.0, R Core Team, 2024). HMC sampling performed using Stan (vers. 2.34.1, Stan Development Team, 2024) brms R package (vers. 2.21.0, Bürkner, 2017) cmdstanr backend (vers. 0.7.1, Gabry, Češnovar, & Johnson, 2024). compared models approximate leave-one-cross validation loo R package (vers. 2.7.0, Vehtari, Gelman, & Gabry, 2017). wrote custom function fitting beta regression models. user passes data flavor flavor determines model formula use. Models seeded using dates (seed) reproducibility. models compared manuscript follows: models (except model_rs_monotonic_length) formula includes categorical effect utterance length tocs_length, speaking rate centered 3 sps tocs_length, adjustment rate effect length speaing_sps_3:tocs_length, 3-df spline age ns(...). phi ~ regression lines allows dispersion beta distribution change age. differentiated models variables used random (-child) effects. model_rs_monotonic_length model_rs_monotonic_length model applies method Bürkner & Charpentier (2020), using mo() mark variable monotonic. winning model basis model comparison model_rs_length (-child length effects). However, bears noting appropriate model comparison compare using leave-one-child-cross-validation. approach computational prohibitive require fitting 538 versions model.","code":"library(splines) library(brms) fit_beta_model <- function(   data,   flavor = NULL,   priors = NULL,   formula = NULL,   file = NULL,   seed = 20221013,   adapt_delta = .8,   iter = 2000 ) {   formulas <- list(     ri_main_interaction = bf(       mwi ~ ns(age_48, knots = c(9, 25), Boundary.knots = c(-18, 71)) +         tocs_length +         speaking_sps_3 +         speaking_sps_3:tocs_length +         (1 | child),       phi ~ ns(age_48, knots = 17, Boundary.knots = c(-18, 71)),       family = Beta()     ),     ri_no_main_length = bf(       mwi ~ ns(age_48, knots = c(9, 25), Boundary.knots = c(-18, 71)) +         speaking_sps +         (1 | child),       phi ~ ns(age_48, knots = 17, Boundary.knots = c(-18, 71)),       family = Beta()     ),     ri_no_main_interaction = bf(       mwi ~ ns(age_48, knots = c(9, 25), Boundary.knots = c(-18, 71)) +         tocs_length +         speaking_sps_3 +         (1 | child),       phi ~ ns(age_48, knots = 17, Boundary.knots = c(-18, 71)),       family = Beta()     ),     rs_for_rate = bf(       mwi ~ ns(age_48, knots = c(9, 25), Boundary.knots = c(-18, 71)) +         tocs_length +         speaking_sps_3 +         speaking_sps_3:tocs_length +         (speaking_sps_3 | child),       phi ~ ns(age_48, knots = 17, Boundary.knots = c(-18, 71)),       family = Beta()     ),     rs_monotonic_length = bf(       mwi ~ ns(age_48, knots = c(9, 25), Boundary.knots = c(-18, 71)) +         mo(tocs_level) * speaking_sps_3 +         (mo(tocs_level) | child),       phi ~ ns(age_48, knots = 17, Boundary.knots = c(-18, 71)),       family = Beta()     ),     rs_for_length = bf(       mwi ~ ns(age_48, knots = c(9, 25), Boundary.knots = c(-18, 71)) +         tocs_length +         speaking_sps_3 +         speaking_sps_3:tocs_length +         (tocs_length | child),       phi ~ ns(age_48, knots = 17, Boundary.knots = c(-18, 71)),       family = Beta()     ),     rs_for_length_plus_rate = bf(       mwi ~ ns(age_48, knots = c(9, 25), Boundary.knots = c(-18, 71)) +         tocs_length +         speaking_sps_3 +         speaking_sps_3:tocs_length +         (speaking_sps_3 + tocs_length | child),       phi ~ ns(age_48, knots = 17, Boundary.knots = c(-18, 71)),       family = Beta()     ),     rs_for_length_colon_rate = bf(       mwi ~ ns(age_48, knots = c(9, 25), Boundary.knots = c(-18, 71)) +         tocs_length +         speaking_sps_3 +         speaking_sps_3:tocs_length +         (speaking_sps_3:tocs_length | child),       phi ~ ns(age_48, knots = 17, Boundary.knots = c(-18, 71)),       family = Beta()     ),     rs_for_length_colon_rate_no_intercept = bf(       mwi ~ ns(age_48, knots = c(9, 25), Boundary.knots = c(-18, 71)) +         tocs_length +         speaking_sps_3 +         speaking_sps_3:tocs_length +         (0 + speaking_sps_3:tocs_length | child),       phi ~ ns(age_48, knots = 17, Boundary.knots = c(-18, 71)),       family = Beta()     ),     rs_for_length_by_rate = bf(       mwi ~ ns(age_48, knots = c(9, 25), Boundary.knots = c(-18, 71)) +         tocs_length +         speaking_sps_3 +         speaking_sps_3:tocs_length +         (speaking_sps_3 * tocs_length | child),       phi ~ ns(age_48, knots = 17, Boundary.knots = c(-18, 71)),       family = Beta()     )   )    files <- c(     ri_main_interaction = \"models/ri_main_interaction\",     ri_no_main_length = \"models/ri_no_main_length\",     ri_no_main_interaction = \"models/ri_no_main_interaction\",     rs_for_rate = \"models/rs_for_rate\",     rs_for_length = \"models/rs_for_length\",     rs_for_length_plus_rate = \"models/rs_for_length_plus_rate\",     rs_for_length_colon_rate = \"models/rs_for_length_colon_rate\",     rs_for_length_colon_rate_no_intercept = \"models/rs_for_length_colon_rate_no_intercept\",     rs_for_length_by_rate = \"models/rs_for_length_by_rate\",     rs_monotonic_length = \"models/rs_monotonic_length\"   )    formula <- formula %||% formulas[[flavor]]   file <- file %||% files[[flavor]]     if (is.null(priors)) {     priors <- c(       set_prior(\"normal(0, 2.5)\", class = \"b\"),       set_prior(\"normal(0, 1)\", class = \"sd\")     )     has_element <- function(x, y) any(x %in% y)     has_correlation <- get_prior(formula, data = data) |>       getElement(\"class\") |>       has_element(\"cor\")     if (has_correlation) {       priors <- c(         priors,         set_prior(\"lkj(2)\", class = \"cor\")       )     }   }     t <- brm(     formula = formula,     data = data,     prior = priors,     backend = \"cmdstanr\",     seed = seed,     file = file,     file_refit = \"on_change\",     chains = 4,     cores = 4,     iter = iter,     save_pars = save_pars(all = TRUE),     control = list(adapt_delta = adapt_delta),     refresh = 50   )   add_criterion(t, c(\"loo\", \"loo_R2\")) } loo_comparison <- targets::tar_read(\"loo_comparison\") labels <- tibble::tribble(   ~model, ~random_effects,   \"model_rs_length\", \"1 + length | child\",    \"model_rs_rate\", \"1 + rate | child\",    \"model_rs_length_plus_rate\", \"1 + length + rate | child\",    \"model_main\", \"1 | child\",    \"model_rs_length_colon_rate\", \"1 + length:rate | child\",    \"model_rs_monotonic_length\", \"1 + monotonic-length | child\",  )  loo_table_data <- loo_comparison |>    inner_join(labels, by = \"model\") |>    mutate(     across(c(2:3), \\(x) round(x, 1)),     across(c(4:9), \\(x) round(x, 0)),     across(c(loo_r2_mean), \\(x) round(x, 2)),     across(c(loo_r2_sd), \\(x) round(x, 3)),   ) |>    relocate(model, random_effects)   loo_table_data |>    knitr::kable() model_rs_length <- targets::tar_read(model_rs_length)  summary(model_rs_length) #>  Family: beta  #>   Links: mu = logit; phi = log  #> Formula: mwi ~ ns(age_48, knots = c(9, 25), Boundary.knots = c(-18, 71)) + tocs_length + speaking_sps_3 + speaking_sps_3:tocs_length + (tocs_length | child)  #>          phi ~ ns(age_48, knots = 17, Boundary.knots = c(-18, 71)) #>    Data: data (Number of observations: 2196)  #>   Draws: 4 chains, each with iter = 3000; warmup = 1500; thin = 1; #>          total post-warmup draws = 6000 #>  #> Multilevel Hyperparameters: #> ~child (Number of levels: 538)  #>                                Estimate Est.Error l-95% CI u-95% CI Rhat #> sd(Intercept)                      0.80      0.04     0.71     0.88 1.01 #> sd(tocs_length4)                   0.50      0.09     0.28     0.65 1.02 #> sd(tocs_length5)                   0.38      0.11     0.11     0.55 1.02 #> sd(tocs_length6)                   0.44      0.09     0.22     0.59 1.02 #> sd(tocs_length7)                   0.54      0.09     0.34     0.68 1.02 #> cor(Intercept,tocs_length4)       -0.27      0.10    -0.43    -0.02 1.01 #> cor(Intercept,tocs_length5)       -0.45      0.10    -0.62    -0.23 1.00 #> cor(tocs_length4,tocs_length5)     0.41      0.18    -0.08     0.66 1.01 #> cor(Intercept,tocs_length6)       -0.31      0.13    -0.49    -0.01 1.00 #> cor(tocs_length4,tocs_length6)     0.57      0.13     0.26     0.77 1.00 #> cor(tocs_length5,tocs_length6)     0.53      0.19     0.01     0.77 1.01 #> cor(Intercept,tocs_length7)       -0.26      0.12    -0.44     0.03 1.01 #> cor(tocs_length4,tocs_length7)     0.54      0.12     0.25     0.73 1.00 #> cor(tocs_length5,tocs_length7)     0.45      0.18    -0.03     0.70 1.01 #> cor(tocs_length6,tocs_length7)     0.86      0.10     0.67     0.96 1.01 #>                                Bulk_ESS Tail_ESS #> sd(Intercept)                       191      280 #> sd(tocs_length4)                    120      163 #> sd(tocs_length5)                    125      181 #> sd(tocs_length6)                    136      160 #> sd(tocs_length7)                    144      163 #> cor(Intercept,tocs_length4)         290      306 #> cor(Intercept,tocs_length5)        2818      915 #> cor(tocs_length4,tocs_length5)      369      270 #> cor(Intercept,tocs_length6)         380      273 #> cor(tocs_length4,tocs_length6)      662      480 #> cor(tocs_length5,tocs_length6)      423      248 #> cor(Intercept,tocs_length7)         232      239 #> cor(tocs_length4,tocs_length7)      718      463 #> cor(tocs_length5,tocs_length7)      384      241 #> cor(tocs_length6,tocs_length7)      567      280 #>  #> Regression Coefficients: #>                                              Estimate Est.Error l-95% CI #> Intercept                                       -0.42      0.11    -0.64 #> phi_Intercept                                    3.82      0.39     3.08 #> nsage_48knotsEQc925Boundary.knotsEQcM18711       3.25      0.15     2.95 #> nsage_48knotsEQc925Boundary.knotsEQcM18712       6.80      0.28     6.26 #> nsage_48knotsEQc925Boundary.knotsEQcM18713       3.63      0.21     3.22 #> tocs_length4                                    -0.10      0.05    -0.20 #> tocs_length5                                    -0.31      0.05    -0.41 #> tocs_length6                                    -0.17      0.06    -0.28 #> tocs_length7                                    -0.51      0.06    -0.63 #> speaking_sps_3                                  -0.08      0.11    -0.31 #> tocs_length4:speaking_sps_3                     -0.05      0.13    -0.30 #> tocs_length5:speaking_sps_3                     -0.45      0.12    -0.69 #> tocs_length6:speaking_sps_3                     -0.25      0.13    -0.51 #> tocs_length7:speaking_sps_3                     -0.34      0.13    -0.59 #> phi_nsage_48knotsEQ17Boundary.knotsEQcM18711     0.13      0.67    -1.22 #> phi_nsage_48knotsEQ17Boundary.knotsEQcM18712     0.53      0.23     0.07 #>                                              u-95% CI Rhat Bulk_ESS Tail_ESS #> Intercept                                       -0.19 1.00     2038     3248 #> phi_Intercept                                    4.59 1.02      118      212 #> nsage_48knotsEQc925Boundary.knotsEQcM18711       3.55 1.00     2596     3175 #> nsage_48knotsEQc925Boundary.knotsEQcM18712       7.35 1.00     1873     3560 #> nsage_48knotsEQc925Boundary.knotsEQcM18713       4.03 1.00     3657     3703 #> tocs_length4                                    -0.01 1.00     3297     3803 #> tocs_length5                                    -0.22 1.00     3023     4015 #> tocs_length6                                    -0.06 1.00     3206     4399 #> tocs_length7                                    -0.41 1.00     2993     3682 #> speaking_sps_3                                   0.14 1.00     1971     2564 #> tocs_length4:speaking_sps_3                      0.21 1.00     3431     4386 #> tocs_length5:speaking_sps_3                     -0.21 1.00     3030     4074 #> tocs_length6:speaking_sps_3                      0.01 1.00     3197     3824 #> tocs_length7:speaking_sps_3                     -0.08 1.00     2527     3713 #> phi_nsage_48knotsEQ17Boundary.knotsEQcM18711     1.33 1.02      140      352 #> phi_nsage_48knotsEQ17Boundary.knotsEQcM18712     0.97 1.01      290      843 #>  #> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS #> and Tail_ESS are effective sample size measures, and Rhat is the potential #> scale reduction factor on split chains (at convergence, Rhat = 1)."},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2025-mahr-intel-rate.html","id":"marginal-means","dir":"Articles","previous_headings":"Beta regression model","what":"Marginal means","title":"Analysis code: 'Speech development between 30 and 119 months in typical  children III: Interaction between speaking rate and intelligibility'\n","text":"model hand, compute marginal predictions simulating new children posterior sample. basic recipe follows. Create grid data make predictions , giving child --sample value. make things easier, break data batches list. Next sample 1000 children draw. 1000 childen 1000 draws multivariate normal distribution used random effects. First extract variance-covariance matrix random effects. use rvar() allows write code 6000 x 5 x 5 array posterior draws 5 x 5 matrix. random effects centered 0 row newdata given mean 0. Given simulated children, walk batch new data, get linear predictions (predictions logits using just fixed effects), add simulated children linear prediction, convert logits proportions average 1000 children draw. part time-consuming load precomputed value. .marginal column 6000 marginal means row prediction grid. Subtracting two rows give 6000 differences. Estimates comparisons computed taking median 95% quantile intervals means differences.","code":"newdata <- tidyr::crossing(   age_48 = (3:7) * 12 - 48,   tocs_length = c(\"3\", \"4\", \"5\", \"6\", \"7\"),   speaking_sps_3 = c(0, .5),   child = \"fake\" )  newdata #> # A tibble: 50 × 4 #>    age_48 tocs_length speaking_sps_3 child #>     <dbl> <chr>                <dbl> <chr> #>  1    -12 3                      0   fake  #>  2    -12 3                      0.5 fake  #>  3    -12 4                      0   fake  #>  4    -12 4                      0.5 fake  #>  5    -12 5                      0   fake  #>  6    -12 5                      0.5 fake  #>  7    -12 6                      0   fake  #>  8    -12 6                      0.5 fake  #>  9    -12 7                      0   fake  #> 10    -12 7                      0.5 fake  #> # ℹ 40 more rows  newdata_batches <- newdata |>   split(~ age_48:speaking_sps_3) cov <- model_rs_length |>   VarCorr(summary = FALSE) |>   _$child$cov |>   posterior::rvar() cov #> rvar<6000>[5,5] mean ± sd: #>              Intercept      tocs_length4   tocs_length5   tocs_length6   #> Intercept     0.64 ± 0.071  -0.11 ± 0.054  -0.14 ± 0.055  -0.11 ± 0.056  #> tocs_length4 -0.11 ± 0.054   0.26 ± 0.084   0.09 ± 0.051   0.13 ± 0.056  #> tocs_length5 -0.14 ± 0.055   0.09 ± 0.051   0.16 ± 0.075   0.10 ± 0.052  #> tocs_length6 -0.11 ± 0.056   0.13 ± 0.056   0.10 ± 0.052   0.20 ± 0.075  #> tocs_length7 -0.12 ± 0.062   0.15 ± 0.060   0.10 ± 0.055   0.21 ± 0.070  #>              tocs_length7   #> Intercept    -0.12 ± 0.062  #> tocs_length4  0.15 ± 0.060  #> tocs_length5  0.10 ± 0.055  #> tocs_length6  0.21 ± 0.070  #> tocs_length7  0.30 ± 0.087  means <- rep(0, ncol(cov))  rows_batches <- vapply(newdata_batches, nrow, integer(1)) stopifnot(all(rows_batches == length(means)))  num_children <- 1000 sim_children <- posterior::rdo(mvtnorm::rmvnorm(num_children, means, cov)) l <- newdata_batches |>   lapply(tidybayes::add_linpred_rvars, model_rs_length, re_formula = NA) |>   lapply(     function(x) {       x$.marginal <- (sim_children + t(x$.linpred)) |>         brms::inv_logit_scaled() |>         posterior::rvar_apply(2, posterior::rvar_mean)       x     }   ) |>   bind_rows() targets::tar_read(marginal_means_model_rs_length) #> # A tibble: 50 × 6 #>    age_48 tocs_length speaking_sps_3 child        .linpred     .marginal #>     <dbl> <chr>                <dbl> <chr>      <rvar[1d]>    <rvar[1d]> #>  1    -12 3                        0 fake    0.231 ± 0.080  0.55 ± 0.018 #>  2    -12 4                        0 fake    0.130 ± 0.080  0.53 ± 0.019 #>  3    -12 5                        0 fake   -0.078 ± 0.077  0.48 ± 0.019 #>  4    -12 6                        0 fake    0.061 ± 0.082  0.51 ± 0.020 #>  5    -12 7                        0 fake   -0.283 ± 0.084  0.43 ± 0.020 #>  6      0 3                        0 fake    1.448 ± 0.065  0.78 ± 0.011 #>  7      0 4                        0 fake    1.347 ± 0.063  0.78 ± 0.010 #>  8      0 5                        0 fake    1.138 ± 0.058  0.75 ± 0.011 #>  9      0 6                        0 fake    1.278 ± 0.064  0.77 ± 0.011 #> 10      0 7                        0 fake    0.933 ± 0.066  0.71 ± 0.013 #> # ℹ 40 more rows"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2025-mahr-intel-rate.html","id":"logistic-regression-model","dir":"Articles","previous_headings":"","what":"Logistic regression model","title":"Analysis code: 'Speech development between 30 and 119 months in typical  children III: Interaction between speaking rate and intelligibility'\n","text":"within-child effects rate, use item-level data 5-, 6-, 7-word utterances. New item: item identifier n_words_correct, n_words_total: number words correctly transcribed listeners number words total, intelligibility proportion words correctly transcribed child_mean_rate: child’s average speaking rate items fitted model following: random effects include -child random intercept, -child rate effects, -item intercepts. age-spline used fixed effects. two predictors speaking rate given utterance child’s average speaking rate. child_mean_rate meant absorb child-level reflect child’s habitual rate speaking_sps_3 can estimate within child effects rate. can extract posterior means child’s rate coefficient like : plotted lines used tidybayes::add_epred_rvars() get child-level predictions trials set 1. take grid 50 points along child’s observed speaking rate, get posterior expectations point plot mean expectations child.","code":"data_model_by_item_anon <- targets::tar_read(data_model_by_item_anon) data_model_by_item_anon #> # A tibble: 11,282 × 18 #>    child item  speaking_sps artic_sps age_months tocs_level length_longest #>    <chr> <chr>        <dbl>     <dbl>      <dbl>      <dbl>          <dbl> #>  1 c130  S5T01         2.47      2.47         34          5              5 #>  2 c130  S5T02         2.26      2.26         34          5              5 #>  3 c130  S5T03         2.55      2.55         34          5              5 #>  4 c130  S5T04         2.77      2.77         34          5              5 #>  5 c130  S5T05         3.01      3.01         34          5              5 #>  6 c130  S5T06         1.89      2.12         34          5              5 #>  7 c130  S5T07         2.39      2.39         34          5              5 #>  8 c130  S5T08         2.20      2.50         34          5              5 #>  9 c130  S5T09         2.83      2.83         34          5              5 #> 10 c227  S5T01         3         3            35          5              5 #> # ℹ 11,272 more rows #> # ℹ 11 more variables: age_bin <dbl>, intelligibility <dbl>, #> #   n_words_correct <dbl>, n_words_total <dbl>, mwi <dbl>, #> #   speaking_sps_3 <dbl>, artic_sps_3 <dbl>, age_48 <dbl>, tocs_length <fct>, #> #   tocs_level_3 <dbl>, child_mean_rate <dbl> model_logistic <- targets::tar_read(   \"model_rs_rate_ri_item_no_length_binom_mundlak\" ) model_logistic #>  Family: binomial  #>   Links: mu = logit  #> Formula: n_words_correct | trials(n_words_total) ~ ns(age_48, knots = c(9, 25), Boundary.knots = c(-18, 71)) + speaking_sps_3 + child_mean_rate + (speaking_sps_3 | child) + (1 | item)  #>    Data: data (Number of observations: 11282)  #>   Draws: 4 chains, each with iter = 3000; warmup = 1500; thin = 1; #>          total post-warmup draws = 6000 #>  #> Multilevel Hyperparameters: #> ~child (Number of levels: 422)  #>                               Estimate Est.Error l-95% CI u-95% CI Rhat #> sd(Intercept)                     0.85      0.03     0.78     0.92 1.00 #> sd(speaking_sps_3)                0.50      0.03     0.44     0.56 1.00 #> cor(Intercept,speaking_sps_3)    -0.15      0.07    -0.28    -0.01 1.00 #>                               Bulk_ESS Tail_ESS #> sd(Intercept)                     1367     2649 #> sd(speaking_sps_3)                2518     3793 #> cor(Intercept,speaking_sps_3)     1917     2597 #>  #> ~item (Number of levels: 29)  #>               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS #> sd(Intercept)     0.71      0.09     0.55     0.92 1.00     1108     2461 #>  #> Regression Coefficients: #>                                            Estimate Est.Error l-95% CI u-95% CI #> Intercept                                      0.49      0.33    -0.16     1.14 #> nsage_48knotsEQc925Boundary.knotsEQcM18711     2.93      0.20     2.52     3.33 #> nsage_48knotsEQc925Boundary.knotsEQcM18712     4.72      0.66     3.42     6.02 #> nsage_48knotsEQc925Boundary.knotsEQcM18713     3.30      0.27     2.74     3.83 #> speaking_sps_3                                -0.23      0.04    -0.30    -0.16 #> child_mean_rate                               -0.12      0.15    -0.42     0.17 #>                                            Rhat Bulk_ESS Tail_ESS #> Intercept                                  1.00      879     1846 #> nsage_48knotsEQc925Boundary.knotsEQcM18711 1.00     1064     1742 #> nsage_48knotsEQc925Boundary.knotsEQcM18712 1.00     1098     1950 #> nsage_48knotsEQc925Boundary.knotsEQcM18713 1.00     1810     2818 #> speaking_sps_3                             1.00     4260     4387 #> child_mean_rate                            1.00     1031     2004 #>  #> Draws were sampled using sample(hmc). For each parameter, Bulk_ESS #> and Tail_ESS are effective sample size measures, and Rhat is the potential #> scale reduction factor on split chains (at convergence, Rhat = 1). # Posterior draws of the slopes child_slopes <- model_logistic |>   coef(summary = FALSE) |>   getElement(\"child\") |>   _[, , c(\"Intercept\", \"speaking_sps_3\")] |>    posterior::rvar()  slope_means <- mean(child_slopes)[, \"speaking_sps_3\"]  f_percent <- scales::label_percent(.1) lslopes <- list(   n_positive = sum(slope_means > 0),   n_negative = sum(slope_means < 0),   pct_positive = f_percent(sum(slope_means > 0) / length(slope_means)),   pct_negative = f_percent(sum(slope_means < 0) / length(slope_means)) ) str(lslopes) #> List of 4 #>  $ n_positive  : int 107 #>  $ n_negative  : int 315 #>  $ pct_positive: chr \"25.4%\" #>  $ pct_negative: chr \"74.6%\" data_binom_rate_grid <- data_model_by_item_anon |>   group_by(child, age_months, age_bin, age_48) |>   reframe(     child_mean_rate = mean(speaking_sps_3),     speaking_sps_3 = c(       seq(min(speaking_sps_3), max(speaking_sps_3), length.out = 50),       child_mean_rate     ),     item = \"fake\",     n_words_total = 1   )  data_binom_rate_grid <- tidybayes::add_epred_rvars(   data_binom_rate_grid,   model_logistic,   allow_new_levels = TRUE )"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/sm-2025-mahr-intel-rate.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Analysis code: 'Speech development between 30 and 119 months in typical  children III: Interaction between speaking rate and intelligibility'\n","text":"Bürkner, P.-C. (2017). brms: R package Bayesian multilevel models using Stan. Journal Statistical Software, 80(1), 1–28. doi:[10.18637/jss.v080.i01](https://doi.org/10.18637/jss.v080.i01) Bürkner, P.-C., & Charpentier, E. (2020). Modelling monotonic effects ordinal predictors Bayesian regression models. British Journal Mathematical Statistical Psychology, 73(3), 420–451. doi:[10.1111/bmsp.12195](https://doi.org/10.1111/bmsp.12195) Gabry, J., Češnovar, R., & Johnson, . (2024). cmdstanr: R interface CmdStan. Retrieved https://mc-stan.org/cmdstanr/ R Core Team. (2024). R: language environment statistical computing. Vienna, Austria: R Foundation Statistical Computing. Retrieved https://www.R-project.org/ Stan Development Team. (2024). Stan modeling language users guide reference manual. Retrieved https://mc-stan.org Vehtari, ., Gelman, ., & Gabry, J. (2017). Practical Bayesian model evaluation using leave-one-cross-validation WAIC. Statistics Computing, 27, 1413–1432. doi:[10.1007/s11222-016-9696-4](https://doi.org/10.1007/s11222-016-9696-4)","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/articles/utterance-length-imputation.html","id":"background","dir":"Articles","previous_headings":"","what":"Background","title":"Utterance length imputation and weighting","text":"Hustad colleagues (2020), modeled intelligibility data young children’s speech. Children hear utterance repeat . utterances started 2 words length, increased 3 words length, batches 10 sentences, way 7 words length. problem, however: children produce utterances every length. Specifically, child reliably produced 5 utterances given length length, task halted. given nature task, child produced 5-word utterances, also produced 2–4-word utterances well. , modeled/re-simulated version dataset, observe number children per utterance length decreases: length utterance plausibly influenced outcome variable: Longer utterances words might help listener understand sentence, example. Therefore, seem appropriate ignore missing values. used following two-step procedure (see Supplemental Materials detail): impute missing values utterance length using values shorter lengths, imputation stages, imputed value length L can used predictor L + 1. weight utterance length probability produced child given age take weighted average outcome variable across length. goal data preparation produce single-number intelligibility score, final weighted average provides number. procedure, missing data ignored implausible data (like longest utterances youngest ages) downweighted.","code":"library(tidyverse)  data_demo |>    count(tocs_level) #> # A tibble: 7 × 2 #>   tocs_level     n #>        <int> <int> #> 1          1   164 #> 2          2   164 #> 3          3   162 #> 4          4   102 #> 5          5    47 #> 6          6    30 #> 7          7    24"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/utterance-length-imputation.html","id":"imputation","dir":"Articles","previous_headings":"","what":"Imputation","title":"Utterance length imputation and weighting","text":"flow code consists following steps: reshape data wide format, train series linear models trained observed data, predict responses missing values reshape back long format recent non-package version function: implementation unsatisfactory number reasons. hardcodes many unneeded variables, apply example dataset, add filler values variables. also hardcodes number models predictors imputation model. y_1 lm() indicates scores single-word trials used imputation. inclusion may may appropriate. speaking rate studies, doesn’t make sense use include data single-word trials. , caveats aside, works: can recreate Figure 3 supplemental materials: Results imputing multiword intelligibility using length--longest utterance average intelligibilities shorter utterance lengths. comparison, package version procedure. specify relevant variables ahead time, number models variables involved longer hard-coded. original models, length longest utterance used continuous predictor imputations. package version behavior optional disabled default. affect results noticeably.","code":"impute_values <- function(data, var, data_train = NULL) {   spec <- build_wider_spec_for_imputation(data, {{ var }})   data_wide <- pivot_wider_for_imputation(data, spec)    if (is.null(data_train)) {     data_train <- data   }    data_wide_train <- pivot_wider_for_imputation(data_train, spec)   models <- fit_imputation_models(data_wide_train)    data_imputed <- data_wide |>     mutate(       y_3 = ifelse(is.na(y_3), predict(models$m_3, pick(everything())), y_3)     ) |>     mutate(       y_4 = ifelse(is.na(y_4), predict(models$m_4, pick(everything())), y_4)     ) |>     mutate(       y_5 = ifelse(is.na(y_5), predict(models$m_5, pick(everything())), y_5)     ) |>     mutate(       y_6 = ifelse(is.na(y_6), predict(models$m_6, pick(everything())), y_6)     ) |>     mutate(       y_7 = ifelse(is.na(y_7), predict(models$m_7, pick(everything())), y_7)     )    # handle these separately because `length_longest` could have been imputed if   # there was a different strategy   data_child_ll <- data_wide |>     distinct(group, subject_num, visit_id, age_months, length_longest)    data_original_values <- data_wide |>     tidyr::pivot_longer_spec(spec) |>     distinct(group, subject_num, visit_id, tocs_level, {{ var }})    d <- data_imputed |>     tidyr::pivot_longer_spec(spec) |>     rename(\"imputed_{{ var }}\" := {{ var }}) |>     select(-length_longest) |>     left_join(       data_child_ll,       by = c(\"group\", \"subject_num\", \"visit_id\", \"age_months\")     ) |>     left_join(       data_original_values,       by = c(\"group\", \"subject_num\", \"visit_id\", \"tocs_level\")     ) |>     mutate(       imputed = ifelse(is.na({{ var }}), \"imputed\", \"observed\"),       facet_lab = paste0(tocs_level, \" words\")     )    d }  fit_imputation_models <- function(data) {   list(     m_7 = lm(y_7 ~ y_1 + y_2 + y_3 + y_4 + y_5 + y_6, data),     m_6 = lm(y_6 ~ y_1 + y_2 + y_3 + y_4 + y_5 + length_longest, data),     m_5 = lm(y_5 ~ y_1 + y_2 + y_3 + y_4 + length_longest, data),     m_4 = lm(y_4 ~ y_1 + y_2 + y_3 + length_longest, data),     m_3 = lm(y_3 ~ y_1 + y_2 + length_longest, data)   ) }  build_wider_spec_for_imputation <- function(data, var) {   tidyr::build_wider_spec(     data,     names_from = tocs_level,     names_prefix = \"y_\",     values_from = {{ var }}   ) }  pivot_wider_for_imputation <- function(data, spec) {   data |>     tidyr::pivot_wider_spec(       spec,       id_cols = c(         group, subject_num, visit_id, length_longest, age_months       )     ) } data_imputation_1 <- data_demo |>   mutate(group = \"fake\", visit_id = 1) |>   rename(subject_num = child) |>    impute_values(sim_intelligibility) plotting_constants <- list(   pal = c(     imputed = \"#C7A76C\",      observed = \"#7DB0DD\",      \"mean ± SE\" = \"grey30\"   ),   guides_pal = guides(     shape = \"none\",     color = guide_legend(       title = NULL,       override.aes = list(         alpha = 1,         shape = c(16, 17, 16),         linetype = c(\"blank\", \"blank\", \"solid\")       )     )   ) )   plotting_constants$scale_pal <- scale_color_manual(   values = plotting_constants$pal,    limits = names(plotting_constants$pal) )  set.seed(100) ggplot(data_imputation_1 |> filter(tocs_level != 1)) +    aes(     x = length_longest,      y = imputed_sim_intelligibility,      color = imputed,      shape = imputed   ) +    geom_jitter(width = .3, alpha = .5, height = 0) +   stat_summary(     aes(group = length_longest, color = \"mean ± SE\"),      fun.data = mean_se   ) +    plotting_constants$scale_pal +    plotting_constants$guides_pal +   facet_wrap(\"facet_lab\") +    scale_y_continuous(     \"Intelligibility\",      labels = scales::percent_format(1)   ) +   labs(     x = \"Length of longest utterance (observed)\"   ) +    theme(legend.position = \"bottom\", legend.justification = \"right\") data_imputation_2 <- data_demo |>    impute_values_by_length(     var_y = sim_intelligibility,     var_length = tocs_level,     id_cols = c(child, age_months, length_longest),      include_max_length = TRUE   )  all.equal(   data_imputation_1$imputed_sim_intelligibility,    data_imputation_2$sim_intelligibility_imputed ) #> [1] TRUE data_imputation_3 <- data_demo |>    impute_values_by_length(     var_y = sim_intelligibility,     var_length = tocs_level,     id_cols = c(child, age_months, length_longest),      include_max_length = FALSE   )  all.equal(   data_imputation_2$sim_intelligibility_imputed,    data_imputation_3$sim_intelligibility_imputed ) #> [1] \"Mean relative difference: 8.473438e-05\"  cor(   data_imputation_2$sim_intelligibility_imputed,    data_imputation_3$sim_intelligibility_imputed ) #> [1] 1  ggplot(data_imputation_2 |> filter(tocs_level != 1)) +    aes(     x = length_longest,      y = sim_intelligibility_imputed,      color = sim_intelligibility_imputation,      shape = sim_intelligibility_imputation   ) +    geom_jitter(width = .3, alpha = .5, height = 0) +   stat_summary(     aes(group = length_longest, color = \"mean ± SE\"),      fun.data = mean_se   ) +    plotting_constants$scale_pal +    plotting_constants$guides_pal +   facet_wrap(\"tocs_level\") +    scale_y_continuous(     \"Intelligibility\",      labels = scales::percent_format(1)   ) +   labs(     x = \"Length of longest utterance (observed)\"   ) +    theme(legend.position = \"bottom\", legend.justification = \"right\")"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/utterance-length-imputation.html","id":"limit-the-observations-used-to-train-the-imputation-model","dir":"Articles","previous_headings":"Imputation","what":"Limit the observations used to train the imputation model","title":"Utterance length imputation and weighting","text":"data_train argument allows us specify dataset train imputation model. data_train can also function applied original data set undergoing imputation.","code":"data_imputation_trained <- data_demo |>    impute_values_by_length(     var_y = sim_intelligibility,     var_length = tocs_level,     id_cols = c(child, age_months, length_longest),      include_max_length = TRUE,     # suppose we wanted to impute using older participants     data_train = data_demo |> filter(age_months > 44)   )  data_imputation_trained_2 <- data_demo |>    impute_values_by_length(     var_y = sim_intelligibility,     var_length = tocs_level,     id_cols = c(child, age_months, length_longest),      include_max_length = TRUE,     # suppose we wanted to impute using older participants     data_train = function(x) x |> filter(age_months > 44)   )  all.equal(   data_imputation_trained$sim_intelligibility_imputed,    data_imputation_trained_2$sim_intelligibility_imputed ) #> [1] TRUE"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/utterance-length-imputation.html","id":"utterance-weighting","dir":"Articles","previous_headings":"","what":"Utterance weighting","title":"Utterance length imputation and weighting","text":"recent non-package version utterance length weighting code. basic steps preparing dataset fitting ordinal regression model length longest utterance using age predictor computing probabilities utterance length function age Originally, used MASS::polr() models think ordinal::clm() easier use. can apply non-package implementation example data: can recreate Figure supplemental materials:  implementation somewhat unsatisfactory. hardcodes variable names, ’s harder reuse, complicated use, requiring three functions join. package implementation, hide everything behind single function. can see match weights two implementations.","code":"prepare_longest_length_model_data <- function(data) {   data |>     group_by(child) |>     mutate(       length_longest = max(tocs_level)     ) |>     ungroup() |>     distinct(child, tocs_level, age_months, length_longest) |>     filter(tocs_level == length_longest) |>     mutate(       lol = factor(length_longest),       lol_ord = ordered(lol)     ) }  fit_longest_length_model <- function(data, df = 2) {   ordinal::clm(     lol_ord ~ splines::ns(age_months, df = df),     data = data   ) }  compute_longest_length_weights <- function(data, model) {   d_ages <- data |>     distinct(age_months) |>     mutate(       # predicted longest length by age       lol_predicted = model |>         predict(newdata = pick(everything()), type = \"class\") |>         getElement(\"fit\")     )    d_ages |>     predict(model, newdata = _, type = \"prob\") |>     getElement(\"fit\") |>     bind_cols(d_ages) |>     tidyr::pivot_longer(       cols = -c(age_months, lol_predicted),       names_to = \"lol\",       values_to = \"lol_prob\"     ) |>     group_by(age_months) |>     mutate(       lol_num = as.numeric(as.character(lol)),       lol_weighted = lol_prob * lol_num     ) |>     arrange(-lol_num) |>     mutate(       prob_reach_length = cumsum(lol_prob),       normalized_prob_reach_length = prob_reach_length / sum(prob_reach_length)     ) |>     ungroup() } data_lol <- data_demo |>    filter(tocs_level != 1) |>    prepare_longest_length_model_data()  m <- fit_longest_length_model(data_lol, df = 2)  data_lol_weights <- data_lol |>    compute_longest_length_weights(model = m) |>   distinct(     age_months,      tocs_level = lol_num,      prob_reach_length,      normalized_prob_reach_length   ) p1 <- ggplot(data_lol_weights) +    aes(x = age_months, y = prob_reach_length  ) +    geom_line(aes(color = ordered(tocs_level))) +   geom_text(     aes(label = tocs_level, color = ordered(tocs_level)),     x = 29,     data = data_lol_weights |>        filter(age_months == 30, tocs_level %in% c(2, 3, 4, 7)),      fontface = \"bold\"   ) +   geom_text(     aes(label = tocs_level, color = ordered(tocs_level)),     x = 48,     data = data_lol_weights |>        filter(age_months == 47, tocs_level %in% c(2, 5, 6, 7)),      fontface = \"bold\"   ) +   scale_color_ordinal(end = .85) +   guides(color = \"none\") +    labs(x = \"Age [months]\", y = \"Prob. of reaching length\") +   scale_x_continuous(limits = c(28, 49)) p1  p2 <- p1 +    aes(y = normalized_prob_reach_length) +    labs(y = \"Weight of utterance length\") p2 data_plot <- weight_lengths_with_ordinal_model(   data_train = data_demo |>      filter(tocs_level != 1),    var_length = tocs_level,   var_x = age_months,    id_cols = c(child),    spline_df = 2 ) all.equal(   data_lol_weights$normalized_prob_reach_length,    data_plot$tocs_level_weight ) #> [1] TRUE  all.equal(   data_lol_weights$prob_reach_length,    data_plot$tocs_level_prob_reached ) #> [1] TRUE"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/utterance-length-imputation.html","id":"combining-the-imputation-and-the-weights","dir":"Articles","previous_headings":"","what":"Combining the imputation and the weights","title":"Utterance length imputation and weighting","text":"last step process combine imputed values length weights, take means imputed values. step straightforward table join. Although used single-word scores imputation (highly correlated intelligibility scores), use weighting. target variable single-number summary intelligibility 2–7-word utterances, exclude 1-word scores. can automate joining step. weight_lengths_with_ordinal_model() takes optional data_join argument specifying dataframe estimated weights joined onto. argument intended dataframe imputed values. pass imputed values data_train missing values imputed, everyone length longest utterance. can see results methods. three values overall similar:  importantly, imputation negatively biased younger children reach longer utterance lengths. Points fall diagonal two types scores compared, difference two scores negative younger ages.   weight utterance length probability, score tighter correlation observed, un-adjusted scores. pointwise differences weighted observed values small, ±2 percentage points intelligibility.","code":"data_weighted_1 <- data_imputation_1 |>    filter(tocs_level != 1) |>    left_join(data_lol_weights, by = join_by(age_months, tocs_level))   data_means_wide_1 <- data_weighted_1 |>    group_by(subject_num, age_months) |>    summarise(     observed = mean(sim_intelligibility, na.rm = TRUE),     imputed = mean(imputed_sim_intelligibility),     weighted = weighted.mean(       imputed_sim_intelligibility,        normalized_prob_reach_length     ),     .groups = \"drop\"   )  data_means_wide_1 #> # A tibble: 164 × 5 #>    subject_num age_months observed imputed weighted #>    <chr>            <int>    <dbl>   <dbl>    <dbl> #>  1 c001                45    0.533   0.530    0.531 #>  2 c002                42    0.389   0.337    0.360 #>  3 c003                41    0.544   0.532    0.540 #>  4 c004                42    0.660   0.660    0.655 #>  5 c005                43    0.707   0.725    0.710 #>  6 c006                42    0.524   0.513    0.523 #>  7 c007                32    0.355   0.307    0.350 #>  8 c008                43    0.828   0.828    0.832 #>  9 c009                38    0.868   0.873    0.869 #> 10 c010                47    0.767   0.767    0.765 #> # ℹ 154 more rows data_weighted_2 <- weight_lengths_with_ordinal_model(   data_train = data_demo |> filter(tocs_level != 1),    var_length = tocs_level,   var_x = age_months,    id_cols = c(child),    spline_df = 2,   data_join = data_imputation_2 |> filter(tocs_level != 1) )  data_means_wide_2 <- data_weighted_2 |>    group_by(child, age_months) |>    summarise(     observed = mean(sim_intelligibility, na.rm = TRUE),     imputed = mean(sim_intelligibility_imputed),     weighted = weighted.mean(sim_intelligibility_imputed, tocs_level_weight),     .groups = \"drop\"   ) all.equal(data_means_wide_1$observed, data_means_wide_2$observed) #> [1] TRUE all.equal(data_means_wide_1$imputed,  data_means_wide_2$imputed) #> [1] TRUE all.equal(data_means_wide_1$weighted, data_means_wide_2$weighted) #> [1] TRUE data_means <- data_means_wide_2 |>    tidyr::pivot_longer(     cols = c(-child, -age_months),     names_to = \"score\",     values_to = \"intelligibility\"   )  ggplot(data_means) +    aes(x = age_months, y = intelligibility, color = score) +    geom_point(alpha = .4) ggplot(data_means_wide_2) +    aes(x = observed, y = imputed) +   geom_abline() +   geom_point() ggplot(data_means_wide_2) +    aes(x = age_months, y = imputed - observed) +   geom_hline(yintercept = 0, linewidth = 2, color = \"white\") +   geom_point() ggplot(data_means_wide_2) +    aes(x = observed, y = weighted) +   geom_abline() +   geom_point() ggplot(data_means_wide_2) +    aes(x = age_months, y = weighted - observed) +   geom_hline(yintercept = 0, linewidth = 2, color = \"white\") +   geom_point()"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/utterance-length-imputation.html","id":"using-a-function-for-data_train","dir":"Articles","previous_headings":"Combining the imputation and the weights","what":"Using a function for data_train","title":"Utterance length imputation and weighting","text":"weight_lengths_with_ordinal_model() also supports providing function data_train. case, function applied data_join. train weights observed values imputation dataframe.","code":"data_weighted_3 <- weight_lengths_with_ordinal_model(   data_train = function(x) x |>      filter(sim_intelligibility_imputation == \"observed\"),    var_length = tocs_level,   var_x = age_months,    id_cols = c(child),    spline_df = 2,   data_join = data_imputation_2 |> filter(tocs_level != 1) )  all.equal(   data_weighted_2$tocs_level_prob_reached,    data_weighted_3$tocs_level_prob_reached ) #> [1] TRUE"},{"path":"https://www.tjmahr.com/wisclabmisc/articles/utterance-length-imputation.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Utterance length imputation and weighting","text":"Hustad, K. C., Mahr, T., Natzke, P. E. M., & Rathouz, P. J. (2020). Development Speech Intelligibility 30 47 Months Typically Developing Children: Cross-Sectional Study Growth. Journal Speech, Language, Hearing Research, 63(6), 1675–1687. https://doi.org/10.1044/2020_JSLHR-20-00008 Hustad, K. C., Mahr, T., Natzke, P. E. M., & J. Rathouz, P. (2020). Supplemental Material S1 (Hustad et al., 2020). ASHA journals. https://doi.org/10.23641/asha.12330956.v1","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Tristan Mahr. Author, maintainer.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Mahr T (2025). wisclabmisc: Tools Support 'WiscLab'. R package version 0.1.1, https://www.tjmahr.com/wisclabmisc/, https://github.com/tjmahr/wisclabmisc.","code":"@Manual{,   title = {wisclabmisc: Tools to Support the 'WiscLab'},   author = {Tristan Mahr},   year = {2025},   note = {R package version 0.1.1,     https://www.tjmahr.com/wisclabmisc/},   url = {https://github.com/tjmahr/wisclabmisc}, }"},{"path":"https://www.tjmahr.com/wisclabmisc/index.html","id":"wisclabmisc","dir":"","previous_headings":"","what":"Tools to Support the WiscLab","title":"Tools to Support the WiscLab","text":"goal wisclabmisc reuse analysis functions across WISC Lab project projects provide share analysis code used projects.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Tools to Support the WiscLab","text":"can install development version wisclabmisc GitHub :","code":"# install.packages(\"devtools\") devtools::install_github(\"tjmahr/wisclabmisc\")"},{"path":"https://www.tjmahr.com/wisclabmisc/index.html","id":"acknowledgments","dir":"","previous_headings":"","what":"Acknowledgments","title":"Tools to Support the WiscLab","text":"wisclabmisc created process data WISC Lab project. Thus, development package supported NIH R01DC009411 NIH R01DC015653.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert age in months to years;months — format_year_month_age","title":"Convert age in months to years;months — format_year_month_age","text":"Convert age months years;months","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert age in months to years;months — format_year_month_age","text":"","code":"format_year_month_age(x)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert age in months to years;months — format_year_month_age","text":"x vector ages months","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert age in months to years;months — format_year_month_age","text":"ages years;months format","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Convert age in months to years;months — format_year_month_age","text":"Ages NA return \"NA;NA\". format default numerically ordered. means c(\"2;0\", \"10;10\", \"10;9\") sort c(\"10;10\", \"10;9\", \"2;0\"). function stringr::str_sort(..., numeric = TRUE) sort vector correctly.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/ages.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert age in months to years;months — format_year_month_age","text":"","code":"ages <- c(26, 58, 25, 67, 21, 59, 36, 43, 27, 49) format_year_month_age(ages) #>  [1] \"2;2\"  \"4;10\" \"2;1\"  \"5;7\"  \"1;9\"  \"4;11\" \"3;0\"  \"3;7\"  \"2;3\"  \"4;1\""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"function fits type GAMLSS model used Hustad colleagues (2021): beta regression model (via gamlss.dist::()) natural cubic splines mean (mu) scale (sigma). model fitted using package's mem_gamlss() wrapper function.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"","code":"fit_beta_gamlss(data, var_x, var_y, df_mu = 3, df_sigma = 2, control = NULL)  fit_beta_gamlss_se(   data,   name_x,   name_y,   df_mu = 3,   df_sigma = 2,   control = NULL )  predict_beta_gamlss(newdata, model, centiles = c(5, 10, 50, 90, 95))  optimize_beta_gamlss_slope(   model,   centiles = 50,   interval = c(30, 119),   maximum = TRUE )  uniroot_beta_gamlss(model, centiles = 50, targets = 0.5, interval = c(30, 119))"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"Associated article: https://doi.org/10.1044/2021_JSLHR-21-00142","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"data data frame var_x, var_y (unquoted) variable names giving predictor variable (e.g., age) outcome variable (.e.g, intelligibility). df_mu, df_sigma degrees freedom. 0 used, splines::ns() term dropped model formula parameter. control gamlss::gamlss.control() controller. Defaults NULL uses default settings, except setting trace FALSE silence output gamlss. name_x, name_y quoted variable names giving predictor variable (e.g., \"age\") outcome variable (.e.g, \"intelligibility\"). arguments apply fit_beta_gamlss_se(). newdata one-column dataframe predictions model model fitted fit_beta_gamlss() centiles centiles use prediction. Defaults c(5, 10, 50, 90, 95) predict_beta_gamlss(). Defaults 50 optimize_beta_gamlss_slope() uniroot_beta_gamlss(), although functions support multiple centile values. interval optimize_beta_gamlss_slope(), range x values optimize . uniroot_beta_gamlss(), range x values search roots (target y values) . maximum optimize_beta_gamlss_slope(), whether find maximum slope (TRUE) minimum slope (FALSE). targets uniroot_beta_gamlss(), target y values use roots. default, .5 used, uniroot_beta_gamlss() returns x value y value .5. Multiple targets supported.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"fit_beta_gamlss() fit_beta_gamlss_se(), mem_gamlss()-fitted model. .user data model includes degrees freedom parameter splines::ns() basis parameter. predict_beta_gamlss(), dataframe containing model predictions mu sigma, plus columns centile centiles. optimize_beta_gamlss_slope(), dataframe optimized x values (maximum minimum), gradient x value (objective), quantile (quantile). uniroot_beta_gamlss(), dataframe one row per quantile/target combination results calling stats::uniroot(). root column x value quantile curve crosses target value.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"two versions function. main version fit_beta_gamlss(), works unquoted column names (e.g., age). alternative version fit_beta_gamlss_se(); final \"se\" stands \"Standard Evaluation\". designation means variable names must given strings (, quoted \"age\" instead bare name age). alternative version necessary fit several models using parallel computing furrr::future_map() (using bootstrap resampling). predict_centiles() work function, likely throw warning message. Therefore, predict_beta_gamlss() provides alternative way compute centiles model. function manually computes centiles instead relying gamlss::centiles(). main difference new x values go splines::predict.ns() multiplied model coefficients. optimize_beta_gamlss_slope() computes point (.e., age) rate steepest growth different quantiles. function wraps following process: internal prediction function computes quantile x model coefficients spline bases. another internal function uses numDeriv::grad() get gradient prediction function x. optimize_beta_gamlss_slope() uses stats::optimize() gradient function find x maximum minimum slope. uniroot_beta_gamlss() also uses internal prediction function find quantile growth curve crosses given value. stats::uniroot() finds function crosses 0 (root). modify prediction function always subtract .5 end, root prediction function x value predicted value crosses .5. idea behind uniroot_beta_gamlss() works. work, use approach find, say, age (root) children 10th percentile (centiles) cross 50% intelligibility (targets).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"gamlss-does-beta-regression-differently","dir":"Reference","previous_headings":"","what":"GAMLSS does beta regression differently","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"part brief note GAMLSS uses different parameterization beta distribution beta family packages. canonical parameterization beta distribution uses shape parameters \\(\\alpha\\) \\(\\beta\\) probability density function: $$f(y;\\alpha,\\beta) = \\frac{1}{B(\\alpha,\\beta)} y^{\\alpha-1}(1-y)^{\\beta-1}$$ \\(B\\) beta function. beta regression, distribution reparameterized mean probability \\(\\mu\\) parameter represents spread around mean. GAMLSS (gamlss.dist::()), use scale parameter \\(\\sigma\\) (larger values mean spread around mean). Everywhere else—betareg::betareg() rstanarm::stan_betareg() vignette(\"betareg\", \"betareg\"), brms::Beta() vignette(\"brms_families\", \"brms\"), mgcv::betar()—precision parameter \\(\\phi\\) (larger values mean precision, less spread around mean). comparison: $$  \\text{betareg, brms, mgcv, etc.} \\\\  \\mu              = \\alpha / (\\alpha + \\beta) \\\\  \\phi             = \\alpha + b \\\\  \\textsf{E}(y)    = \\mu \\\\  \\textsf{VAR}(y)  = \\mu(1-\\mu)/(1 + \\phi) \\\\ $$ $$  \\text{GAMLSS} \\\\  \\mu             = \\alpha / (\\alpha + \\beta) \\\\  \\sigma          = (1 / (\\alpha + \\beta + 1))^.5 \\\\  \\textsf{E}(y)   = \\mu \\\\  \\textsf{VAR}(y) = \\mu(1-\\mu)\\sigma^2 $$","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/beta-intelligibility.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a beta regression model (for intelligibility) — fit_beta_gamlss","text":"","code":"data_fake_intelligibility #> # A tibble: 200 × 2 #>    age_months intelligibility #>         <int>           <dbl> #>  1         28           0.539 #>  2         29           0.375 #>  3         31           0.221 #>  4         31           0.253 #>  5         32           0.276 #>  6         32           0.750 #>  7         32           0.820 #>  8         33           0.325 #>  9         33           0.446 #> 10         33           0.592 #> # ℹ 190 more rows  m <- fit_beta_gamlss(   data_fake_intelligibility,   age_months,   intelligibility )  # using \"qr\" in summary() just to suppress a warning message summary(m, type = \"qr\") #> ****************************************************************** #> Family:  c(\"BE\", \"Beta\")  #>  #> Call:  gamlss::gamlss(formula = intelligibility ~ ns(age_months, df = 3),   #>     sigma.formula = ~ns(age_months, df = 2), family = BE(), data = ~data_fake_intelligibility,   #>     control = list(c.crit = 0.001, n.cyc = 20, mu.step = 1, sigma.step = 1,   #>         nu.step = 1, tau.step = 1, gd.tol = Inf, iter = 0, trace = FALSE,   #>         autostep = TRUE, save = TRUE))  #>  #> Fitting method: RS()  #>  #> ------------------------------------------------------------------ #> Mu link function:  logit #> Mu Coefficients: #>                         Estimate Std. Error t value Pr(>|t|)     #> (Intercept)              -0.3414     0.2090  -1.634    0.104     #> ns(age_months, df = 3)1   2.4951     0.1896  13.162   <2e-16 *** #> ns(age_months, df = 3)2   5.0171     0.4960  10.116   <2e-16 *** #> ns(age_months, df = 3)3   3.1454     0.1746  18.017   <2e-16 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ------------------------------------------------------------------ #> Sigma link function:  logit #> Sigma Coefficients: #>                         Estimate Std. Error t value Pr(>|t|)     #> (Intercept)              -0.4380     0.1935  -2.264   0.0247 *   #> ns(age_months, df = 2)1  -1.8670     0.4175  -4.472 1.31e-05 *** #> ns(age_months, df = 2)2  -1.6193     0.2120  -7.639 9.29e-13 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ------------------------------------------------------------------ #> No. of observations in the fit:  200  #> Degrees of Freedom for the fit:  7 #>       Residual Deg. of Freedom:  193  #>                       at cycle:  8  #>   #> Global Deviance:     -510.3592  #>             AIC:     -496.3592  #>             SBC:     -473.271  #> ******************************************************************  # Alternative interface d <- data_fake_intelligibility m2 <- fit_beta_gamlss_se(   data = d,   name_x = \"age_months\",   name_y = \"intelligibility\" ) coef(m2) == coef(m) #>             (Intercept) ns(age_months, df = 3)1 ns(age_months, df = 3)2  #>                    TRUE                    TRUE                    TRUE  #> ns(age_months, df = 3)3  #>                    TRUE   # how to use control to change gamlss() behavior m_traced <- fit_beta_gamlss(   data_fake_intelligibility,   age_months,   intelligibility,   control = gamlss::gamlss.control(n.cyc = 15, trace = TRUE) ) #> GAMLSS-RS iteration 1: Global Deviance = -394.1232  #> GAMLSS-RS iteration 2: Global Deviance = -456.3473  #> GAMLSS-RS iteration 3: Global Deviance = -489.514  #> GAMLSS-RS iteration 4: Global Deviance = -506.4194  #> GAMLSS-RS iteration 5: Global Deviance = -510.1204  #> GAMLSS-RS iteration 6: Global Deviance = -510.3517  #> GAMLSS-RS iteration 7: Global Deviance = -510.359  #> GAMLSS-RS iteration 8: Global Deviance = -510.3592   # The `.user` space includes the spline bases, so that we can make accurate # predictions of new xs. names(m$.user) #> [1] \"data\"         \"session_info\" \"call\"         \"df_mu\"        \"df_sigma\"     #> [6] \"basis_mu\"     \"basis_sigma\"   # predict logit(mean) at 55 months: logit_mean_55 <- cbind(1, predict(m$.user$basis_mu, 55)) %*% coef(m) logit_mean_55 #>          [,1] #> [1,] 1.627416 stats::plogis(logit_mean_55) #>           [,1] #> [1,] 0.8358153  # But predict_gen_gamma_gamlss() does this work for us and also provides # centiles new_ages <- data.frame(age_months = 48:71) centiles <- predict_beta_gamlss(new_ages, m) centiles #> # A tibble: 24 × 8 #>    age_months    mu sigma    c5   c10   c50   c90   c95 #>         <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #>  1         48 0.761 0.296 0.527 0.586 0.778 0.912 0.936 #>  2         49 0.773 0.291 0.547 0.605 0.791 0.918 0.941 #>  3         50 0.785 0.286 0.566 0.623 0.803 0.924 0.946 #>  4         51 0.796 0.281 0.584 0.640 0.814 0.929 0.950 #>  5         52 0.807 0.276 0.602 0.656 0.824 0.934 0.953 #>  6         53 0.817 0.271 0.619 0.672 0.834 0.938 0.957 #>  7         54 0.827 0.266 0.636 0.687 0.844 0.943 0.960 #>  8         55 0.836 0.261 0.652 0.702 0.852 0.946 0.963 #>  9         56 0.844 0.256 0.668 0.716 0.861 0.950 0.965 #> 10         57 0.852 0.251 0.683 0.730 0.868 0.953 0.968 #> # ℹ 14 more rows  # Confirm that the manual prediction matches the automatic one centiles[centiles$age_months == 55, \"mu\"] #> # A tibble: 1 × 1 #>      mu #>   <dbl> #> 1 0.836 stats::plogis(logit_mean_55) #>           [,1] #> [1,] 0.8358153  if(requireNamespace(\"ggplot2\", quietly = TRUE)) {   library(ggplot2)   ggplot(pivot_centiles_longer(centiles)) +     aes(x = age_months, y = .value) +     geom_line(aes(group = .centile, color = .centile_pair)) +     geom_point(       aes(y = intelligibility),       data = subset(         data_fake_intelligibility,         48 <= age_months & age_months <= 71       )     ) }   # Age of steepest growth for each centile optimize_beta_gamlss_slope(   model = m,   centiles = c(5, 10, 50, 90),   interval = range(data_fake_intelligibility$age_months) ) #> # A tibble: 4 × 3 #>   maximum objective quantile #>     <dbl>     <dbl>    <dbl> #> 1    40.0    0.0222     0.05 #> 2    37.7    0.0225     0.1  #> 3    29.8    0.0217     0.5  #> 4    28.0    0.0153     0.9   # Manual approach: Make fine grid of predictions and find largest jump centiles_grid <- predict_beta_gamlss(   newdata = data.frame(age_months = seq(28, 95, length.out = 1000)),   model = m ) centiles_grid[which.max(diff(centiles_grid$c5)), \"age_months\"] #> # A tibble: 1 × 1 #>   age_months #>        <dbl> #> 1       39.9  # When do children in different centiles reach 50%, 70% intelligibility? uniroot_beta_gamlss(   model = m,   centiles = c(5, 10, 50),   targets = c(.5, .7) ) #> # A tibble: 6 × 7 #>   quantile target  root    f.root  iter init.it estim.prec #>      <dbl>  <dbl> <dbl>     <dbl> <int>   <int>      <dbl> #> 1     0.05    0.5  46.7 -3.31e-11     7      NA  0.0000610 #> 2     0.1     0.5  43.7  1.38e- 7     6      NA  0.0000610 #> 3     0.5     0.5  32.4 -1.91e- 9     5      NA  0.0000610 #> 4     0.05    0.7  58.2 -1.23e-11     7      NA  0.0000610 #> 5     0.1     0.7  54.8 -1.72e- 8     7      NA  0.0000610 #> 6     0.5     0.7  42.7 -2.51e- 7     7      NA  0.0000610"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/brms_args_create.html","id":null,"dir":"Reference","previous_headings":"","what":"Set default arguments for brms model fitting — brms_args_create","title":"Set default arguments for brms model fitting — brms_args_create","text":"Set default arguments brms model fitting","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/brms_args_create.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Set default arguments for brms model fitting — brms_args_create","text":"","code":"brms_args_create(   ...,   .backend = \"cmdstanr\",   .threads = 2,   .chains = 4,   .cores = 4,   .iter = 2000,   .silent = 0,   .file_refit = \"on_change\",   .refresh = 25,   .control = list() )"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/brms_args_create.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Set default arguments for brms model fitting — brms_args_create","text":"... arguments brms::brm() use default values. .backend, .threads, .chains, .cores, .iter, .silent, .file_refit, .refresh, .control arguments set default default values. Overwrite defaults using argument without . prefix.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/brms_args_create.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Set default arguments for brms model fitting — brms_args_create","text":"function generating list arguments brms::brm().","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/brms_args_create.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Set default arguments for brms model fitting — brms_args_create","text":"One can set default value adapt_delta directly (adapt_delta = .98). value propagated control$adapt_delta.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/brms_args_create.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Set default arguments for brms model fitting — brms_args_create","text":"","code":"brm_args <- brms_args_create()  # using package-provided defaults brm_args() #> List of 9 #>  $ backend   : chr \"cmdstanr\" #>  $ threads   : num 2 #>  $ chains    : num 4 #>  $ cores     : num 4 #>  $ iter      : num 2000 #>  $ silent    : num 0 #>  $ file_refit: chr \"on_change\" #>  $ refresh   : num 25 #>  $ control   : list() #>  - attr(*, \"class\")= chr [1:2] \"brm_args\" \"list\"  # overwriting a default value brm_args(iter = 500) #> List of 9 #>  $ backend   : chr \"cmdstanr\" #>  $ threads   : num 2 #>  $ chains    : num 4 #>  $ cores     : num 4 #>  $ iter      : num 500 #>  $ silent    : num 0 #>  $ file_refit: chr \"on_change\" #>  $ refresh   : num 25 #>  $ control   : list() #>  - attr(*, \"class\")= chr [1:2] \"brm_args\" \"list\"  # adapt_delta is handled specially brm_args(adapt_delta = .95) #> List of 9 #>  $ backend   : chr \"cmdstanr\" #>  $ threads   : num 2 #>  $ chains    : num 4 #>  $ cores     : num 4 #>  $ iter      : num 2000 #>  $ silent    : num 0 #>  $ file_refit: chr \"on_change\" #>  $ refresh   : num 25 #>  $ control   :List of 1 #>   ..$ adapt_delta: num 0.95 #>  - attr(*, \"class\")= chr [1:2] \"brm_args\" \"list\" # adapt_delta is handled specially brm_args(adapt_delta = .95) #> List of 9 #>  $ backend   : chr \"cmdstanr\" #>  $ threads   : num 2 #>  $ chains    : num 4 #>  $ cores     : num 4 #>  $ iter      : num 2000 #>  $ silent    : num 0 #>  $ file_refit: chr \"on_change\" #>  $ refresh   : num 25 #>  $ control   :List of 1 #>   ..$ adapt_delta: num 0.95 #>  - attr(*, \"class\")= chr [1:2] \"brm_args\" \"list\"  # We can overwrite the package-provided defaults other_brm_args <- brms_args_create(iter = 4000, backend = \"rstan\") other_brm_args() #> List of 9 #>  $ backend   : chr \"rstan\" #>  $ threads   : num 2 #>  $ chains    : num 4 #>  $ cores     : num 4 #>  $ iter      : num 4000 #>  $ silent    : num 0 #>  $ file_refit: chr \"on_change\" #>  $ refresh   : num 25 #>  $ control   : list() #>  - attr(*, \"class\")= chr [1:2] \"brm_args\" \"list\"  # And overwrite them too other_brm_args(backend = \"cmdstanr\") #> List of 9 #>  $ backend   : chr \"cmdstanr\" #>  $ threads   : num 2 #>  $ chains    : num 4 #>  $ cores     : num 4 #>  $ iter      : num 4000 #>  $ silent    : num 0 #>  $ file_refit: chr \"on_change\" #>  $ refresh   : num 25 #>  $ control   : list() #>  - attr(*, \"class\")= chr [1:2] \"brm_args\" \"list\""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/check_model_centiles.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the percentage of points under each centile line — check_model_centiles","title":"Compute the percentage of points under each centile line — check_model_centiles","text":"check_model_centiles() computes centiles model computes calibration centile. check_computed_centiles() works dataframe precomputed c[XX] columns centiles.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/check_model_centiles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the percentage of points under each centile line — check_model_centiles","text":"","code":"check_model_centiles(   data,   model,   var_x,   var_y,   centiles = c(5, 10, 25, 50, 75, 90, 95) )  check_computed_centiles(data, var_y)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/check_model_centiles.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the percentage of points under each centile line — check_model_centiles","text":"data dataset used fit model. dataframe grouped dplyr::group_by(), sample centiles computed group. model gamlss model prepared mem_gamlss() var_x, var_y bare column names predictor outcome variables centiles centiles use prediction. Defaults c(5, 10, 25, 50, 75, 90, 95).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/check_model_centiles.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the percentage of points under each centile line — check_model_centiles","text":"tibble number points percentage points less equal quantile value.","code":""},{"path":[]},{"path":"https://www.tjmahr.com/wisclabmisc/reference/check_sample_centiles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"(Deprecated) Compute the percentage of points under each centile line — check_sample_centiles","text":"","code":"check_sample_centiles(   data,   model,   var_x,   var_y,   centiles = c(5, 10, 25, 50, 75, 90, 95) )"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/check_sample_centiles.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"(Deprecated) Compute the percentage of points under each centile line — check_sample_centiles","text":"check_sample_centiles() renamed check_model_centiles().","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/chrono_age.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute chronological age in months — chrono_age","title":"Compute chronological age in months — chrono_age","text":"Ages rounded nearest month. difference 20 months, 29 days interpreted 20 months.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/chrono_age.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute chronological age in months — chrono_age","text":"","code":"chrono_age(t1, t2)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/chrono_age.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute chronological age in months — chrono_age","text":"t1, t2 dates \"yyyy-mm-dd\" format","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/chrono_age.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute chronological age in months — chrono_age","text":"chronological ages months. NA returned age computed.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/chrono_age.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute chronological age in months — chrono_age","text":"","code":"# Two years exactly chrono_age(\"2014-01-20\", \"2012-01-20\") #> [1] 24 #> 24  # Shift a year chrono_age(\"2014-01-20\", \"2013-01-20\") #> [1] 12 #> 12 chrono_age(\"2014-01-20\", \"2011-01-20\") #> [1] 36 #> 36  # Shift a month chrono_age(\"2014-01-20\", \"2012-02-20\") #> [1] 23 #> 23 chrono_age(\"2014-01-20\", \"2011-12-20\") #> [1] 25 #> 25  # 3 months exactly chrono_age(\"2014-05-10\", \"2014-02-10\") #> [1] 3 #> 3  # Borrow a month when the earlier date has a later day chrono_age(\"2014-05-10\", \"2014-02-11\") #> [1] 2 #> 2, equal to 2 months, 29 days rounded down to nearest month  # Inverted argument order chrono_age(\"2012-01-20\", \"2014-01-20\") #> [1] 24 #> 24  # Multiple dates t1 <- c(\"2012-01-20\", \"2014-02-10\", \"2010-10-10\") t2 <- c(\"2014-01-20\", \"2014-05-10\", \"2014-11-10\") chrono_age(t1, t2) #> [1] 24  3 49 #> [1] 24  3 49"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_empirical_roc.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an ROC curve from observed data — compute_empirical_roc","title":"Create an ROC curve from observed data — compute_empirical_roc","text":"Create ROC curve observed data","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_empirical_roc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an ROC curve from observed data — compute_empirical_roc","text":"","code":"compute_empirical_roc(   data,   response,   predictor,   direction = \"auto\",   best_weights = c(1, 0.5),   ... )"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_empirical_roc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an ROC curve from observed data — compute_empirical_roc","text":"data dataframe containing responses (groupings) predictor variable response bare column name group status (control vs. cases) predictor bare column name predictor use classification direction direction set pROC::roc(). Defaults \"auto\". best_weights weights computing best ROC curve points. Defaults c(1, .5), defaults used pROC::coords(). ... additional arguments passed pROC::roc().","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_empirical_roc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an ROC curve from observed data — compute_empirical_roc","text":"new dataframe ROC coordinates returned columns predictor variable, .sensitivities, .specificities, .auc, .direction, .controls, .cases, .n_controls, .n_cases, .is_best_youden .is_best_closest_topleft.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_empirical_roc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an ROC curve from observed data — compute_empirical_roc","text":"","code":"set.seed(100) x1 <- rnorm(100, 4, 1) x2 <- rnorm(100, 2, .5) both <- c(x1, x2) steps <- seq(min(both), max(both), length.out = 200) d1 <- dnorm(steps, mean(x1), sd(x1)) d2 <- dnorm(steps, mean(x2), sd(x2)) data <- tibble::tibble(   y = steps,   d1 = d1,   d2 = d2,   outcome = rbinom(200, 1, prob = 1 - (d1 / (d1 + d2))),   group = ifelse(outcome, \"case\", \"control\") )  # get an ROC on the fake data compute_empirical_roc(data, outcome, y) #> Setting levels: control = 0, case = 1 #> Setting direction: controls > cases # this guess the cases and controls from the group name and gets it wrong compute_empirical_roc(data, group, y) #> Setting levels: control = case, case = control #> Setting direction: controls < cases # better compute_empirical_roc(data, group, y, levels = c(\"control\", \"case\")) #> Setting direction: controls > cases"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute positive and negative predictive value — compute_predictive_value_from_rates","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"Compute positive negative predictive value","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"","code":"compute_predictive_value_from_rates(sensitivity, specificity, prevalence)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"sensitivity, specificity, prevalence vectors confusion matrix rates","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"tibble columns sensitivity, specificity, prevalence, ppv, npv ppv npv positive predictive value negative predictive value.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"vectors passed function common length /length 1. example, 4 sensitivities, 4 specificities 1 incidence work sensitivities specificities common length can safely recycle (reuse) incidence value. 4 sensitivities, 2 specificities, 1 incidence fail common length.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_predictive_value_from_rates.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute positive and negative predictive value — compute_predictive_value_from_rates","text":"","code":"compute_predictive_value_from_rates(   sensitivity = .9,   specificity = .8,   prevalence = .05 ) #> # A tibble: 1 × 5 #>   sensitivity specificity prevalence   ppv   npv #>         <dbl>       <dbl>      <dbl> <dbl> <dbl> #> 1         0.9         0.8       0.05 0.191 0.993  compute_predictive_value_from_rates(   sensitivity = .67,   specificity = .53,   prevalence = c(.15, .3) ) #> # A tibble: 2 × 5 #>   sensitivity specificity prevalence   ppv   npv #>         <dbl>       <dbl>      <dbl> <dbl> <dbl> #> 1        0.67        0.53       0.15 0.201 0.901 #> 2        0.67        0.53       0.3  0.379 0.789"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_smooth_density_roc.html","id":null,"dir":"Reference","previous_headings":"","what":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","title":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","text":"Create ROC curve smoothed densities","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_smooth_density_roc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","text":"","code":"compute_smooth_density_roc(   data,   controls,   cases,   along = NULL,   best_weights = c(1, 0.5),   direction = \"auto\",   ... )"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_smooth_density_roc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","text":"data dataframe containing densities controls, cases bare column name densities control group along optional bare column name response values best_weights weights computing best ROC curve points. Defaults c(1, .5), defaults used pROC::coords(). direction direction set pROC::roc(). Defaults \"auto\". ... additional arguments. used currently.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_smooth_density_roc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","text":"dataframe updated new columns .sensitivities, .specificities, .auc, .roc_row, .is_best_youden .is_best_closest_topleft.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/compute_smooth_density_roc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Create an ROC curve from smoothed densities — compute_smooth_density_roc","text":"","code":"set.seed(100) x1 <- rnorm(100, 4, 1) x2 <- rnorm(100, 2, .5) both <- c(x1, x2) steps <- seq(min(both), max(both), length.out = 200) d1 <- dnorm(steps, mean(x1), sd(x1)) d2 <- dnorm(steps, mean(x2), sd(x2)) data <- tibble::tibble(   y = steps,   d1 = d1,   d2 = d2,   outcome = rbinom(200, 1, prob = 1 - (d1 / (d1 + d2))),   group = ifelse(outcome, \"case\", \"control\") ) compute_smooth_density_roc(data, d1, d2) #> # A tibble: 202 × 12 #>        y      d1     d2 outcome group .sensitivities .specificities  .auc #>    <dbl>   <dbl>  <dbl>   <int> <chr>          <dbl>          <dbl> <dbl> #>  1 0.932 0.00423 0.0264       1 case        0.000751          1.00  0.967 #>  2 0.960 0.00460 0.0319       1 case        0.00166           1.00  0.967 #>  3 0.989 0.00499 0.0383       1 case        0.00275           1.00  0.967 #>  4 1.02  0.00542 0.0459       1 case        0.00406           0.999 0.967 #>  5 1.05  0.00587 0.0546       1 case        0.00561           0.999 0.967 #>  6 1.07  0.00636 0.0647       1 case        0.00746           0.999 0.967 #>  7 1.10  0.00689 0.0763       1 case        0.00963           0.999 0.967 #>  8 1.13  0.00745 0.0895       1 case        0.0122            0.999 0.967 #>  9 1.16  0.00806 0.104        1 case        0.0152            0.998 0.967 #> 10 1.19  0.00870 0.121        1 case        0.0186            0.998 0.967 #> # ℹ 192 more rows #> # ℹ 4 more variables: .roc_row <int>, .direction <chr>, .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl> compute_smooth_density_roc(data, d1, d2, along = y) #> # A tibble: 202 × 12 #>        y      d1     d2 outcome group .sensitivities .specificities  .auc #>    <dbl>   <dbl>  <dbl>   <int> <chr>          <dbl>          <dbl> <dbl> #>  1 0.932 0.00423 0.0264       1 case        0.000751          1.00  0.967 #>  2 0.960 0.00460 0.0319       1 case        0.00166           1.00  0.967 #>  3 0.989 0.00499 0.0383       1 case        0.00275           1.00  0.967 #>  4 1.02  0.00542 0.0459       1 case        0.00406           0.999 0.967 #>  5 1.05  0.00587 0.0546       1 case        0.00561           0.999 0.967 #>  6 1.07  0.00636 0.0647       1 case        0.00746           0.999 0.967 #>  7 1.10  0.00689 0.0763       1 case        0.00963           0.999 0.967 #>  8 1.13  0.00745 0.0895       1 case        0.0122            0.999 0.967 #>  9 1.16  0.00806 0.104        1 case        0.0152            0.998 0.967 #> 10 1.19  0.00870 0.121        1 case        0.0186            0.998 0.967 #> # ℹ 192 more rows #> # ℹ 4 more variables: .roc_row <int>, .direction <chr>, .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl>  # terrible ROC because the response is not present (just the densities) data_shuffled <- data[sample(seq_len(nrow(data))), ] compute_smooth_density_roc(data_shuffled, d1, d2) #> # A tibble: 202 × 12 #>        y     d1       d2 outcome group   .sensitivities .specificities  .auc #>    <dbl>  <dbl>    <dbl>   <int> <chr>            <dbl>          <dbl> <dbl> #>  1  2.29 0.0963 7.70e- 1       1 case            0.0219          0.997 0.551 #>  2  3.43 0.334  1.66e- 3       0 control         0.0220          0.988 0.551 #>  3  4.40 0.363  1.49e- 8       0 control         0.0220          0.977 0.551 #>  4  6.33 0.0293 2.63e-26       0 control         0.0220          0.976 0.551 #>  5  6.35 0.0275 1.21e-26       0 control         0.0220          0.976 0.551 #>  6  5.59 0.117  2.59e-18       0 control         0.0220          0.972 0.551 #>  7  3.00 0.242  4.30e- 2       0 control         0.0232          0.965 0.551 #>  8  3.26 0.300  7.00e- 3       1 case            0.0234          0.957 0.551 #>  9  3.20 0.288  1.09e- 2       0 control         0.0237          0.949 0.551 #> 10  4.20 0.384  2.64e- 7       0 control         0.0237          0.938 0.551 #> # ℹ 192 more rows #> # ℹ 4 more variables: .roc_row <int>, .direction <chr>, .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl>  # sorted along response first: correct AUC compute_smooth_density_roc(data_shuffled, d1, d2, along = y) #> # A tibble: 202 × 12 #>        y     d1       d2 outcome group   .sensitivities .specificities  .auc #>    <dbl>  <dbl>    <dbl>   <int> <chr>            <dbl>          <dbl> <dbl> #>  1  2.29 0.0963 7.70e- 1       1 case             0.776        0.952   0.967 #>  2  3.43 0.334  1.66e- 3       0 control          1.00         0.707   0.967 #>  3  4.40 0.363  1.49e- 8       0 control          1.00         0.342   0.967 #>  4  6.33 0.0293 2.63e-26       0 control          1            0.00551 0.967 #>  5  6.35 0.0275 1.21e-26       0 control          1            0.00472 0.967 #>  6  5.59 0.117  2.59e-18       0 control          1            0.0534  0.967 #>  7  3.00 0.242  4.30e- 2       0 control          0.995        0.833   0.967 #>  8  3.26 0.300  7.00e- 3       1 case             0.999        0.762   0.967 #>  9  3.20 0.288  1.09e- 2       0 control          0.999        0.779   0.967 #> 10  4.20 0.384  2.64e- 7       0 control          1.00         0.416   0.967 #> # ℹ 192 more rows #> # ℹ 4 more variables: .roc_row <int>, .direction <chr>, .is_best_youden <lgl>, #> #   .is_best_closest_topleft <lgl>"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_example_intelligibility_by_length.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulated intelligibility scores by utterance length — data_example_intelligibility_by_length","title":"Simulated intelligibility scores by utterance length — data_example_intelligibility_by_length","text":"dataset simulated intelligibility scores testing demonstrating modeling functions. created fitting Bayesian model raw Hustad colleagues (2020) drawing 1 sample posterior distribution expected predictions (.e., \"epreds). words, values model predictions original dataset. correlated original dataset values r = .86. might think simulation adding random noise original dataset.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_example_intelligibility_by_length.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulated intelligibility scores by utterance length — data_example_intelligibility_by_length","text":"","code":"data_example_intelligibility_by_length"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_example_intelligibility_by_length.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Simulated intelligibility scores by utterance length — data_example_intelligibility_by_length","text":"data frame 694 rows 5 variables: child identifier child age_months child's age months length_longest length child's longest utterance tocs_level utterance length sim_intelligibility child's intelligibility given utterance length (proportion words said child correctly transcribed two listeners)","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_example_intelligibility_by_length.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Simulated intelligibility scores by utterance length — data_example_intelligibility_by_length","text":"Hustad, K. C., Mahr, T., Natzke, P. E. M., & Rathouz, P. J. (2020). Development Speech Intelligibility 30 47 Months Typically Developing Children: Cross-Sectional Study Growth. Journal Speech, Language, Hearing Research, 63(6), 1675–1687. https://doi.org/10.1044/2020_JSLHR-20-00008","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_intelligibility.html","id":null,"dir":"Reference","previous_headings":"","what":"Fake intelligibility data — data_fake_intelligibility","title":"Fake intelligibility data — data_fake_intelligibility","text":"dataset fake intelligibility scores testing demonstrating modeling functions. created randomly sampling 200 rows intelligibility dataset adding random noise age_months intelligibility variables. values measure real children represent plausible age intelligibility measurements kind work.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_intelligibility.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fake intelligibility data — data_fake_intelligibility","text":"","code":"data_fake_intelligibility"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_intelligibility.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fake intelligibility data — data_fake_intelligibility","text":"data frame 200 rows 2 variables: age_months child's age months intelligibility child's intelligibility (proportion words said child correctly transcribed two listeners)","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_rates.html","id":null,"dir":"Reference","previous_headings":"","what":"Fake speaking rate data — data_fake_rates","title":"Fake speaking rate data — data_fake_rates","text":"dataset fake speaking rate measures testing demonstrating modeling functions. created randomly sampling 200 rows speaking rate dataset adding random noise age_months speaking_sps variables. values measure real children represent plausible age rate measurements kind work.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_rates.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fake speaking rate data — data_fake_rates","text":"","code":"data_fake_rates"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_fake_rates.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Fake speaking rate data — data_fake_rates","text":"data frame 200 rows 2 variables: age_months child's age months speaking_sps child's speaking rate syllables per second","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_features_consonants.html","id":null,"dir":"Reference","previous_headings":"","what":"Phonetic and acquisition features of consonants and vowels — data_features_consonants","title":"Phonetic and acquisition features of consonants and vowels — data_features_consonants","text":"dataframes contain information consonants vowels American English. phonetic features conventional descriptions sounds produced. acquisition (acq) features (try ) describe expected acquisition speech-motor difficulty speech sounds.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_features_consonants.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Phonetic and acquisition features of consonants and vowels — data_features_consonants","text":"","code":"data_features_consonants  data_features_vowels  data_acq_consonants"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_features_consonants.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Phonetic and acquisition features of consonants and vowels — data_features_consonants","text":"object class tbl_df (inherits tbl, data.frame) 24 rows 11 columns. object class tbl_df (inherits tbl, data.frame) 17 rows 13 columns. object class tbl_df (inherits tbl, data.frame) 24 rows 9 columns.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_features_consonants.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Phonetic and acquisition features of consonants and vowels — data_features_consonants","text":"phonetic features self-evident definitional. example, /p/ bilabial voiceless stop. fuzzier features, consulted general IPA chart Wikipedia page English phonology. issues included things like: lax vowels ? last two rows consonant tables approximants, /r,l,j/ approximants. features alternative feature sets order accommodate degrees aggregation. example, /r,l,j,w/ approximant manner divided liquid glide manner_alt.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_features_consonants.html","id":"phonetic-features-of-consonants","dir":"Reference","previous_headings":"","what":"Phonetic features of consonants","title":"Phonetic and acquisition features of consonants and vowels — data_features_consonants","text":"data_features_consonants dataframe 24 rows 10 variables.   Description column: phone phone IPA cmubet phone CMU alphabet wiscbet phone older system used lab voicing voiced versus voiceless voicing_alt spread_glottis versus plain manner manner articulation manner_alt alternative manner coding separates approximants liquids glides place place articulation place_fct place coded factor ordered based frontness articulators. labiovelar recoded NA. sonorance obstruent versus sonorant sonorance_alt obstruant versus sonorant versus strident. Levels factor columns:","code":"knitr::kable(data_features_consonants) data_features_consonants |>   lapply(levels) |>   Filter(length, x = _) #> $place_fct #> [1] \"labial\"       \"labiodental\"  \"dental\"       \"alveolar\"     \"postalveolar\" #> [6] \"palatal\"      \"velar\"        \"glottal\""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_features_consonants.html","id":"considerations-about-consonant-phonetic-features","dir":"Reference","previous_headings":"","what":"Considerations about consonant phonetic features","title":"Phonetic and acquisition features of consonants and vowels — data_features_consonants","text":"CMU alphabet include glottal stop. /f,v/ coded strident following Wikipedia Sound Pattern English. feature value seem right, probably use alternative feature sibilant stridents minus /f,v/. alternative voicing scheme suggested colleague voice-voiceless phonetic contrast achieved different articulatory strategies different languages. Note voicing_alt assign feature nasals approximants.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_features_consonants.html","id":"phonetic-features-of-vowels","dir":"Reference","previous_headings":"","what":"Phonetic features of vowels","title":"Phonetic and acquisition features of consonants and vowels — data_features_consonants","text":"data_features_vowels dataframe 17 rows 11 variables.   Description column: phone phone IPA cmubet phone CMU alphabet wiscbet phone older system used lab hint word containing selected vowel manner manner articulation manner_alt alternative manner vowel, diphthong r-colored tenseness tense versus lax (versus diphthong r-colored) height vowel height four-level scale height_fct height coded factor ordered high, mid-high, mid-low, low. diphthong recoded NA. height_alt vowel height three-level scale backness vowel backness backness_fct backness coded factor ordered front, central, back. diphthong recoded NA. rounding unrounded versus rounded (versus diphthong r-colored) Levels factor columns:","code":"knitr::kable(data_features_vowels) data_features_vowels |>   lapply(levels) |>   Filter(length, x = _) #> $height_fct #> [1] \"high\"     \"mid-high\" \"mid-low\"  \"low\" #> #> $backness_fct #> [1] \"front\"   \"central\" \"back\""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_features_consonants.html","id":"considerations-about-vowel-features","dir":"Reference","previous_headings":"","what":"Considerations about vowel features","title":"Phonetic and acquisition features of consonants and vowels — data_features_consonants","text":"consider /eɪ/ /oʊ/ diphthongs, perhaps manner_alt encode difference vowels others. CMU alphabet ARPAbet, vowels can include number indicate vowel stress, AH1 AH2 /ʌ/ AH0 /ə/. vowel features General American English, according Wikipedia, follows: adapted features way: four-level breakdown height (high, mid-high, mid-low, low) used instead three-level one (close, mid, open). tense lax features directly borrowed. Diphthongs r-colored vowels assign tenseness. /ɑ/ moved back (following general IPA) diphthongs backness height r-colored vowels given backness height /ʌ,ə/ Based assumption /ʌ,ə/ general vowel differing stress, vowels features. definition clashes general IPA chart places /ʌ/ back vowel. However, /ʌ/ conventional notation. Quoting Wikipedia : \"Although notation /ʌ/ used vowel STRUT RP General American, actual pronunciation closer near-open central vowel [ɐ] RP advanced back [ʌ̟] General American.\" , /ʌ/ fronted American English (hence, mid) American English.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_features_consonants.html","id":"acquisition-features-of-consonants","dir":"Reference","previous_headings":"","what":"Acquisition features of consonants","title":"Phonetic and acquisition features of consonants and vowels — data_features_consonants","text":"data_acq_consonants provides   Description column: phone phone IPA cmubet phone CMU alphabet wiscbet phone older system used lab cm2020_90_age_mean, cm2020_90_age_sd, cm2020_90_age_min, cm2020_90_age_max Age acquisition statistics reported Crowe & McLeod (2020). Statistics mean, SD, min max age (months) children reached 90% accuracy consonant. cm2020_90_num_studies Number studies used Crowe & McLeod (2020) compute corresponding statistics. cm2020_90_stage Developmental stage assigned consonant Crowe & McLeod (2020). Sounds age_mean 48 months early, 60 months middle, 60 older late. Crowe McLeod (2020, cm2020_ variables) provides systematic review summary statistics age acquisition norms English consonants. scoured literature acquisition ages individual consonants computed summary statistics . considered just accuracy sounds produced single words. sources include mix journal articles norms articulation assessments. weight statistics individual studies sample size sampling procedure. prepared data copying relevant numbers Table 2 making following changes: 1) rounding mean SD values 1 decimal point (3 days ages months), 2) dropping /ʍ/, 3) using /r/, /g/, /tʃ/, /dʒ/ IPA characters instead specialized characters used article.","code":"knitr::kable(data_acq_consonants)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/data_features_consonants.html","id":"acquisition-references","dir":"Reference","previous_headings":"","what":"Acquisition references","title":"Phonetic and acquisition features of consonants and vowels — data_features_consonants","text":"Crowe, K., & McLeod, S. (2020). Children’s English Consonant Acquisition United States: Review. American Journal Speech-Language Pathology, 29(4), 2155–2169. https://doi.org/10.1044/2020_AJSLP-19-00168","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/file_rename_with.html","id":null,"dir":"Reference","previous_headings":"","what":"Rename file basenames using functions — file_replace_name","title":"Rename file basenames using functions — file_replace_name","text":"file_replace_name() uses stringr::str_replace() rename files. file_rename_with() allows rename files generic string-transforming function.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/file_rename_with.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Rename file basenames using functions — file_replace_name","text":"","code":"file_replace_name(   path,   pattern,   replacement,   .dry_run = FALSE,   .overwrite = FALSE )  file_rename_with(path, .fn, ..., .dry_run = FALSE, .overwrite = FALSE)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/file_rename_with.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Rename file basenames using functions — file_replace_name","text":"path vector paths files rename pattern, replacement arguments forwarded stringr::str_replace() .dry_run FALSE (default), files renamed. TRUE, files renamed affected files printed . .overwrite Whether overwrite files. Defaults FALSE overwriting files opt-. .fn function call file paths ... arguments passed onto .fn","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/file_rename_with.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Rename file basenames using functions — file_replace_name","text":"contents paths updated file names. Duplicated elements removed. function throws error name collision detected (two files renamed target path).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/file_rename_with.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Rename file basenames using functions — file_replace_name","text":"basename file (returned basename() undergoes string replacement).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/file_rename_with.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Rename file basenames using functions — file_replace_name","text":"","code":"# With .dry_run = TRUE, we can make up some file paths. dir <- \"//some-fake-location/\" path <- file.path(   dir,   c(\"report_1.csv\", \"report_2.csv\", \"report-1.csv\", \"skipped.csv\") )  updated <- file_replace_name(path, \"report_\", \"report-\", .dry_run = TRUE) #> Planned changes: #> ! report_1.csv -> report-1.csv (overwrites an existing file) #>   report_2.csv -> report-2.csv  # Collisions are detected updated <- file_replace_name(path, \"report_\\\\d\", \"report-1\", .dry_run = TRUE) #> Planned changes: #> ✖ (report_1.csv, report_2.csv) -> report-1.csv (naming collision)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/fit_kmeans.html","id":null,"dir":"Reference","previous_headings":"","what":"Run (scaled) k-means on a dataset. — fit_kmeans","title":"Run (scaled) k-means on a dataset. — fit_kmeans","text":"Observations scale()-ed clustering.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/fit_kmeans.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run (scaled) k-means on a dataset. — fit_kmeans","text":"","code":"fit_kmeans(data, k, vars, args_kmeans = list())"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/fit_kmeans.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run (scaled) k-means on a dataset. — fit_kmeans","text":"data dataframe k number clusters create vars variable selection clustering. Select multiple variables c(), e.g., c(x, y). selection supports tidyselect semantics tidyselect::select_helpers, e.g., c(x, starts_with(\"mean_\"). args_kmeans additional arguments passed stats::kmeans().","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/fit_kmeans.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run (scaled) k-means on a dataset. — fit_kmeans","text":"original data augmented additional columns clustering details. including .kmeans_cluster (cluster number observation, factor) .kmeans_k (selected number clusters). Cluster-level information also included. example, suppose cluster using variable x. output column .kmeans_x giving cluster mean x .kmeans_rank_x giving cluster labels reordered using cluster means x. column .kmeans_sort contains cluster sorted using first principal component scaled variables. columns cluster indices factor() can plotted discrete variables.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/fit_kmeans.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run (scaled) k-means on a dataset. — fit_kmeans","text":"Note variable scaled() clustering cluster means unscaled match original data scale. function provides original kmeans labels .kmeans_cluster alternative labeling based different sortings data. provided order deal label-swapping Bayesian models. See bootstrapping example .","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/fit_kmeans.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run (scaled) k-means on a dataset. — fit_kmeans","text":"","code":"data_kmeans <- fit_kmeans(mtcars, 3, c(mpg, wt, hp))  library(ggplot2) ggplot(data_kmeans) +   aes(x = wt, y = mpg) +   geom_point(aes(color = .kmeans_cluster))   ggplot(data_kmeans) +   aes(x = wt, y = mpg) +   geom_point(aes(color = .kmeans_rank_wt))   # Example of label swapping set.seed(123) data_boots <- lapply(   1:10,   function(x) {     rows <- sample(seq_len(nrow(mtcars)), replace = TRUE)     data <- mtcars[rows, ]     data$.bootstrap <- x     data   } ) |>   lapply(fit_kmeans, k = 3, c(mpg, wt, hp)) |>   dplyr::bind_rows() |>   dplyr::select(.bootstrap, dplyr::starts_with(\".kmeans_\")) |>   dplyr::distinct()  # Clusters start off in random locations and move to center, so the labels # differ between model runs and across bootstraps. ggplot(data_boots) +   aes(x = .kmeans_wt, y = .kmeans_mpg) +   geom_point(aes(color = .kmeans_cluster)) +   labs(title = \"k-means centers on 10 bootstraps\")   # Labels sorted using first principal component # so the labels are more consistent. ggplot(data_boots) +   aes(x = .kmeans_wt, y = .kmeans_mpg) +   geom_point(aes(color = .kmeans_sort)) +   labs(title = \"k-means centers on 10 bootstraps\")"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"function fits type GAMLSS model used Mahr colleagues (2021): generalized gamma regression model (via gamlss.dist::GG()) natural cubic splines mean (mu), scale (sigma), shape (nu) distribution. model fitted using package's mem_gamlss() wrapper function.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"","code":"fit_gen_gamma_gamlss(   data,   var_x,   var_y,   df_mu = 3,   df_sigma = 2,   df_nu = 1,   control = NULL )  fit_gen_gamma_gamlss_se(   data,   name_x,   name_y,   df_mu = 3,   df_sigma = 2,   df_nu = 1,   control = NULL )  predict_gen_gamma_gamlss(newdata, model, centiles = c(5, 10, 50, 90, 95))"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"Associated article: https://doi.org/10.1044/2021_JSLHR-21-00206","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"data data frame var_x, var_y (unquoted) variable names giving predictor variable (e.g., age) outcome variable (.e.g, rate). df_mu, df_sigma, df_nu degrees freedom. 0 used, splines::ns() term dropped model formula parameter. control gamlss::gamlss.control() controller. Defaults NULL uses default settings, except setting trace FALSE silence output gamlss. name_x, name_y quoted variable names giving predictor variable (e.g., \"age\") outcome variable (.e.g, \"rate\"). arguments apply fit_gen_gamma_gamlss_se(). newdata one-column dataframe predictions model model fitted fit_gen_gamma_gamlss() centiles centiles use prediction. Defaults c(5, 10, 50, 90, 95).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"fit_gen_gamma_gamlss() fit_gen_gamma_gamlss_se(), mem_gamlss()-fitted model. .user data model includes degrees freedom parameter splines::ns() basis parameter. predict_gen_gamma_gamlss(), dataframe containing model predictions mu, sigma, nu, plus columns centile centiles.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"two versions function. main version fit_gen_gamma_gamlss(), works unquoted column names (e.g., age). alternative version fit_gen_gamma_gamlss_se(); final \"se\" stands \"Standard Evaluation\". designation means variable names must given strings (, quoted \"age\" instead bare name age). alternative version necessary fit several models using parallel computing furrr::future_map() (using bootstrap resampling). predict_centiles() work function, likely throw warning message. Therefore, predict_gen_gamma_gamlss() provides alternative way compute centiles model. function manually computes centiles instead relying gamlss::centiles(). main difference new x values go splines::predict.ns() multiplied model coefficients.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/gen-gamma-rate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Fit a generalized gamma regression model (for speaking rate) — fit_gen_gamma_gamlss","text":"","code":"data_fake_rates #> # A tibble: 200 × 2 #>    age_months speaking_sps #>         <int>        <dbl> #>  1         66         3.76 #>  2         29         2.08 #>  3         90         3.07 #>  4         61         2.64 #>  5         46         3.54 #>  6         61         3.23 #>  7         63         3.55 #>  8         51         2.84 #>  9         48         3.24 #> 10         37         2.39 #> # ℹ 190 more rows  m <- fit_gen_gamma_gamlss(data_fake_rates, age_months, speaking_sps)  # using \"qr\" in summary() just to suppress a warning message summary(m, type = \"qr\") #> ****************************************************************** #> Family:  c(\"GG\", \"generalised Gamma Lopatatsidis-Green\")  #>  #> Call:  gamlss::gamlss(formula = speaking_sps ~ ns(age_months, df = 3),   #>     sigma.formula = ~ns(age_months, df = 2), nu.formula = ~ns(age_months,   #>         df = 1), family = GG(), data = ~data_fake_rates, control = list(  #>         c.crit = 0.001, n.cyc = 20, mu.step = 1, sigma.step = 1,   #>         nu.step = 1, tau.step = 1, gd.tol = Inf, iter = 0, trace = FALSE,   #>         autostep = TRUE, save = TRUE))  #>  #> Fitting method: RS()  #>  #> ------------------------------------------------------------------ #> Mu link function:  log #> Mu Coefficients: #>                         Estimate Std. Error t value Pr(>|t|)     #> (Intercept)              0.92763    0.04539  20.435  < 2e-16 *** #> ns(age_months, df = 3)1  0.14393    0.03919   3.672 0.000310 *** #> ns(age_months, df = 3)2  0.36779    0.10288   3.575 0.000441 *** #> ns(age_months, df = 3)3  0.20240    0.03780   5.355 2.38e-07 *** #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ------------------------------------------------------------------ #> Sigma link function:  log #> Sigma Coefficients: #>                         Estimate Std. Error t value Pr(>|t|)     #> (Intercept)              -1.7623     0.1597 -11.038   <2e-16 *** #> ns(age_months, df = 2)1  -0.6923     0.3379  -2.049   0.0418 *   #> ns(age_months, df = 2)2  -0.3832     0.2073  -1.848   0.0661 .   #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ------------------------------------------------------------------ #> Nu link function:  identity  #> Nu Coefficients: #>                        Estimate Std. Error t value Pr(>|t|)   #> (Intercept)              -3.438      1.647  -2.088   0.0381 * #> ns(age_months, df = 1)    8.336      4.312   1.933   0.0547 . #> --- #> Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 #>  #> ------------------------------------------------------------------ #> No. of observations in the fit:  200  #> Degrees of Freedom for the fit:  9 #>       Residual Deg. of Freedom:  191  #>                       at cycle:  14  #>   #> Global Deviance:     184.5313  #>             AIC:     202.5313  #>             SBC:     232.2161  #> ******************************************************************  # Alternative interface d <- data_fake_rates m2 <- fit_gen_gamma_gamlss_se(   data = d,   name_x = \"age_months\",   name_y = \"speaking_sps\" ) coef(m2) == coef(m) #>             (Intercept) ns(age_months, df = 3)1 ns(age_months, df = 3)2  #>                    TRUE                    TRUE                    TRUE  #> ns(age_months, df = 3)3  #>                    TRUE   # how to use control to change gamlss() behavior m_traced <- fit_gen_gamma_gamlss(   data_fake_rates,   age_months,   speaking_sps,   control = gamlss::gamlss.control(n.cyc = 15, trace = TRUE) ) #> GAMLSS-RS iteration 1: Global Deviance = 185.9307  #> GAMLSS-RS iteration 2: Global Deviance = 185.2312  #> GAMLSS-RS iteration 3: Global Deviance = 184.9112  #> GAMLSS-RS iteration 4: Global Deviance = 184.7408  #> GAMLSS-RS iteration 5: Global Deviance = 184.6483  #> GAMLSS-RS iteration 6: Global Deviance = 184.5971  #> GAMLSS-RS iteration 7: Global Deviance = 184.5691  #> GAMLSS-RS iteration 8: Global Deviance = 184.553  #> GAMLSS-RS iteration 9: Global Deviance = 184.5436  #> GAMLSS-RS iteration 10: Global Deviance = 184.5381  #> GAMLSS-RS iteration 11: Global Deviance = 184.5348  #> GAMLSS-RS iteration 12: Global Deviance = 184.5329  #> GAMLSS-RS iteration 13: Global Deviance = 184.5319  #> GAMLSS-RS iteration 14: Global Deviance = 184.5313   # The `.user` space includes the spline bases, so that we can make accurate # predictions of new xs. names(m$.user) #> [1] \"data\"         \"session_info\" \"call\"         \"df_mu\"        \"df_sigma\"     #> [6] \"df_nu\"        \"basis_mu\"     \"basis_sigma\"  \"basis_nu\"      # predict log(mean) at 55 months: log_mean_55 <- cbind(1, predict(m$.user$basis_mu, 55)) %*% coef(m) log_mean_55 #>          [,1] #> [1,] 1.070221 exp(log_mean_55) #>          [,1] #> [1,] 2.916024  # But predict_gen_gamma_gamlss() does this work for us and also provides # centiles new_ages <- data.frame(age_months = 48:71) centiles <- predict_gen_gamma_gamlss(new_ages, m) centiles #> # A tibble: 24 × 9 #>    age_months    mu sigma     nu    c5   c10   c50   c90   c95 #>         <int> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #>  1         48  2.83 0.142 -1.60   2.29  2.40  2.86  3.47  3.68 #>  2         49  2.84 0.141 -1.50   2.30  2.41  2.87  3.48  3.68 #>  3         50  2.86 0.140 -1.40   2.32  2.43  2.88  3.48  3.68 #>  4         51  2.87 0.138 -1.31   2.33  2.44  2.89  3.48  3.68 #>  5         52  2.88 0.137 -1.21   2.34  2.45  2.90  3.49  3.68 #>  6         53  2.89 0.136 -1.11   2.35  2.46  2.91  3.49  3.68 #>  7         54  2.91 0.135 -1.02   2.36  2.47  2.92  3.49  3.68 #>  8         55  2.92 0.134 -0.920  2.37  2.48  2.93  3.50  3.68 #>  9         56  2.93 0.132 -0.823  2.38  2.49  2.94  3.50  3.68 #> 10         57  2.94 0.131 -0.726  2.39  2.50  2.95  3.50  3.68 #> # ℹ 14 more rows  # Confirm that the manual prediction matches the automatic one centiles[centiles$age_months == 55, \"mu\"] #> # A tibble: 1 × 1 #>      mu #>   <dbl> #> 1  2.92 exp(log_mean_55) #>          [,1] #> [1,] 2.916024  if(requireNamespace(\"ggplot2\", quietly = TRUE)) {   library(ggplot2)   ggplot(pivot_centiles_longer(centiles)) +     aes(x = age_months, y = .value) +     geom_line(aes(group = .centile, color = .centile_pair)) +     geom_point(       aes(y = speaking_sps),       data = subset(         data_fake_rates,         48 <= age_months & age_months <= 71       )     ) }   # Example of 0-df splines m <- fit_gen_gamma_gamlss(   data_fake_rates,   age_months,   speaking_sps,   df_mu = 0,   df_sigma = 2,   df_nu = 0 ) coef(m, what = \"mu\") #> (Intercept)  #>    1.113478  coef(m, what = \"sigma\") #>             (Intercept) ns(age_months, df = 2)1 ns(age_months, df = 2)2  #>              -1.5223591              -1.0914888              -0.2153066  coef(m, what = \"nu\") #> (Intercept)  #>    1.642225   # mu and nu fixed, c50 mostly locked in predict_gen_gamma_gamlss(new_ages, m)[c(1, 9, 17, 24), ] #> # A tibble: 4 × 9 #>   age_months    mu sigma    nu    c5   c10   c50   c90   c95 #>        <int> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> #> 1         48  3.04 0.148  1.64  2.31  2.46  3.01  3.61  3.79 #> 2         56  3.04 0.132  1.64  2.39  2.52  3.02  3.55  3.70 #> 3         64  3.04 0.122  1.64  2.44  2.56  3.02  3.51  3.65 #> 4         71  3.04 0.118  1.64  2.46  2.58  3.02  3.50  3.64"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":null,"dir":"Reference","previous_headings":"","what":"Staged imputation — impute_values_by_length","title":"Staged imputation — impute_values_by_length","text":"Impute missing data different utterance lengths using successive linear models.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Staged imputation — impute_values_by_length","text":"","code":"impute_values_by_length(   data,   var_y,   var_length,   id_cols = NULL,   include_max_length = FALSE,   data_train = NULL )"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Staged imputation — impute_values_by_length","text":"data dataframe impute missing value var_y bare name response variable imputation var_length bare name length variable id_cols selection variable names uniquely identify group related observations. example, c(child_id, age_months). include_max_length whether use maximum length value predictor imputation models. Defaults FALSE. data_train (optional) dataframe used train imputation models. example, might data reference group children data_train clinical population data. omitted, dataframe data used train models. data_train can also function. case, applied data argument order derive (filter) subset data training.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Staged imputation — impute_values_by_length","text":"dataframe additional columns {var_y}_imputed (imputed value), .max_{var_length} highest value var_length observed data, {var_y}_imputation labeling whether observations \"imputed\" \"observed\".","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":"background","dir":"Reference","previous_headings":"","what":"Background","title":"Staged imputation — impute_values_by_length","text":"Hustad colleagues (2020), modeled intelligibility data young children's speech. Children hear utterance repeat . utterances started 2 words length, increased 3 words length, batches 10 sentences, way 7 words length. problem, however: children produce utterances every length. Specifically, child reliably produced 5 utterances given length length, task halted. given nature task, child produced 5-word utterances, also produced 2–4-word utterances well. length utterance probably influenced outcome variable: Longer utterances words might help listener understand sentence, example. Therefore, seem appropriate ignore missing values. used following two-step procedure (see Supplemental Materials detail):","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":"other-notes","dir":"Reference","previous_headings":"","what":"Other notes","title":"Staged imputation — impute_values_by_length","text":"Remark data data_train: One might ask, children help train data imputation models? consider norm-referenced standardized testing scenario: new participant (observations data), want know compare age peers (participants data_train). separating data_train fixing reference group, can apply adjustment/imputation procedure new participants.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Staged imputation — impute_values_by_length","text":"Hustad, K. C., Mahr, T., Natzke, P. E. M., & Rathouz, P. J. (2020). Development Speech Intelligibility 30 47 Months Typically Developing Children: Cross-Sectional Study Growth. Journal Speech, Language, Hearing Research, 63(6), 1675–1687. https://doi.org/10.1044/2020_JSLHR-20-00008 Hustad, K. C., Mahr, T., Natzke, P. E. M., & J. Rathouz, P. (2020). Supplemental Material S1 (Hustad et al., 2020). ASHA journals. https://doi.org/10.23641/asha.12330956.v1","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/impute_values_by_length.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Staged imputation — impute_values_by_length","text":"","code":"set.seed(1) fake_data <- tibble::tibble(   child = c(     \"a\", \"a\", \"a\", \"a\", \"a\",     \"b\", \"b\", \"b\", \"b\", \"b\",     \"c\", \"c\", \"c\", \"c\", \"c\",     \"e\", \"e\", \"e\", \"e\", \"e\",     \"f\", \"f\", \"f\", \"f\", \"f\",     \"g\", \"g\", \"g\", \"g\", \"g\",     \"h\", \"h\", \"h\", \"h\", \"h\",     \"i\", \"i\", \"i\", \"i\", \"i\"   ),   level = c(1:5, 1:5, 1:5, 1:5, 1:5, 1:5, 1:5, 1:5),   x = c(     c(100, 110, 120, 130, 150) + c(-8, -5, 0, NA, NA),     c(100, 110, 120, 130, 150) + c(6, 6, 4, NA, NA),     c(100, 110, 120, 130, 150) + c(-5, -5, -2, 2, NA),     c(100, 110, 120, 130, 150) + rbinom(5, 12, .5) - 6,     c(100, 110, 120, 130, 150) + rbinom(5, 12, .5) - 6,     c(100, 110, 120, 130, 150) + rbinom(5, 12, .5) - 6,     c(100, 110, 120, 130, 150) + rbinom(5, 12, .5) - 6,     c(100, 110, 120, 130, 150) + rbinom(5, 12, .5) - 6   ) ) data_imputed <- impute_values_by_length(   fake_data,   x,   level,   id_cols = c(child),   include_max_length = FALSE )  if (requireNamespace(\"ggplot2\")) {   library(ggplot2)   ggplot(data_imputed) +     aes(x = level, y = x_imputed) +     geom_line(aes(group = child)) +     geom_point(aes(color = x_imputation)) }"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/information.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute entropy and related measures — info_surprisal","title":"Compute entropy and related measures — info_surprisal","text":"Compute entropy related measures","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/information.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute entropy and related measures — info_surprisal","text":"","code":"info_surprisal(x, base = NULL)  info_entropy(p, base = NULL)  info_cross_entropy(p, q, base = NULL)  info_kl_divergence(p, q, base = NULL)  info_kl_divergence_matrix(mat, base = NULL)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/information.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute entropy and related measures — info_surprisal","text":"x vector probabilities base base logarithm. Defaults e (exp(1)). p, q probability distribution (vector probabilities sum 1) mat matrix row probability distribution","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/information.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute entropy and related measures — info_surprisal","text":"entropy","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/information.html","id":"information-theory-basics","dir":"Reference","previous_headings":"","what":"Information theory basics","title":"Compute entropy and related measures — info_surprisal","text":"Given probability x, surprisal value (information content) x log inverse probability, log(1 / x). Rare events (smaller probabilities) larger surprisal values common events. word \"surprise\" conveys unexpected informative event . (idea surprises \"informative\" contain information makes sense intuition much learned predictable events.) division works logarithms, log(1 / x) simplifies info_surprisal(x) negative log probability,-log(x). units information content depends base logarithm. functions default use natural log() provides information nats, also common see log2()-based surprisals provide information bits. Given probability distribution p—vector probabilities sum 1—entropy distribution probability-weighted average surprisal values. weighted average sum(weights * values) / sum(weights), weights probabilities sum 1, info_entropy(p) sum(p * info_surprisal(p)). Entropy can interpreted measure uncertainty distribution; expected surprisal value distribution. info_entropy(p) surprisal values weights came probability distribution. need case. Suppose ground-truth probabilities p also estimated probabilities q. info_entropy(q) computes weighted average events surprisal info_surprisal(q) frequencies q. knew true frequencies, surprisals q occur frequency p, weighted average sum(p * info_surprisal(q)). value cross entropy q respect p, info_cross_entropy(p, q) implements function. Cross entropy commonly used loss function machine learning. Gibb's inequality says info_entropy(p) <= info_cross_entropy(p, q). Unless p q distribution, always excess uncertainty surprise cross entropy compared entropy: info_cross_entropy(p, q) = info_entropy(p) + *excess*. excess Kullback-Liebler divergence (KL divergence relative entropy). Due properties logarithms, info_kl_divergence(p, q) simplifies sum (p * log(p / q)). KL divergence important quantity comparing probability distributions. example, row confusion matrix probability distribution, KL divergence can identify rows similar. info_kl_divergence_matrix(mat) computes distance matrix using pair rows mat using KL divergence.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/information.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute entropy and related measures — info_surprisal","text":"","code":"wikipedia_example <- rbind(   p = c(9, 12, 4) / 25,   q = c(1,  1, 1) / 3 )  info_kl_divergence_matrix(wikipedia_example) #>            p         q #> p 0.00000000 0.0852996 #> q 0.09745501 0.0000000"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/join_to_split.html","id":null,"dir":"Reference","previous_headings":"","what":"Join data onto resampled IDs — join_to_split","title":"Join data onto resampled IDs — join_to_split","text":"Join data onto resampled IDs","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/join_to_split.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Join data onto resampled IDs — join_to_split","text":"","code":"join_to_split(x, y, by, validate = FALSE)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/join_to_split.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Join data onto resampled IDs — join_to_split","text":"x rset object created rsample::bootstraps() y y dataframe column id values resampled create x name column y data validate whether validate join counting number rows associated id. Defaults FALSE.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/join_to_split.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Join data onto resampled IDs — join_to_split","text":"original rset object x$data updated join y row numbers x$in_id updated work expanded dataset.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/join_to_split.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Join data onto resampled IDs — join_to_split","text":"","code":"library(dplyr) #>  #> Attaching package: ‘dplyr’ #> The following objects are masked from ‘package:stats’: #>  #>     filter, lag #> The following objects are masked from ‘package:base’: #>  #>     intersect, setdiff, setequal, union data_trees <- tibble::as_tibble(datasets::Orange)  data_tree_ids <- distinct(data_trees, Tree)  # Resample ids data_bootstraps <- data_tree_ids %>%   rsample::bootstraps(times = 20) %>%   rename(splits_id = splits) %>%   # Attach data to resampled ids   mutate(     data_splits = splits_id %>% purrr::map(       join_to_split,       data_trees,       by = \"Tree\",       validate = TRUE     )   )  data_bootstraps #> # A tibble: 20 × 3 #>    splits_id     id          data_splits     #>    <list>        <chr>       <list>          #>  1 <split [5/2]> Bootstrap01 <split [35/14]> #>  2 <split [5/2]> Bootstrap02 <split [35/14]> #>  3 <split [5/3]> Bootstrap03 <split [35/21]> #>  4 <split [5/2]> Bootstrap04 <split [35/14]> #>  5 <split [5/2]> Bootstrap05 <split [35/14]> #>  6 <split [5/1]> Bootstrap06 <split [35/7]>  #>  7 <split [5/1]> Bootstrap07 <split [35/7]>  #>  8 <split [5/2]> Bootstrap08 <split [35/14]> #>  9 <split [5/2]> Bootstrap09 <split [35/14]> #> 10 <split [5/2]> Bootstrap10 <split [35/14]> #> 11 <split [5/2]> Bootstrap11 <split [35/14]> #> 12 <split [5/1]> Bootstrap12 <split [35/7]>  #> 13 <split [5/2]> Bootstrap13 <split [35/14]> #> 14 <split [5/2]> Bootstrap14 <split [35/14]> #> 15 <split [5/2]> Bootstrap15 <split [35/14]> #> 16 <split [5/2]> Bootstrap16 <split [35/14]> #> 17 <split [5/2]> Bootstrap17 <split [35/14]> #> 18 <split [5/2]> Bootstrap18 <split [35/14]> #> 19 <split [5/1]> Bootstrap19 <split [35/7]>  #> 20 <split [5/1]> Bootstrap20 <split [35/7]>"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/logitnorm_mean.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute the mean of logit-normal distribution(s) — logitnorm_mean","title":"Compute the mean of logit-normal distribution(s) — logitnorm_mean","text":"function wrapper around logitnorm::momentsLogitnorm().","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/logitnorm_mean.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute the mean of logit-normal distribution(s) — logitnorm_mean","text":"","code":"logitnorm_mean(mu, sigma)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/logitnorm_mean.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute the mean of logit-normal distribution(s) — logitnorm_mean","text":"mu mean(s) logit scale sigma standard deviation(s) logit scale","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/logitnorm_mean.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute the mean of logit-normal distribution(s) — logitnorm_mean","text":"means distributions","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/logitnorm_mean.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute the mean of logit-normal distribution(s) — logitnorm_mean","text":"","code":"# \\donttest{ x <- logitnorm_mean(2, 1) x #>      mean  #> 0.8445375  # } # compare to simulation set.seed(100) rnorm(1000, 2, 1) |> plogis() |> mean() #> [1] 0.8444758"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/mem_gamlss.html","id":null,"dir":"Reference","previous_headings":"","what":"Fit a gamlss model but store user data — mem_gamlss","title":"Fit a gamlss model but store user data — mem_gamlss","text":"Think gamlss model memories (mem. gamlss).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/mem_gamlss.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Fit a gamlss model but store user data — mem_gamlss","text":"","code":"mem_gamlss(...)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/mem_gamlss.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Fit a gamlss model but store user data — mem_gamlss","text":"... arguments passed gamlss::gamlss()","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/mem_gamlss.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Fit a gamlss model but store user data — mem_gamlss","text":"fitted model object updated include user information model$.user. Includes dataset used fit model model$.user$data, session info model$.user$session_info call used fit model model$.user$call. model$call updated match","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/pipe.html","id":null,"dir":"Reference","previous_headings":"","what":"Pipe operator — %>%","title":"Pipe operator — %>%","text":"See magrittr::[\\%>\\%][magrittr::pipe] details.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/pipe.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Pipe operator — %>%","text":"","code":"lhs %>% rhs"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/predict_centiles.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict and tidy centiles from a GAMLSS model — predict_centiles","title":"Predict and tidy centiles from a GAMLSS model — predict_centiles","text":"gamlss trouble predictions without original training data.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/predict_centiles.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict and tidy centiles from a GAMLSS model — predict_centiles","text":"","code":"predict_centiles(newdata, model, centiles = c(5, 10, 50, 90, 95), ...)  pivot_centiles_longer(data)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/predict_centiles.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict and tidy centiles from a GAMLSS model — predict_centiles","text":"newdata one-column dataframe predictions model gamlss model prepared mem_gamlss() centiles centiles use prediction. Defaults c(5, 10, 50, 90, 95). ... arguments passed gamlss::centiles.pred() data centile predictions reshape pivot_centiles_longer()","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/predict_centiles.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict and tidy centiles from a GAMLSS model — predict_centiles","text":"tibble fitted centiles predict_centiles() long-format tibble one centile value per row pivot_centiles_longer()","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/tidyeval.html","id":null,"dir":"Reference","previous_headings":"","what":"Tidy eval helpers — tidyeval","title":"Tidy eval helpers — tidyeval","text":"rlang::sym() creates symbol string syms() creates list symbols character vector. enquo() enquos() delay execution one several function arguments. enquo() returns single quoted expression, like blueprint delayed computation. enquos() returns list quoted expressions. expr() quotes new expression locally. mostly useful build new expressions around arguments captured enquo() enquos(): expr(mean(!!enquo(arg), na.rm = TRUE)). rlang::as_name() transforms quoted variable name string. Supplying something else quoted variable name error. unlike rlang::as_label() also returns single string supports kind R object input, including quoted function calls vectors. purpose summarise object single label. label often suitable default name. know quoted expression contains (instance expressions captured enquo() variable name, call function, unquoted constant), use as_label(). know quoted simple variable name, like enforce , use as_name(). learn tidy eval use tools, visit https://tidyeval.tidyverse.org Metaprogramming section Advanced R.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/tocs_item.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract the TOCS details from a string (usually a filename) — tocs_item","title":"Extract the TOCS details from a string (usually a filename) — tocs_item","text":"Extract TOCS details string (usually filename)","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/tocs_item.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract the TOCS details from a string (usually a filename) — tocs_item","text":"","code":"tocs_item(xs)  tocs_type(xs)  tocs_length(xs)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/tocs_item.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract the TOCS details from a string (usually a filename) — tocs_item","text":"xs character vector","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/tocs_item.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract the TOCS details from a string (usually a filename) — tocs_item","text":"tocs_item() returns substring TOCS item, tocs_type() returns whether item \"single-word\" \"multiword\", tocs_length() returns length TOCS item (.e., number words).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/tocs_item.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract the TOCS details from a string (usually a filename) — tocs_item","text":"","code":"x <- c(   \"XXv16s7T06.lab\", \"XXv15s5T06.TextGrid\", \"XXv13s3T10.WAV\",   \"XXv18wT11.wav\", \"non-matching\", \"s2T01\",   \"XXv01s4B01.wav\", \"XXv01wB01.wav\" ) data.frame(   x = x,   item = tocs_item(x),   type = tocs_type(x),   length = tocs_length(x) ) #>                     x  item        type length #> 1      XXv16s7T06.lab S7T06   multiword      7 #> 2 XXv15s5T06.TextGrid S5T06   multiword      5 #> 3      XXv13s3T10.WAV S3T10   multiword      3 #> 4       XXv18wT11.wav  WT11 single-word      1 #> 5        non-matching  <NA>        <NA>     NA #> 6               s2T01 S2T01   multiword      2 #> 7      XXv01s4B01.wav S4B01   multiword      4 #> 8       XXv01wB01.wav  WB01 single-word      1"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/trapezoid_auc.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute AUCs using the trapezoid method — trapezoid_auc","title":"Compute AUCs using the trapezoid method — trapezoid_auc","text":"Compute AUCs using trapezoid method","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/trapezoid_auc.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute AUCs using the trapezoid method — trapezoid_auc","text":"","code":"trapezoid_auc(xs, ys)  partial_trapezoid_auc(xs, ys, xlim)"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/trapezoid_auc.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute AUCs using the trapezoid method — trapezoid_auc","text":"xs, ys x y positions xlim two-element vector (range) xs sum ","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/trapezoid_auc.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute AUCs using the trapezoid method — trapezoid_auc","text":"area curve computed using trapezoid method. partial_trapezoid_auc(), partial area curve computed.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/trapezoid_auc.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute AUCs using the trapezoid method — trapezoid_auc","text":"","code":"if (requireNamespace(\"rstanarm\", quietly = TRUE)) {   wells <- rstanarm::wells   r <- pROC::roc(switch ~ arsenic, wells)   pROC::auc(r)   trapezoid_auc(r$specificities, r$sensitivities)    pROC::auc(r, partial.auc = c(.9, 1), partial.auc.focus = \"sp\")   partial_trapezoid_auc(r$specificities, r$sensitivities, c(.9, 1))    pROC::auc(r, partial.auc = c(.9, 1), partial.auc.focus = \"se\")   partial_trapezoid_auc(r$sensitivities, r$specificities, c(.9, 1))    pROC::auc(r, partial.auc = c(.1, .9), partial.auc.focus = \"sp\")   partial_trapezoid_auc(r$specificities, r$sensitivities, c(.1, .9))    pROC::auc(r, partial.auc = c(.1, .9), partial.auc.focus = \"se\")   partial_trapezoid_auc(r$sensitivities, r$specificities, c(.1, .9)) } #> Setting levels: control = 0, case = 1 #> Setting direction: controls < cases #> [1] 0.5086048"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/weight_lengths_with_ordinal_model.html","id":null,"dir":"Reference","previous_headings":"","what":"Weight utterance lengths by using an ordinal regression model — weight_lengths_with_ordinal_model","title":"Weight utterance lengths by using an ordinal regression model — weight_lengths_with_ordinal_model","text":"participant, find length longest utterance. predict longest utterance length nonlinear function variable, compute probability reaching utterance length value predictor variable. probabilities normalized provide weights utterance length.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/weight_lengths_with_ordinal_model.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Weight utterance lengths by using an ordinal regression model — weight_lengths_with_ordinal_model","text":"","code":"weight_lengths_with_ordinal_model(   data_train,   var_length,   var_x,   id_cols,   spline_df = 2,   data_join = NULL )"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/weight_lengths_with_ordinal_model.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Weight utterance lengths by using an ordinal regression model — weight_lengths_with_ordinal_model","text":"data_train dataframe used train ordinal model. data_train can also function. case, applied data_join argument order derive (filter) subset data training. var_length bare name length variable. example, tocs_level. var_x bare name predictor variable. example, age_months. id_cols selection variable names uniquely identify group related observations. example, c(child_id, age_months). spline_df number degrees freedom use ordinal regression model. data_join (optional) dataset use join weights onto. feature necessary want train dataset observed data supply weights dataset missing values imputed.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/weight_lengths_with_ordinal_model.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Weight utterance lengths by using an ordinal regression model — weight_lengths_with_ordinal_model","text":"probability weights utterance length observed value var_x. added columns {var_length}_prob_reached {var_length}_weight, respectively.","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/weight_lengths_with_ordinal_model.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Weight utterance lengths by using an ordinal regression model — weight_lengths_with_ordinal_model","text":"Hustad, K. C., Mahr, T., Natzke, P. E. M., & Rathouz, P. J. (2020). Development Speech Intelligibility 30 47 Months Typically Developing Children: Cross-Sectional Study Growth. Journal Speech, Language, Hearing Research, 63(6), 1675–1687. https://doi.org/10.1044/2020_JSLHR-20-00008 Hustad, K. C., Mahr, T., Natzke, P. E. M., & J. Rathouz, P. (2020). Supplemental Material S1 (Hustad et al., 2020). ASHA journals. https://doi.org/10.23641/asha.12330956.v1","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/reference/weight_lengths_with_ordinal_model.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Weight utterance lengths by using an ordinal regression model — weight_lengths_with_ordinal_model","text":"","code":"data_weights <- weight_lengths_with_ordinal_model(   data_example_intelligibility_by_length,   tocs_level,   age_months,   child,   spline_df = 2 )  if (requireNamespace(\"ggplot2\")) {   library(ggplot2)   p1 <- ggplot(data_weights) +     aes(x = age_months, y = tocs_level_prob_reached) +     geom_line(aes(color = ordered(tocs_level)), linewidth = 1) +     scale_color_ordinal(end = .85) +     labs(y = \"Prob. of reaching length\", color = \"Utterance length\")   print(p1)    p2 <- p1 +     aes(y = tocs_level_weight) +     labs(y = \"Weight of utterance length\")   print(p2) }"},{"path":"https://www.tjmahr.com/wisclabmisc/reference/wisclabmisc-package.html","id":null,"dir":"Reference","previous_headings":"","what":"wisclabmisc: Tools to Support the 'WiscLab' — wisclabmisc-package","title":"wisclabmisc: Tools to Support the 'WiscLab' — wisclabmisc-package","text":"collection 'R' functions use (re-use) across 'WiscLab' projects. analysis presentation oriented functions–, data reading data cleaning.","code":""},{"path":[]},{"path":"https://www.tjmahr.com/wisclabmisc/reference/wisclabmisc-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"wisclabmisc: Tools to Support the 'WiscLab' — wisclabmisc-package","text":"Maintainer: Tristan Mahr tristan.mahr@wisc.edu (ORCID)","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/news/index.html","id":"wisclabmisc-011","dir":"Changelog","previous_headings":"","what":"wisclabmisc 0.1.1","title":"wisclabmisc 0.1.1","text":"Added file_rename_with() file_replace_name(). Added info_surprisal() friends computing entropy. Deprecated check_sample_centiles() deprecated. Use newly added check_model_centiles() instead. Added check_computed_centiles() calibrating precomputed centiles (allows examining centiles outside GAMLSS).","code":""},{"path":"https://www.tjmahr.com/wisclabmisc/news/index.html","id":"wisclabmisc-010","dir":"Changelog","previous_headings":"","what":"wisclabmisc 0.1.0","title":"wisclabmisc 0.1.0","text":"Package features November 2024","code":""}]
